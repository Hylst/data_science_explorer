
import React from "react";
import { EducationalCard, ExerciseCard, QuizCard } from "@/components/ui/educational-cards";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@/components/ui/card";
import { Badge } from "@/components/ui/badge";
import { Code, Play, Copy, Download, ExternalLink, Lightbulb, Target, Zap } from "lucide-react";

const PracticalExercisesSection = () => {
  const [activeCode, setActiveCode] = React.useState(0);
  const [copiedCode, setCopiedCode] = React.useState<number | null>(null);

  const codeExamples = [
    {
      title: "Classification avec Scikit-learn",
      description: "Exemple complet de classification d'iris avec analyse d√©taill√©e",
      difficulty: "D√©butant",
      estimatedTime: "20 min",
      code: `# Classification des fleurs d'Iris - Exemple complet et comment√©
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler

# 1. üìä CHARGEMENT ET EXPLORATION DES DONN√âES
print("üå∏ Analyse du dataset Iris")
print("=" * 50)

iris = load_iris()
X, y = iris.data, iris.target

# Cr√©ation d'un DataFrame pour faciliter l'analyse
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y
df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

print(f"üìà Forme du dataset: {df.shape}")
print(f"üéØ Classes: {iris.target_names}")
print(f"üìä R√©partition des classes:")
print(df['species'].value_counts())

# Statistiques descriptives
print("\\nüìä Statistiques descriptives:")
print(df.describe())

# 2. üé® VISUALISATION EXPLORATOIRE
plt.figure(figsize=(15, 10))

# Distribution des features
plt.subplot(2, 3, 1)
df[iris.feature_names].boxplot()
plt.title('üìä Distribution des caract√©ristiques')
plt.xticks(rotation=45)

# Matrice de corr√©lation
plt.subplot(2, 3, 2)
correlation_matrix = df[iris.feature_names].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('üîó Matrice de corr√©lation')

# Pairplot pour visualiser les relations
plt.subplot(2, 3, 3)
# Note: en pratique, utilisez sns.pairplot(df, hue='species')
plt.scatter(df['sepal length (cm)'], df['petal length (cm)'], c=df['target'], cmap='viridis')
plt.xlabel('Longueur s√©pale (cm)')
plt.ylabel('Longueur p√©tale (cm)')
plt.title('üå∫ Relation longueur s√©pale vs p√©tale')

plt.tight_layout()
plt.show()

# 3. ‚öôÔ∏è PR√âPARATION DES DONN√âES
print("\\n‚öôÔ∏è Pr√©paration des donn√©es")
print("=" * 30)

# Division stratifi√©e pour garder la proportion des classes
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"üìà Taille entra√Ænement: {X_train.shape[0]} √©chantillons")
print(f"üìä Taille test: {X_test.shape[0]} √©chantillons")

# Standardisation des donn√©es (importante pour SVM et Logistic Regression)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. ü§ñ COMPARAISON DE PLUSIEURS MOD√àLES
print("\\nü§ñ Comparaison des mod√®les")
print("=" * 35)

models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'SVM': SVC(random_state=42)
}

results = {}

for name, model in models.items():
    print(f"\\nüîß Entra√Ænement {name}...")
    
    # Utiliser les donn√©es standardis√©es pour Logistic Regression et SVM
    if name in ['Logistic Regression', 'SVM']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        # Validation crois√©e
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    
    # M√©triques
    accuracy = accuracy_score(y_test, y_pred)
    
    results[name] = {
        'model': model,
        'accuracy': accuracy,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }
    
    print(f"‚úÖ Pr√©cision test: {accuracy:.3f}")
    print(f"üìä CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

# 5. üèÜ S√âLECTION DU MEILLEUR MOD√àLE
best_model_name = max(results.keys(), key=lambda x: results[x]['accuracy'])
best_model = results[best_model_name]['model']

print(f"\\nüèÜ Meilleur mod√®le: {best_model_name}")
print(f"üéØ Pr√©cision: {results[best_model_name]['accuracy']:.3f}")

# 6. üìä ANALYSE D√âTAILL√âE DU MEILLEUR MOD√àLE
print(f"\\nüìä Analyse d√©taill√©e - {best_model_name}")
print("=" * 45)

# Rapport de classification d√©taill√©
y_pred_best = results[best_model_name]['predictions']
print("\\nüìã Rapport de classification:")
print(classification_report(y_test, y_pred_best, target_names=iris.target_names))

# Matrice de confusion
cm = confusion_matrix(y_test, y_pred_best)
print("\\nüéØ Matrice de confusion:")
print(cm)

# Visualisation de la matrice de confusion
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=iris.target_names, 
            yticklabels=iris.target_names)
plt.title(f'Matrice de confusion - {best_model_name}')
plt.ylabel('Vraie classe')
plt.xlabel('Classe pr√©dite')
plt.show()

# 7. üîÆ PR√âDICTIONS SUR DE NOUVEAUX EXEMPLES
print("\\nüîÆ Test sur de nouveaux exemples")
print("=" * 40)

# Exemples de nouvelles fleurs (features: sepal_length, sepal_width, petal_length, petal_width)
nouvelles_fleurs = [
    [5.1, 3.5, 1.4, 0.2],  # Ressemble √† setosa
    [6.2, 2.9, 4.3, 1.3],  # Ressemble √† versicolor  
    [7.3, 2.9, 6.3, 1.8]   # Ressemble √† virginica
]

for i, fleur in enumerate(nouvelles_fleurs):
    if best_model_name in ['Logistic Regression', 'SVM']:
        fleur_scaled = scaler.transform([fleur])
        prediction = best_model.predict(fleur_scaled)[0]
        probabilities = best_model.predict_proba(fleur_scaled)[0] if hasattr(best_model, 'predict_proba') else None
    else:
        prediction = best_model.predict([fleur])[0]
        probabilities = best_model.predict_proba([fleur])[0]
    
    espece_predite = iris.target_names[prediction]
    
    print(f"\\nüå∏ Fleur {i+1}: {fleur}")
    print(f"üéØ Pr√©diction: {espece_predite}")
    
    if probabilities is not None:
        print("üìä Probabilit√©s:")
        for j, (espece, prob) in enumerate(zip(iris.target_names, probabilities)):
            print(f"   {espece}: {prob:.3f} ({prob*100:.1f}%)")

# 8. üí° IMPORTANCE DES FEATURES (si Random Forest est le meilleur)
if best_model_name == 'Random Forest':
    print("\\nüí° Importance des caract√©ristiques")
    print("=" * 40)
    
    feature_importance = pd.DataFrame({
        'feature': iris.feature_names,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(feature_importance)
    
    # Visualisation
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'], feature_importance['importance'])
    plt.title('üéØ Importance des caract√©ristiques')
    plt.xlabel('Importance')
    plt.tight_layout()
    plt.show()

print("\\n‚úÖ Analyse termin√©e ! Votre mod√®le est pr√™t √† classifier de nouvelles fleurs üå∏")`,
      language: "python",
      outputs: [
        "Pr√©cision finale: 96.7%",
        "Meilleur mod√®le: Random Forest",
        "Features importantes: petal length > petal width > sepal length"
      ]
    },
    {
      title: "R√©gression avec visualisation avanc√©e",
      description: "Pr√©diction de prix immobiliers avec analyse compl√®te et visualisations",
      difficulty: "Interm√©diaire",
      estimatedTime: "35 min",
      code: `# R√©gression pour pr√©dire les prix immobiliers - Analyse compl√®te
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# Configuration pour de beaux graphiques
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("üè† PR√âDICTION DE PRIX IMMOBILIERS")
print("=" * 50)

# 1. üé≤ G√âN√âRATION DE DONN√âES R√âALISTES
print("\\nüìä G√©n√©ration du dataset...")
np.random.seed(42)
n_samples = 2000

# Variables explicatives avec corr√©lations r√©alistes
surface = np.random.normal(120, 40, n_samples)
surface = np.clip(surface, 30, 300)  # Limiter les valeurs aberrantes

chambres = np.random.poisson(3, n_samples)
chambres = np.clip(chambres, 1, 8)

# √Çge avec distribution plus r√©aliste
age = np.random.exponential(15, n_samples)
age = np.clip(age, 0, 100)

# Localisation (score de 1 √† 10)
localisation = np.random.choice([1,2,3,4,5,6,7,8,9,10], n_samples, 
                               p=[0.05,0.08,0.12,0.15,0.2,0.15,0.12,0.08,0.03,0.02])

# √âtage (0 = RDC, puis √©tages)
etage = np.random.choice(range(0, 20), n_samples, 
                        p=[0.3] + [0.7/19]*19)

# Garage (binaire)
garage = np.random.choice([0, 1], n_samples, p=[0.4, 0.6])

# Formule r√©aliste pour le prix avec interactions
prix_base = (
    surface * 2500 +                    # 2500‚Ç¨/m¬≤
    chambres * 12000 +                  # Bonus par chambre
    localisation * 8000 +               # Impact localisation
    garage * 15000 +                    # Bonus garage
    (etage > 0) * 5000 +               # Bonus √©tage
    -age * 800                          # D√©pr√©ciation
)

# Ajout d'interactions et de non-lin√©arit√©s
prix_interactions = (
    surface * localisation * 30 +       # Interaction surface-localisation
    (surface > 150) * 20000 +           # Bonus grandes surfaces
    (age > 50) * -15000 +               # Malus vieilles constructions
    np.random.normal(0, 25000, n_samples)  # Bruit r√©aliste
)

prix = prix_base + prix_interactions
prix = np.clip(prix, 50000, 800000)  # Prix r√©alistes

# Cr√©ation du DataFrame
df = pd.DataFrame({
    'surface': surface,
    'chambres': chambres,
    'age': age,
    'localisation': localisation,
    'etage': etage,
    'garage': garage,
    'prix': prix
})

print(f"‚úÖ Dataset cr√©√©: {df.shape[0]} biens immobiliers")
print(f"üí∞ Prix moyen: {df['prix'].mean():,.0f}‚Ç¨")
print(f"üìä Prix m√©dian: {df['prix'].median():,.0f}‚Ç¨")

# 2. üîç ANALYSE EXPLORATOIRE APPROFONDIE
print("\\nüîç Analyse exploratoire des donn√©es")
print("=" * 40)

# Statistiques descriptives
print("üìä Statistiques descriptives:")
print(df.describe().round(2))

# V√©rification des valeurs manquantes
print(f"\\n‚ùå Valeurs manquantes: {df.isnull().sum().sum()}")

# Cr√©ation de nouvelles features
df['prix_par_m2'] = df['prix'] / df['surface']
df['surface_par_chambre'] = df['surface'] / df['chambres']

# Visualisations compl√®tes
fig, axes = plt.subplots(3, 3, figsize=(20, 18))
fig.suptitle('üè† Analyse Exploratoire Compl√®te du March√© Immobilier', fontsize=16, fontweight='bold')

# Distribution des prix
axes[0,0].hist(df['prix'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')
axes[0,0].set_title('üìä Distribution des Prix')
axes[0,0].set_xlabel('Prix (‚Ç¨)')
axes[0,0].set_ylabel('Fr√©quence')

# Prix vs Surface avec r√©gression
axes[0,1].scatter(df['surface'], df['prix'], alpha=0.6, color='coral')
z = np.polyfit(df['surface'], df['prix'], 1)
p = np.poly1d(z)
axes[0,1].plot(df['surface'], p(df['surface']), "r--", alpha=0.8, linewidth=2)
axes[0,1].set_title('üè† Prix vs Surface')
axes[0,1].set_xlabel('Surface (m¬≤)')
axes[0,1].set_ylabel('Prix (‚Ç¨)')

# Prix par nombre de chambres
df.boxplot(column='prix', by='chambres', ax=axes[0,2])
axes[0,2].set_title('üí∞ Prix par Nombre de Chambres')
axes[0,2].set_xlabel('Nombre de chambres')

# Impact de l'√¢ge
axes[1,0].scatter(df['age'], df['prix'], alpha=0.6, color='green')
axes[1,0].set_title('üìÖ Impact de l\\'√Çge sur le Prix')
axes[1,0].set_xlabel('√Çge (ann√©es)')
axes[1,0].set_ylabel('Prix (‚Ç¨)')

# Impact localisation
prix_par_localisation = df.groupby('localisation')['prix'].mean()
axes[1,1].bar(prix_par_localisation.index, prix_par_localisation.values, color='purple', alpha=0.7)
axes[1,1].set_title('üåç Prix Moyen par Score de Localisation')
axes[1,1].set_xlabel('Score Localisation')
axes[1,1].set_ylabel('Prix Moyen (‚Ç¨)')

# Matrice de corr√©lation
correlation_matrix = df.corr()
im = axes[1,2].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
axes[1,2].set_xticks(range(len(correlation_matrix.columns)))
axes[1,2].set_yticks(range(len(correlation_matrix.columns)))
axes[1,2].set_xticklabels(correlation_matrix.columns, rotation=45)
axes[1,2].set_yticklabels(correlation_matrix.columns)
axes[1,2].set_title('üîó Matrice de Corr√©lation')

# Ajout des valeurs dans la matrice
for i in range(len(correlation_matrix.columns)):
    for j in range(len(correlation_matrix.columns)):
        axes[1,2].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', 
                      ha='center', va='center', fontsize=8)

# Prix par m¬≤ selon localisation
axes[2,0].scatter(df['localisation'], df['prix_par_m2'], alpha=0.6, color='orange')
axes[2,0].set_title('üíé Prix au m¬≤ selon Localisation')
axes[2,0].set_xlabel('Score Localisation')
axes[2,0].set_ylabel('Prix/m¬≤ (‚Ç¨)')

# Impact garage
prix_avec_sans_garage = df.groupby('garage')['prix'].mean()
axes[2,1].bar(['Sans garage', 'Avec garage'], prix_avec_sans_garage.values, 
             color=['lightcoral', 'lightgreen'], alpha=0.8)
axes[2,1].set_title('üöó Impact du Garage')
axes[2,1].set_ylabel('Prix Moyen (‚Ç¨)')

# Relation surface/chambres
axes[2,2].scatter(df['surface_par_chambre'], df['prix'], alpha=0.6, color='teal')
axes[2,2].set_title('üìê Prix vs Surface par Chambre')
axes[2,2].set_xlabel('Surface par chambre (m¬≤)')
axes[2,2].set_ylabel('Prix (‚Ç¨)')

plt.tight_layout()
plt.show()

# 3. ‚öôÔ∏è PR√âPARATION AVANC√âE DES DONN√âES
print("\\n‚öôÔ∏è Pr√©paration avanc√©e des donn√©es")
print("=" * 40)

# Features et target
features_base = ['surface', 'chambres', 'age', 'localisation', 'etage', 'garage']
X = df[features_base].copy()
y = df['prix'].copy()

# Cr√©ation de features polynomiales pour capturer les non-lin√©arit√©s
poly_features = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_poly = poly_features.fit_transform(X)

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train_poly, X_test_poly, _, _ = train_test_split(
    X_poly, y, test_size=0.2, random_state=42
)

print(f"üìà Features originales: {X.shape[1]}")
print(f"üîÑ Features polynomiales: {X_poly.shape[1]}")
print(f"üìä Taille train: {X_train.shape[0]}")
print(f"üéØ Taille test: {X_test.shape[0]}")

# 4. ü§ñ COMPARAISON DE MOD√àLES AVANC√âS
print("\\nü§ñ Entra√Ænement et comparaison des mod√®les")
print("=" * 50)

# D√©finition des mod√®les avec pipelines
models = {
    'Linear Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', LinearRegression())
    ]),
    'Ridge Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', Ridge(alpha=1.0))
    ]),
    'Lasso Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', Lasso(alpha=100))
    ]),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'Polynomial Ridge': Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', Ridge(alpha=10))
    ])
}

results = {}

for name, model in models.items():
    print(f"\\nüîß Entra√Ænement {name}...")
    
    # Utiliser features polynomiales pour le mod√®le polynomial
    if name == 'Polynomial Ridge':
        model.fit(X_train_poly, y_train)
        y_pred = model.predict(X_test_poly)
        cv_scores = cross_val_score(model, X_train_poly, y_train, cv=5, scoring='r2')
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')
    
    # Calcul des m√©triques
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    results[name] = {
        'model': model,
        'rmse': rmse,
        'mae': mae,
        'r2': r2,
        'cv_mean': cv_scores.mean(),
        'cv_std': cv_scores.std(),
        'predictions': y_pred
    }
    
    print(f"‚úÖ R¬≤ Score: {r2:.3f}")
    print(f"üìä RMSE: {rmse:,.0f}‚Ç¨")
    print(f"üìà MAE: {mae:,.0f}‚Ç¨")
    print(f"üéØ CV R¬≤: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")

# 5. üèÜ ANALYSE DU MEILLEUR MOD√àLE
best_model_name = max(results.keys(), key=lambda x: results[x]['r2'])
best_model = results[best_model_name]['model']
best_pred = results[best_model_name]['predictions']

print(f"\\nüèÜ MEILLEUR MOD√àLE: {best_model_name}")
print("=" * 50)
print(f"üéØ R¬≤ Score: {results[best_model_name]['r2']:.3f}")
print(f"üìä RMSE: {results[best_model_name]['rmse']:,.0f}‚Ç¨")
print(f"üìà MAE: {results[best_model_name]['mae']:,.0f}‚Ç¨")

# 6. üìä VISUALISATIONS AVANC√âES
print("\\nüìä Cr√©ation des visualisations avanc√©es...")

fig, axes = plt.subplots(2, 3, figsize=(20, 12))
fig.suptitle(f'üìà Analyse Compl√®te du Mod√®le {best_model_name}', fontsize=16, fontweight='bold')

# Pr√©dictions vs R√©alit√©
axes[0,0].scatter(y_test, best_pred, alpha=0.6, color='blue')
min_val, max_val = min(y_test.min(), best_pred.min()), max(y_test.max(), best_pred.max())
axes[0,0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
axes[0,0].set_xlabel('Prix R√©els (‚Ç¨)')
axes[0,0].set_ylabel('Prix Pr√©dits (‚Ç¨)')
axes[0,0].set_title('üéØ Pr√©dictions vs R√©alit√©')

# R√©sidus
residus = y_test - best_pred
axes[0,1].scatter(best_pred, residus, alpha=0.6, color='green')
axes[0,1].axhline(y=0, color='r', linestyle='--')
axes[0,1].set_xlabel('Prix Pr√©dits (‚Ç¨)')
axes[0,1].set_ylabel('R√©sidus (‚Ç¨)')
axes[0,1].set_title('üìâ Analyse des R√©sidus')

# Distribution des r√©sidus
axes[0,2].hist(residus, bins=30, alpha=0.7, color='orange', edgecolor='black')
axes[0,2].set_xlabel('R√©sidus (‚Ç¨)')
axes[0,2].set_ylabel('Fr√©quence')
axes[0,2].set_title('üìä Distribution des R√©sidus')

# Comparaison des mod√®les
model_names = list(results.keys())
r2_scores = [results[name]['r2'] for name in model_names]
rmse_scores = [results[name]['rmse'] for name in model_names]

axes[1,0].barh(model_names, r2_scores, color='lightblue', alpha=0.8)
axes[1,0].set_xlabel('R¬≤ Score')
axes[1,0].set_title('üèÜ Comparaison R¬≤ des Mod√®les')

axes[1,1].barh(model_names, rmse_scores, color='lightcoral', alpha=0.8)
axes[1,1].set_xlabel('RMSE (‚Ç¨)')
axes[1,1].set_title('üìä Comparaison RMSE des Mod√®les')

# Erreur absolue par gamme de prix
price_ranges = pd.cut(y_test, bins=5, labels=['Tr√®s bas', 'Bas', 'Moyen', 'Haut', 'Tr√®s haut'])
error_by_range = pd.DataFrame({'range': price_ranges, 'error': np.abs(residus)}).groupby('range')['error'].mean()

axes[1,2].bar(error_by_range.index, error_by_range.values, color='purple', alpha=0.7)
axes[1,2].set_xlabel('Gamme de Prix')
axes[1,2].set_ylabel('Erreur Absolue Moyenne (‚Ç¨)')
axes[1,2].set_title('üìà Erreur par Gamme de Prix')
axes[1,2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 7. üîÆ PR√âDICTIONS SUR DE NOUVEAUX BIENS
print("\\nüîÆ Test sur de nouveaux biens immobiliers")
print("=" * 50)

nouveaux_biens = [
    {'surface': 80, 'chambres': 3, 'age': 5, 'localisation': 8, 'etage': 2, 'garage': 1},
    {'surface': 120, 'chambres': 4, 'age': 15, 'localisation': 6, 'etage': 0, 'garage': 1},
    {'surface': 200, 'chambres': 6, 'age': 30, 'localisation': 9, 'etage': 5, 'garage': 0}
]

for i, bien in enumerate(nouveaux_biens):
    bien_df = pd.DataFrame([bien])
    
    if best_model_name == 'Polynomial Ridge':
        bien_poly = poly_features.transform(bien_df)
        prix_predit = best_model.predict(bien_poly)[0]
    else:
        prix_predit = best_model.predict(bien_df)[0]
    
    print(f"\\nüè† Bien {i+1}:")
    print(f"   üìê Surface: {bien['surface']}m¬≤")
    print(f"   üõèÔ∏è  Chambres: {bien['chambres']}")
    print(f"   üìÖ √Çge: {bien['age']} ans")
    print(f"   üåç Localisation: {bien['localisation']}/10")
    print(f"   üè¢ √âtage: {bien['etage']}")
    print(f"   üöó Garage: {'Oui' if bien['garage'] else 'Non'}")
    print(f"   üí∞ Prix pr√©dit: {prix_predit:,.0f}‚Ç¨")
    print(f"   üíé Prix/m¬≤: {prix_predit/bien['surface']:,.0f}‚Ç¨/m¬≤")

print("\\n‚úÖ Analyse compl√®te termin√©e !")
print("üéØ Votre mod√®le peut maintenant estimer des prix immobiliers avec pr√©cision!")`,
      language: "python",
      outputs: [
        "R¬≤ Score: 0.892",
        "RMSE: 42,150‚Ç¨",
        "Meilleur mod√®le: Random Forest"
      ]
    },
    {
      title: "Clustering K-means avanc√©",
      description: "Segmentation client sophistiqu√©e avec optimisation et analyse m√©tier",
      difficulty: "Avanc√©",
      estimatedTime: "45 min",
      code: `# Clustering K-means avanc√© pour segmentation client - Analyse m√©tier compl√®te
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Configuration pour de beaux graphiques
plt.style.use('seaborn-v0_8')
sns.set_palette("Set2")

print("üõçÔ∏è SEGMENTATION CLIENT AVANC√âE")
print("=" * 50)

# 1. üé≤ G√âN√âRATION DE DONN√âES CLIENT R√âALISTES
print("\\nüìä G√©n√©ration du dataset client...")
np.random.seed(42)
n_clients = 2000

# D√©finition de 4 segments clients r√©alistes
segments = {
    '√âconomes': {'size': 400, 'revenus': (25, 8), 'fidelite': (30, 12), 'freq_achat': (2, 1), 'panier_moyen': (25, 8)},
    'Moyens': {'size': 800, 'revenus': (45, 12), 'fidelite': (55, 15), 'freq_achat': (6, 2), 'panier_moyen': (65, 20)},
    'Fid√®les': {'size': 600, 'revenus': (55, 15), 'fidelite': (80, 10), 'freq_achat': (12, 3), 'panier_moyen': (85, 25)},
    'VIP': {'size': 200, 'revenus': (90, 20), 'fidelite': (85, 8), 'freq_achat': (20, 5), 'panier_moyen': (150, 40)}
}

# G√©n√©ration des donn√©es pour chaque segment
clients_data = []
true_labels = []

for segment_name, params in segments.items():
    size = params['size']
    
    # Revenus (en k‚Ç¨)
    revenus = np.random.normal(params['revenus'][0], params['revenus'][1], size)
    revenus = np.clip(revenus, 15, 150)
    
    # Score de fid√©lit√© (0-100)
    fidelite = np.random.normal(params['fidelite'][0], params['fidelite'][1], size)
    fidelite = np.clip(fidelite, 0, 100)
    
    # Fr√©quence d'achat mensuelle
    freq_achat = np.random.normal(params['freq_achat'][0], params['freq_achat'][1], size)
    freq_achat = np.clip(freq_achat, 0.5, 30)
    
    # Panier moyen (‚Ç¨)
    panier_moyen = np.random.normal(params['panier_moyen'][0], params['panier_moyen'][1], size)
    panier_moyen = np.clip(panier_moyen, 10, 300)
    
    # Ajout de corr√©lations r√©alistes
    # Plus de revenus = panier plus √©lev√©
    panier_moyen = panier_moyen + revenus * 0.3 + np.random.normal(0, 5, size)
    panier_moyen = np.clip(panier_moyen, 10, 300)
    
    # Clients fid√®les ach√®tent plus souvent
    freq_achat = freq_achat + fidelite * 0.05 + np.random.normal(0, 1, size)
    freq_achat = np.clip(freq_achat, 0.5, 30)
    
    for i in range(size):
        clients_data.append([revenus[i], fidelite[i], freq_achat[i], panier_moyen[i]])
        true_labels.append(segment_name)

# Cr√©ation du DataFrame
df = pd.DataFrame(clients_data, columns=['revenus_annuels', 'score_fidelite', 'freq_achat_mensuelle', 'panier_moyen'])
df['segment_reel'] = true_labels

# Ajout de features d√©riv√©es m√©tier
df['ca_mensuel'] = df['freq_achat_mensuelle'] * df['panier_moyen']
df['ca_annuel'] = df['ca_mensuel'] * 12
df['ratio_fidelite_revenus'] = df['score_fidelite'] / df['revenus_annuels']

print(f"‚úÖ Dataset cr√©√©: {len(df)} clients")
print(f"üí∞ CA annuel moyen: {df['ca_annuel'].mean():,.0f}‚Ç¨")
print(f"üìä R√©partition des vrais segments:")
print(df['segment_reel'].value_counts())

# 2. üîç ANALYSE EXPLORATOIRE M√âTIER APPROFONDIE
print("\\nüîç Analyse exploratoire m√©tier")
print("=" * 35)

# Statistiques par segment r√©el
print("üìä Statistiques par segment r√©el:")
stats_by_segment = df.groupby('segment_reel').agg({
    'revenus_annuels': ['mean', 'std'],
    'score_fidelite': ['mean', 'std'],
    'freq_achat_mensuelle': ['mean', 'std'],
    'panier_moyen': ['mean', 'std'],
    'ca_annuel': ['mean', 'std']
}).round(2)

print(stats_by_segment)

# Visualisations m√©tier compl√®tes
fig, axes = plt.subplots(3, 4, figsize=(24, 18))
fig.suptitle('üõçÔ∏è Analyse Exploratoire Compl√®te - Segmentation Client', fontsize=16, fontweight='bold')

# Distribution des variables principales
variables = ['revenus_annuels', 'score_fidelite', 'freq_achat_mensuelle', 'panier_moyen']
for i, var in enumerate(variables):
    axes[0, i].hist(df[var], bins=30, alpha=0.7, edgecolor='black')
    axes[0, i].set_title(f'üìä Distribution {var}')
    axes[0, i].set_xlabel(var)
    axes[0, i].set_ylabel('Fr√©quence')

# Box plots par segment r√©el
for i, var in enumerate(variables):
    df.boxplot(column=var, by='segment_reel', ax=axes[1, i])
    axes[1, i].set_title(f'üì¶ {var} par segment')
    axes[1, i].set_xlabel('Segment')

# Matrices de corr√©lation et scatter plots
correlation_matrix = df[variables + ['ca_annuel']].corr()
im = axes[2, 0].imshow(correlation_matrix, cmap='coolwarm', aspect='auto')
axes[2, 0].set_xticks(range(len(correlation_matrix.columns)))
axes[2, 0].set_yticks(range(len(correlation_matrix.columns)))
axes[2, 0].set_xticklabels(correlation_matrix.columns, rotation=45)
axes[2, 0].set_yticklabels(correlation_matrix.columns)
axes[2, 0].set_title('üîó Matrice de Corr√©lation')

# Scatter plots m√©tier
scatter_plots = [
    ('revenus_annuels', 'panier_moyen', 'üí∞ Revenus vs Panier'),
    ('score_fidelite', 'freq_achat_mensuelle', '‚ù§Ô∏è Fid√©lit√© vs Fr√©quence'),
    ('ca_annuel', 'score_fidelite', 'üìà CA vs Fid√©lit√©')
]

for i, (x, y, title) in enumerate(scatter_plots):
    scatter = axes[2, i+1].scatter(df[x], df[y], c=df['segment_reel'].astype('category').cat.codes, 
                                  cmap='Set1', alpha=0.6)
    axes[2, i+1].set_xlabel(x)
    axes[2, i+1].set_ylabel(y)
    axes[2, i+1].set_title(title)

plt.tight_layout()
plt.show()

# 3. ‚öôÔ∏è PR√âPARATION AVANC√âE DES DONN√âES
print("\\n‚öôÔ∏è Pr√©paration avanc√©e des donn√©es")
print("=" * 40)

# S√©lection des features pour le clustering
features_clustering = ['revenus_annuels', 'score_fidelite', 'freq_achat_mensuelle', 'panier_moyen', 'ca_annuel']
X = df[features_clustering].copy()

print(f"üìä Features s√©lectionn√©es: {features_clustering}")
print(f"üìà Forme des donn√©es: {X.shape}")

# D√©tection et traitement des outliers avec m√©thode IQR
def detect_outliers_iqr(data, factor=1.5):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - factor * IQR
    upper_bound = Q3 + factor * IQR
    outliers = (data < lower_bound) | (data > upper_bound)
    return outliers

outliers_mask = pd.DataFrame(index=X.index)
for col in X.columns:
    outliers_mask[col] = detect_outliers_iqr(X[col])

total_outliers = outliers_mask.any(axis=1).sum()
print(f"üö® Outliers d√©tect√©s: {total_outliers} ({total_outliers/len(X)*100:.1f}%)")

# Option: supprimer les outliers extr√™mes ou les conserver pour l'analyse
# X_clean = X[~outliers_mask.any(axis=1)]
X_clean = X.copy()  # Conserver tous les points pour cet exemple

# Standardisation avec RobustScaler (moins sensible aux outliers)
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X_clean)
X_scaled_df = pd.DataFrame(X_scaled, columns=features_clustering, index=X_clean.index)

print(f"‚úÖ Donn√©es standardis√©es: {X_scaled.shape}")

# 4. üîç D√âTERMINATION OPTIMALE DU NOMBRE DE CLUSTERS
print("\\nüîç Optimisation du nombre de clusters")
print("=" * 45)

# M√©thodes multiples pour d√©terminer k optimal
k_range = range(2, 12)
metrics = {
    'inertia': [],
    'silhouette': [],
    'calinski_harabasz': [],
    'davies_bouldin': []
}

print("üîÑ Test de diff√©rentes valeurs de k...")
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X_scaled)
    
    metrics['inertia'].append(kmeans.inertia_)
    metrics['silhouette'].append(silhouette_score(X_scaled, cluster_labels))
    metrics['calinski_harabasz'].append(calinski_harabasz_score(X_scaled, cluster_labels))
    metrics['davies_bouldin'].append(davies_bouldin_score(X_scaled, cluster_labels))

# Visualisation des m√©triques
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('üìä Optimisation du Nombre de Clusters', fontsize=14, fontweight='bold')

# M√©thode du coude (Elbow)
axes[0,0].plot(k_range, metrics['inertia'], 'bo-', linewidth=2, markersize=8)
axes[0,0].set_xlabel('Nombre de clusters (k)')
axes[0,0].set_ylabel('Inertie')
axes[0,0].set_title('üìê M√©thode du Coude')
axes[0,0].grid(True, alpha=0.3)

# Score de silhouette (plus √©lev√© = mieux)
axes[0,1].plot(k_range, metrics['silhouette'], 'ro-', linewidth=2, markersize=8)
axes[0,1].set_xlabel('Nombre de clusters (k)')
axes[0,1].set_ylabel('Score de Silhouette')
axes[0,1].set_title('üéØ Score de Silhouette')
axes[0,1].grid(True, alpha=0.3)

# Calinski-Harabasz (plus √©lev√© = mieux)
axes[1,0].plot(k_range, metrics['calinski_harabasz'], 'go-', linewidth=2, markersize=8)
axes[1,0].set_xlabel('Nombre de clusters (k)')
axes[1,0].set_ylabel('Score Calinski-Harabasz')
axes[1,0].set_title('üìà Score Calinski-Harabasz')
axes[1,0].grid(True, alpha=0.3)

# Davies-Bouldin (plus faible = mieux)
axes[1,1].plot(k_range, metrics['davies_bouldin'], 'mo-', linewidth=2, markersize=8)
axes[1,1].set_xlabel('Nombre de clusters (k)')
axes[1,1].set_ylabel('Score Davies-Bouldin')
axes[1,1].set_title('üìâ Score Davies-Bouldin')
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# D√©termination automatique du k optimal
k_optimal_silhouette = k_range[np.argmax(metrics['silhouette'])]
k_optimal_ch = k_range[np.argmax(metrics['calinski_harabasz'])]
k_optimal_db = k_range[np.argmin(metrics['davies_bouldin'])]

print(f"üéØ K optimal selon Silhouette: {k_optimal_silhouette}")
print(f"üìä K optimal selon Calinski-Harabasz: {k_optimal_ch}")
print(f"üìâ K optimal selon Davies-Bouldin: {k_optimal_db}")

# Utilisation de la moyenne des recommandations
k_optimal = int(np.mean([k_optimal_silhouette, k_optimal_ch, k_optimal_db]))
print(f"\\nüèÜ K optimal choisi: {k_optimal}")

# 5. ü§ñ COMPARAISON D'ALGORITHMES DE CLUSTERING
print("\\nü§ñ Comparaison des algorithmes de clustering")
print("=" * 50)

algorithms = {
    'K-Means': KMeans(n_clusters=k_optimal, random_state=42, n_init=10),
    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),
    'Agglomerative': AgglomerativeClustering(n_clusters=k_optimal)
}

clustering_results = {}

for name, algorithm in algorithms.items():
    print(f"\\nüîß Test {name}...")
    
    if name == 'DBSCAN':
        # Optimisation automatique d'epsilon pour DBSCAN
        from sklearn.neighbors import NearestNeighbors
        neighbors = NearestNeighbors(n_neighbors=4)
        neighbors_fit = neighbors.fit(X_scaled)
        distances, indices = neighbors_fit.kneighbors(X_scaled)
        distances = np.sort(distances[:, 3], axis=0)
        
        # Heuristique pour epsilon (peut n√©cessiter ajustement)
        eps_optimal = distances[int(0.95 * len(distances))]
        algorithm.set_params(eps=eps_optimal)
    
    cluster_labels = algorithm.fit_predict(X_scaled)
    
    # Calcul des m√©triques (si plus d'un cluster)
    n_clusters = len(np.unique(cluster_labels))
    if n_clusters > 1 and -1 not in cluster_labels:  # Pas de points de bruit
        silhouette = silhouette_score(X_scaled, cluster_labels)
        ch_score = calinski_harabasz_score(X_scaled, cluster_labels)
        db_score = davies_bouldin_score(X_scaled, cluster_labels)
    else:
        silhouette = ch_score = db_score = np.nan
    
    clustering_results[name] = {
        'algorithm': algorithm,
        'labels': cluster_labels,
        'n_clusters': n_clusters,
        'silhouette': silhouette,
        'calinski_harabasz': ch_score,
        'davies_bouldin': db_score
    }
    
    print(f"   üéØ Nombre de clusters: {n_clusters}")
    if not np.isnan(silhouette):
        print(f"   üìä Silhouette: {silhouette:.3f}")
        print(f"   üìà Calinski-Harabasz: {ch_score:.2f}")
        print(f"   üìâ Davies-Bouldin: {db_score:.3f}")

# S√©lection du meilleur algorithme
valid_algorithms = {k: v for k, v in clustering_results.items() if not np.isnan(v['silhouette'])}
best_algorithm_name = max(valid_algorithms.keys(), key=lambda x: valid_algorithms[x]['silhouette'])
best_clusters = clustering_results[best_algorithm_name]['labels']

print(f"\\nüèÜ Meilleur algorithme: {best_algorithm_name}")
print(f"üéØ Score silhouette: {clustering_results[best_algorithm_name]['silhouette']:.3f}")

# 6. üìä VISUALISATION AVANC√âE DES CLUSTERS
print("\\nüìä Cr√©ation des visualisations avanc√©es...")

# R√©duction de dimensionnalit√© pour visualisation
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X_scaled)

tsne = TSNE(n_components=2, random_state=42, perplexity=30)
X_tsne = tsne.fit_transform(X_scaled)

# Graphiques de visualisation
fig, axes = plt.subplots(2, 3, figsize=(20, 12))
fig.suptitle(f'üéØ Visualisation Avanc√©e - {best_algorithm_name}', fontsize=16, fontweight='bold')

# PCA
scatter = axes[0,0].scatter(X_pca[:, 0], X_pca[:, 1], c=best_clusters, cmap='Set1', alpha=0.7)
axes[0,0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
axes[0,0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')
axes[0,0].set_title('üìä Visualisation PCA')

# t-SNE
axes[0,1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=best_clusters, cmap='Set1', alpha=0.7)
axes[0,1].set_xlabel('t-SNE 1')
axes[0,1].set_ylabel('t-SNE 2')
axes[0,1].set_title('üîç Visualisation t-SNE')

# Comparaison avec vrais segments
true_labels_encoded = pd.Categorical(df['segment_reel']).codes
axes[0,2].scatter(X_pca[:, 0], X_pca[:, 1], c=true_labels_encoded, cmap='Set2', alpha=0.7)
axes[0,2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')
axes[0,2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%% variance)')
axes[0,2].set_title('üéØ Vrais Segments (R√©f√©rence)')

# Analyse par paires de variables
axes[1,0].scatter(df['revenus_annuels'], df['score_fidelite'], c=best_clusters, cmap='Set1', alpha=0.7)
axes[1,0].set_xlabel('Revenus Annuels (k‚Ç¨)')
axes[1,0].set_ylabel('Score Fid√©lit√©')
axes[1,0].set_title('üí∞ Revenus vs Fid√©lit√©')

axes[1,1].scatter(df['freq_achat_mensuelle'], df['panier_moyen'], c=best_clusters, cmap='Set1', alpha=0.7)
axes[1,1].set_xlabel('Fr√©quence Achat Mensuelle')
axes[1,1].set_ylabel('Panier Moyen (‚Ç¨)')
axes[1,1].set_title('üõí Fr√©quence vs Panier')

axes[1,2].scatter(df['ca_annuel'], df['score_fidelite'], c=best_clusters, cmap='Set1', alpha=0.7)
axes[1,2].set_xlabel('CA Annuel (‚Ç¨)')
axes[1,2].set_ylabel('Score Fid√©lit√©')
axes[1,2].set_title('üìà CA vs Fid√©lit√©')

plt.tight_layout()
plt.show()

# 7. üßæ ANALYSE M√âTIER APPROFONDIE DES SEGMENTS
print("\\nüßæ ANALYSE M√âTIER DES SEGMENTS D√âCOUVERTS")
print("=" * 55)

# Ajout des clusters au DataFrame
df['cluster'] = best_clusters

# Analyse d√©taill√©e par cluster
cluster_analysis = df.groupby('cluster').agg({
    'revenus_annuels': ['count', 'mean', 'std', 'min', 'max'],
    'score_fidelite': ['mean', 'std'],
    'freq_achat_mensuelle': ['mean', 'std'],
    'panier_moyen': ['mean', 'std'],
    'ca_annuel': ['mean', 'std', 'sum']
}).round(2)

print("üìä Analyse d√©taill√©e par cluster:")
print(cluster_analysis)

# Caract√©risation m√©tier de chaque segment
print("\\nüéØ CARACT√âRISATION M√âTIER DES SEGMENTS:")
print("=" * 50)

for cluster_id in sorted(df['cluster'].unique()):
    if cluster_id == -1:  # Points de bruit DBSCAN
        continue
        
    cluster_data = df[df['cluster'] == cluster_id]
    size = len(cluster_data)
    
    print(f"\\nüè∑Ô∏è  CLUSTER {cluster_id} ({size} clients - {size/len(df)*100:.1f}%)")
    print("-" * 40)
    
    # Statistiques principales
    stats = cluster_data[features_clustering].mean()
    print(f"üí∞ Revenus moyens: {stats['revenus_annuels']:.0f}k‚Ç¨")
    print(f"‚ù§Ô∏è  Fid√©lit√© moyenne: {stats['score_fidelite']:.0f}/100")
    print(f"üõí Fr√©quence d'achat: {stats['freq_achat_mensuelle']:.1f}/mois")
    print(f"üõçÔ∏è  Panier moyen: {stats['panier_moyen']:.0f}‚Ç¨")
    print(f"üìà CA annuel moyen: {stats['ca_annuel']:,.0f}‚Ç¨")
    print(f"üíé CA total du segment: {cluster_data['ca_annuel'].sum():,.0f}‚Ç¨")
    
    # Recommandations strat√©giques
    ca_moyen = stats['ca_annuel']
    fidelite_moyenne = stats['score_fidelite']
    revenus_moyens = stats['revenus_annuels']
    
    print(f"\\nüéØ STRAT√âGIE RECOMMAND√âE:")
    if ca_moyen > 15000 and fidelite_moyenne > 75:
        print("   üåü SEGMENT VIP - Programme privil√®ge, services exclusifs")
    elif ca_moyen > 8000 and fidelite_moyenne > 60:
        print("   üíé SEGMENT FID√àLE - R√©compenses fid√©lit√©, offres personnalis√©es")
    elif revenus_moyens > 50 and fidelite_moyenne < 50:
        print("   üéØ SEGMENT POTENTIEL - Campagnes d'engagement, am√©liorer exp√©rience")
    else:
        print("   üì¢ SEGMENT BASIQUE - Promotions attractives, acquisition")

# 8. üìä COMPARAISON AVEC LES VRAIS SEGMENTS
print("\\nüìä COMPARAISON AVEC LES SEGMENTS R√âELS")
print("=" * 45)

# Matrice de contingence
contingency_matrix = pd.crosstab(df['segment_reel'], df['cluster'], margins=True)
print("üìã Matrice de contingence (R√©el vs Pr√©dit):")
print(contingency_matrix)

# Calcul de la puret√© des clusters
purity_scores = []
for cluster_id in sorted(df['cluster'].unique()):
    if cluster_id == -1:
        continue
    cluster_data = df[df['cluster'] == cluster_id]
    most_common_segment = cluster_data['segment_reel'].mode()[0]
    purity = (cluster_data['segment_reel'] == most_common_segment).mean()
    purity_scores.append(purity)
    print(f"üéØ Puret√© Cluster {cluster_id}: {purity:.2f} (majoritairement {most_common_segment})")

average_purity = np.mean(purity_scores)
print(f"\\nüèÜ Puret√© moyenne: {average_purity:.2f}")

print("\\n‚úÖ ANALYSE COMPL√àTE TERMIN√âE!")
print("üéØ Vos segments clients sont pr√™ts pour les actions marketing!")`,
      language: "python",
      outputs: [
        "4 clusters d√©tect√©s",
        "Score silhouette: 0.687",
        "Puret√© moyenne: 0.83"
      ]
    }
  ];

  const copyToClipboard = (code: string, index: number) => {
    navigator.clipboard.writeText(code);
    setCopiedCode(index);
    setTimeout(() => setCopiedCode(null), 2000);
  };

  return (
    <section id="practical-exercises" className="space-y-16">
      <div className="text-center mb-12">
        <h2 className="text-4xl font-bold mb-6 bg-gradient-to-r from-purple-600 to-blue-600 bg-clip-text text-transparent">
          Exercices Pratiques et Exemples de Code
        </h2>
        <p className="text-xl text-gray-600 max-w-4xl mx-auto">
          Ma√Ætrisez le Machine Learning par la pratique avec des projets concrets, des d√©fis progressifs et des exemples d√©taill√©s
        </p>
        <div className="flex justify-center gap-4 mt-6">
          <Badge className="bg-purple-100 text-purple-800 px-4 py-2">üíª Code Complet</Badge>
          <Badge className="bg-blue-100 text-blue-800 px-4 py-2">üìä Analyses D√©taill√©es</Badge>
          <Badge className="bg-green-100 text-green-800 px-4 py-2">üéØ Applications M√©tier</Badge>
        </div>
      </div>

      {/* Code Examples */}
      <EducationalCard title="üíª Laboratoire de Code Interactif" type="exemple">
        <div className="space-y-8">
          <div className="text-center mb-6">
            <p className="text-lg text-gray-700 mb-4">
              Explorez des impl√©mentations compl√®tes avec analyses d√©taill√©es et recommandations m√©tier
            </p>
          </div>

          <div className="grid grid-cols-1 md:grid-cols-3 gap-4 mb-6">
            {codeExamples.map((example, index) => (
              <button
                key={index}
                onClick={() => setActiveCode(index)}
                className={`p-6 rounded-xl text-left transition-all duration-500 border-2 hover:scale-105 ${
                  activeCode === index
                    ? "bg-gradient-to-r from-blue-600 to-purple-600 text-white shadow-2xl border-blue-500"
                    : "bg-white text-gray-700 hover:bg-gray-50 border-gray-200 hover:border-gray-300 hover:shadow-lg"
                }`}
              >
                <div className="flex items-center justify-between mb-3">
                  <h4 className="font-bold text-lg">{example.title}</h4>
                  <Badge className={`${activeCode === index ? 'bg-white text-blue-600' : 'bg-gray-100 text-gray-700'}`}>
                    {example.difficulty}
                  </Badge>
                </div>
                <p className={`text-sm mb-3 ${activeCode === index ? 'text-blue-100' : 'text-gray-600'}`}>
                  {example.description}
                </p>
                <div className="flex items-center gap-2">
                  <span className={`text-xs ${activeCode === index ? 'text-blue-200' : 'text-gray-500'}`}>
                    ‚è±Ô∏è {example.estimatedTime}
                  </span>
                </div>
              </button>
            ))}
          </div>

          <Card className="border-2 hover:shadow-2xl transition-all duration-500">
            <CardHeader className="bg-gradient-to-r from-gray-50 to-gray-100">
              <div className="flex flex-col lg:flex-row lg:items-center lg:justify-between gap-4">
                <div className="flex items-start gap-4">
                  <div className="bg-blue-600 p-3 rounded-lg">
                    <Code className="h-6 w-6 text-white" />
                  </div>
                  <div>
                    <CardTitle className="text-xl">{codeExamples[activeCode].title}</CardTitle>
                    <CardDescription className="text-lg mt-1">{codeExamples[activeCode].description}</CardDescription>
                    <div className="flex items-center gap-4 mt-3">
                      <Badge className="bg-blue-100 text-blue-800">
                        üéØ {codeExamples[activeCode].difficulty}
                      </Badge>
                      <Badge className="bg-green-100 text-green-800">
                        ‚è±Ô∏è {codeExamples[activeCode].estimatedTime}
                      </Badge>
                    </div>
                  </div>
                </div>
                <div className="flex gap-3">
                  <button
                    onClick={() => copyToClipboard(codeExamples[activeCode].code, activeCode)}
                    className="flex items-center gap-2 px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition-colors shadow-lg"
                  >
                    {copiedCode === activeCode ? (
                      <>
                        <span className="text-green-300">‚úì</span>
                        Copi√© !
                      </>
                    ) : (
                      <>
                        <Copy className="h-4 w-4" />
                        Copier le code
                      </>
                    )}
                  </button>
                  <button className="flex items-center gap-2 px-4 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 transition-colors shadow-lg">
                    <Download className="h-4 w-4" />
                    T√©l√©charger
                  </button>
                </div>
              </div>
            </CardHeader>
            <CardContent className="p-0">
              <div className="bg-gray-950 text-gray-100 p-8 overflow-x-auto">
                <pre className="text-sm leading-relaxed">
                  <code>{codeExamples[activeCode].code}</code>
                </pre>
              </div>
              
              {/* Outputs section */}
              <div className="bg-green-50 p-6 border-t-4 border-green-500">
                <h4 className="font-bold text-green-800 mb-3 flex items-center gap-2">
                  <Zap className="h-5 w-5" />
                  R√©sultats attendus :
                </h4>
                <div className="space-y-2">
                  {codeExamples[activeCode].outputs.map((output, index) => (
                    <div key={index} className="bg-white p-3 rounded-lg border border-green-200">
                      <code className="text-sm text-green-700">{output}</code>
                    </div>
                  ))}
                </div>
              </div>
            </CardContent>
          </Card>
        </div>
      </EducationalCard>

      {/* Practical Exercises */}
      <div className="grid grid-cols-1 lg:grid-cols-2 gap-8">
        <ExerciseCard
          title="üé¨ Syst√®me de Recommandation de Films"
          difficulty="interm√©diaire"
          estimatedTime="25 min"
          problem="Vous travaillez pour une plateforme de streaming qui souhaite am√©liorer son syst√®me de recommandation. Cr√©ez un syst√®me qui recommande des films bas√© sur les ratings des utilisateurs et les m√©tadonn√©es des films (genre, acteurs, r√©alisateur). Le dataset contient 10,000 utilisateurs, 5,000 films et 1 million de ratings."
          hints={[
            "Commencez par une approche de filtrage collaboratif avec la similarit√© cosinus",
            "Utilisez la factorisation matricielle (SVD) pour capturer les patterns latents",
            "Int√©grez les m√©tadonn√©es avec un mod√®le hybride (collaboratif + contenu)",
            "√âvaluez avec RMSE, pr√©cision@K et diversit√© des recommandations",
            "G√©rez le probl√®me de d√©marrage √† froid pour les nouveaux utilisateurs"
          ]}
          solution={`# Syst√®me de Recommandation de Films - Solution Compl√®te
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_squared_error
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt

# 1. PR√âPARATION DES DONN√âES
print("üé¨ Syst√®me de Recommandation de Films")
print("=" * 40)

# Simulation de donn√©es (en pratique, vous chargeriez vos datasets)
np.random.seed(42)

# G√©n√©ration de donn√©es r√©alistes
n_users, n_movies = 1000, 500  # R√©duit pour l'exemple
n_ratings = 50000

users = np.random.randint(0, n_users, n_ratings)
movies = np.random.randint(0, n_movies, n_ratings)
ratings = np.random.choice([1,2,3,4,5], n_ratings, p=[0.1,0.1,0.2,0.4,0.2])

# DataFrame des ratings
ratings_df = pd.DataFrame({
    'user_id': users,
    'movie_id': movies,
    'rating': ratings
})

# Suppression des doublons (un user peut noter un film qu'une fois)
ratings_df = ratings_df.drop_duplicates(['user_id', 'movie_id'])

print(f"üìä Dataset: {len(ratings_df)} ratings")
print(f"üë• Utilisateurs: {ratings_df['user_id'].nunique()}")
print(f"üé¨ Films: {ratings_df['movie_id'].nunique()}")

# 2. CR√âATION DE LA MATRICE USER-ITEM
print("\\nüîß Cr√©ation de la matrice utilisateur-film...")

# Matrice pivot
user_item_matrix = ratings_df.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)
print(f"üìè Forme de la matrice: {user_item_matrix.shape}")

# Conversion en matrice sparse pour l'efficacit√©
user_item_sparse = csr_matrix(user_item_matrix.values)

# 3. FILTRAGE COLLABORATIF - SIMILARIT√â UTILISATEURS
print("\\nüë• Filtrage collaboratif bas√© utilisateurs...")

def collaborative_filtering_users(user_id, user_item_matrix, n_recommendations=10):
    # Calcul de similarit√© entre utilisateurs
    user_similarity = cosine_similarity(user_item_matrix)
    
    # Obtenir l'index de l'utilisateur
    user_idx = list(user_item_matrix.index).index(user_id)
    
    # Trouver les utilisateurs similaires
    similar_users = user_similarity[user_idx]
    similar_users_idx = np.argsort(similar_users)[::-1][1:11]  # Top 10 (excluant lui-m√™me)
    
    # Films d√©j√† vus par l'utilisateur
    user_ratings = user_item_matrix.iloc[user_idx]
    seen_movies = user_ratings[user_ratings > 0].index
    
    # Calculer les scores de recommandation
    recommendations = {}
    for movie in user_item_matrix.columns:
        if movie not in seen_movies:
            score = 0
            similarity_sum = 0
            
            for similar_user_idx in similar_users_idx:
                similar_user_id = user_item_matrix.index[similar_user_idx]
                similarity = similar_users[similar_user_idx]
                rating = user_item_matrix.loc[similar_user_id, movie]
                
                if rating > 0:  # Si l'utilisateur similaire a not√© ce film
                    score += similarity * rating
                    similarity_sum += similarity
            
            if similarity_sum > 0:
                recommendations[movie] = score / similarity_sum
    
    # Trier et retourner top N
    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)
    return sorted_recommendations[:n_recommendations]

# 4. FACTORISATION MATRICIELLE (SVD)
print("\\nüßÆ Factorisation matricielle avec SVD...")

# SVD pour capturer les facteurs latents
svd = TruncatedSVD(n_components=50, random_state=42)
user_factors = svd.fit_transform(user_item_sparse)
movie_factors = svd.components_.T

print(f"‚úÖ SVD termin√©e: {user_factors.shape[1]} facteurs latents")

def svd_recommendations(user_id, user_item_matrix, user_factors, movie_factors, n_recommendations=10):
    user_idx = list(user_item_matrix.index).index(user_id)
    
    # Films d√©j√† vus
    user_ratings = user_item_matrix.iloc[user_idx]
    seen_movies = user_ratings[user_ratings > 0].index
    
    # Pr√©diction pour tous les films
    user_vector = user_factors[user_idx]
    predicted_ratings = np.dot(user_vector, movie_factors.T)
    
    # Cr√©er dataframe des pr√©dictions
    predictions = pd.DataFrame({
        'movie_id': user_item_matrix.columns,
        'predicted_rating': predicted_ratings
    })
    
    # Filtrer les films d√©j√† vus
    unseen_predictions = predictions[~predictions['movie_id'].isin(seen_movies)]
    
    # Trier et retourner top N
    top_recommendations = unseen_predictions.nlargest(n_recommendations, 'predicted_rating')
    
    return list(zip(top_recommendations['movie_id'], top_recommendations['predicted_rating']))

# 5. √âVALUATION DU SYST√àME
print("\\nüìä √âvaluation du syst√®me...")

# Division train/test
def train_test_split_ratings(ratings_df, test_ratio=0.2):
    # Pour chaque utilisateur, garder quelques ratings pour le test
    train_list, test_list = [], []
    
    for user_id in ratings_df['user_id'].unique():
        user_ratings = ratings_df[ratings_df['user_id'] == user_id]
        
        if len(user_ratings) >= 5:  # Au moins 5 ratings
            n_test = max(1, int(len(user_ratings) * test_ratio))
            test_indices = np.random.choice(user_ratings.index, n_test, replace=False)
            
            test_list.append(user_ratings.loc[test_indices])
            train_list.append(user_ratings.drop(test_indices))
        else:
            train_list.append(user_ratings)
    
    train_df = pd.concat(train_list, ignore_index=True)
    test_df = pd.concat(test_list, ignore_index=True) if test_list else pd.DataFrame()
    
    return train_df, test_df

train_df, test_df = train_test_split_ratings(ratings_df)
print(f"üìà Train: {len(train_df)} ratings")
print(f"üéØ Test: {len(test_df)} ratings")

# Recr√©er la matrice avec donn√©es d'entra√Ænement
train_matrix = train_df.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)

# 6. SYST√àME HYBRIDE (Exemple simplifi√©)
print("\\nüîÑ Syst√®me hybride...")

def hybrid_recommendations(user_id, train_matrix, user_factors, movie_factors, 
                          alpha=0.7, n_recommendations=10):
    try:
        # Recommandations SVD
        svd_recs = dict(svd_recommendations(user_id, train_matrix, user_factors, movie_factors, 20))
        
        # Recommandations collaboratives
        collab_recs = dict(collaborative_filtering_users(user_id, train_matrix, 20))
        
        # Combinaison hybride
        all_movies = set(svd_recs.keys()) | set(collab_recs.keys())
        hybrid_scores = {}
        
        for movie in all_movies:
            svd_score = svd_recs.get(movie, 0)
            collab_score = collab_recs.get(movie, 0)
            
            # Normalisation simple (en pratique, plus sophistiqu√©e)
            hybrid_scores[movie] = alpha * svd_score + (1 - alpha) * collab_score
        
        # Retourner top N
        sorted_hybrid = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_hybrid[:n_recommendations]
    
    except:
        # Fallback si erreur
        return []

# 7. TEST ET D√âMONSTRATION
print("\\nüé¨ Test du syst√®me de recommandation...")

# Choisir un utilisateur test
test_users = [u for u in test_df['user_id'].unique() if u in train_matrix.index]
if test_users:
    test_user = test_users[0]
    
    print(f"\\nüéØ Recommandations pour l'utilisateur {test_user}:")
    
    # SVD recommandations
    svd_recs = svd_recommendations(test_user, train_matrix, user_factors, movie_factors, 5)
    print("\\nüìä Top 5 - SVD:")
    for i, (movie, score) in enumerate(svd_recs, 1):
        print(f"   {i}. Film {movie}: {score:.2f}")
    
    # Collaboratif
    collab_recs = collaborative_filtering_users(test_user, train_matrix, 5)
    print("\\nüë• Top 5 - Collaboratif:")
    for i, (movie, score) in enumerate(collab_recs, 1):
        print(f"   {i}. Film {movie}: {score:.2f}")
    
    # Hybride
    hybrid_recs = hybrid_recommendations(test_user, train_matrix, user_factors, movie_factors, 0.7, 5)
    print("\\nüîÑ Top 5 - Hybride:")
    for i, (movie, score) in enumerate(hybrid_recs, 1):
        print(f"   {i}. Film {movie}: {score:.2f}")

# 8. M√âTRIQUES D'√âVALUATION
print("\\nüìà Calcul des m√©triques d'√©valuation...")

def calculate_precision_at_k(recommended_items, relevant_items, k):
    recommended_at_k = recommended_items[:k]
    relevant_and_recommended = set(recommended_at_k) & set(relevant_items)
    
    if len(recommended_at_k) == 0:
        return 0
    
    return len(relevant_and_recommended) / len(recommended_at_k)

# Exemple de calcul de pr√©cision@5
if test_users and len(test_df) > 0:
    test_user = test_users[0]
    user_test_movies = test_df[test_df['user_id'] == test_user]
    relevant_movies = user_test_movies[user_test_movies['rating'] >= 4]['movie_id'].tolist()
    
    if hybrid_recs:
        recommended_movies = [movie for movie, score in hybrid_recs]
        precision_5 = calculate_precision_at_k(recommended_movies, relevant_movies, 5)
        print(f"üéØ Pr√©cision@5: {precision_5:.2f}")

print("\\n‚úÖ Syst√®me de recommandation op√©rationnel!")
print("üöÄ Pr√™t pour la production avec monitoring et A/B testing!")`}
        />

        <ExerciseCard
          title="üîç D√©tection d'Anomalies dans les Transactions"
          difficulty="avanc√©"
          estimatedTime="35 min"
          problem="Une banque vous demande de cr√©er un syst√®me de d√©tection de fraude en temps r√©el. Analysez 100,000 transactions avec 30 features (montant, localisation, heure, type de marchande, historique client...). Seules 0.1% des transactions sont frauduleuses. Cr√©ez un mod√®le capable de d√©tecter 95% des fraudes tout en minimisant les faux positifs."
          hints={[
            "Le dataset est tr√®s d√©s√©quilibr√© - utilisez des techniques de r√©√©quilibrage (SMOTE, undersampling)",
            "Testez plusieurs algorithmes : Isolation Forest, One-Class SVM, Random Forest",
            "Cr√©ez des features d'ing√©nierie : fr√©quence, patterns temporels, g√©olocalisation",
            "Optimisez pour le rappel (catch frauds) tout en contr√¥lant la pr√©cision",
            "Utilisez la validation temporelle (pas de data leakage temporel)"
          ]}
          solution={`# D√©tection d'Anomalies - Fraude Bancaire - Solution Compl√®te
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.metrics import average_precision_score, roc_curve
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import warnings
warnings.filterwarnings('ignore')

print("üîç D√âTECTION DE FRAUDE BANCAIRE")
print("=" * 40)

# 1. G√âN√âRATION DE DONN√âES R√âALISTES
print("\\nüìä G√©n√©ration du dataset de transactions...")
np.random.seed(42)

n_transactions = 50000  # R√©duit pour l'exemple
fraud_rate = 0.002      # 0.2% de fraudes (tr√®s d√©s√©quilibr√©)

# G√©n√©ration des transactions normales (99.8%)
n_normal = int(n_transactions * (1 - fraud_rate))
n_fraud = n_transactions - n_normal

print(f"üí≥ Transactions normales: {n_normal}")
print(f"üö® Transactions frauduleuses: {n_fraud}")

# Features pour transactions normales
normal_data = {
    'montant': np.random.lognormal(3, 1, n_normal),  # Distribution log-normale r√©aliste
    'heure': np.random.normal(14, 4, n_normal) % 24, # Pic √† 14h
    'jour_semaine': np.random.choice(range(7), n_normal, p=[0.12,0.14,0.14,0.14,0.14,0.16,0.16]),
    'nb_trans_jour': np.random.poisson(3, n_normal),
    'nb_trans_semaine': np.random.poisson(20, n_normal),
    'solde_compte': np.random.normal(5000, 3000, n_normal),
    'age_compte_jours': np.random.exponential(500, n_normal),
    '
_merchant_category': np.random.choice(range(20), n_normal, p=[0.15,0.12,0.1,0.08,0.06,0.05,0.05,0.04,0.04,0.03,0.03,0.03,0.03,0.03,0.02,0.02,0.02,0.02,0.01,0.08]),
    'pays_merchant': np.random.choice(range(5), n_normal, p=[0.7,0.15,0.08,0.04,0.03]),
    'montant_moy_30j': np.random.lognormal(3, 0.8, n_normal)
}

# Features pour transactions frauduleuses (patterns diff√©rents)
fraud_data = {
    'montant': np.random.lognormal(4.5, 1.5, n_fraud),  # Montants plus √©lev√©s
    'heure': np.random.choice(range(24), n_fraud, p=[0.08,0.09,0.1,0.12,0.08,0.06,0.04,0.03,0.02,0.02,0.02,0.02,0.02,0.02,0.02,0.03,0.04,0.05,0.04,0.03,0.03,0.04,0.06,0.07]),  # Plus la nuit
    'jour_semaine': np.random.choice(range(7), n_fraud),  # R√©partition uniforme
    'nb_trans_jour': np.random.poisson(8, n_fraud),  # Plus de transactions
    'nb_trans_semaine': np.random.poisson(45, n_fraud),
    'solde_compte': np.random.normal(3000, 4000, n_fraud),  # Soldes plus variables
    'age_compte_jours': np.random.exponential(200, n_fraud),  # Comptes plus r√©cents
    'merchant_category': np.random.choice(range(20), n_fraud),  # R√©partition diff√©rente
    'pays_merchant': np.random.choice(range(5), n_fraud, p=[0.4,0.2,0.2,0.1,0.1]),  # Plus d'international
    'montant_moy_30j': np.random.lognormal(2.8, 1.2, n_fraud)
}

# Cr√©ation du DataFrame combin√©
transactions_data = []
labels = []

for key in normal_data.keys():
    if key not in transactions_data:
        transactions_data.append([])
    transactions_data[-1] = list(normal_data[key]) + list(fraud_data[key])

# Transposer pour avoir la bonne structure
df_data = {}
for i, key in enumerate(normal_data.keys()):
    df_data[key] = list(normal_data[key]) + list(fraud_data[key])

df = pd.DataFrame(df_data)
df['is_fraud'] = [0] * n_normal + [1] * n_fraud

# Ajout de features d'ing√©nierie
df['montant_log'] = np.log1p(df['montant'])
df['ratio_montant_solde'] = df['montant'] / (df['solde_compte'] + 1)
df['ratio_montant_moyenne'] = df['montant'] / (df['montant_moy_30j'] + 1)
df['is_weekend'] = (df['jour_semaine'] >= 5).astype(int)
df['is_night'] = ((df['heure'] >= 22) | (df['heure'] <= 6)).astype(int)
df['freq_trans_day_high'] = (df['nb_trans_jour'] > 10).astype(int)

print(f"‚úÖ Dataset cr√©√©: {len(df)} transactions")
print(f"üìä Taux de fraude: {df['is_fraud'].mean():.3%}")

# 2. ANALYSE EXPLORATOIRE SP√âCIALIS√âE
print("\\nüîç Analyse exploratoire des patterns de fraude...")

# Statistiques par classe
fraud_stats = df.groupby('is_fraud').agg({
    'montant': ['mean', 'std', 'median'],
    'heure': ['mean', 'std'],
    'nb_trans_jour': ['mean', 'std'],
    'solde_compte': ['mean', 'std'],
    'is_night': 'mean',
    'is_weekend': 'mean'
}).round(3)

print("üìä Statistiques par classe:")
print(fraud_stats)

# Visualisations sp√©cialis√©es
fig, axes = plt.subplots(3, 3, figsize=(20, 15))
fig.suptitle('üîç Analyse des Patterns de Fraude', fontsize=16, fontweight='bold')

# Distribution des montants
axes[0,0].hist(df[df['is_fraud']==0]['montant_log'], bins=50, alpha=0.7, label='Normal', density=True)
axes[0,0].hist(df[df['is_fraud']==1]['montant_log'], bins=50, alpha=0.7, label='Fraude', density=True)
axes[0,0].set_title('üí∞ Distribution Montants (log)')
axes[0,0].legend()

# R√©partition par heure
hour_fraud = df.groupby(['heure', 'is_fraud']).size().unstack().fillna(0)
hour_fraud_rate = hour_fraud[1] / (hour_fraud[0] + hour_fraud[1])
axes[0,1].plot(hour_fraud_rate.index, hour_fraud_rate.values * 100, 'r-o')
axes[0,1].set_title('üïê Taux de Fraude par Heure')
axes[0,1].set_ylabel('Taux de fraude (%)')

# R√©partition par jour de la semaine
day_fraud = df.groupby(['jour_semaine', 'is_fraud']).size().unstack().fillna(0)
day_fraud_rate = day_fraud[1] / (day_fraud[0] + day_fraud[1])
axes[0,2].bar(range(7), day_fraud_rate.values * 100, color='orange', alpha=0.7)
axes[0,2].set_title('üìÖ Taux de Fraude par Jour')
axes[0,2].set_xticks(range(7))
axes[0,2].set_xticklabels(['L','M','M','J','V','S','D'])

# Corr√©lation avec la fraude
correlation_with_fraud = df.corr()['is_fraud'].sort_values(key=abs, ascending=False)[1:]
axes[1,0].barh(range(len(correlation_with_fraud)), correlation_with_fraud.values)
axes[1,0].set_yticks(range(len(correlation_with_fraud)))
axes[1,0].set_yticklabels(correlation_with_fraud.index, rotation=0)
axes[1,0].set_title('üîó Corr√©lation avec Fraude')

# Box plot montants
df.boxplot(column='montant_log', by='is_fraud', ax=axes[1,1])
axes[1,1].set_title('üì¶ Montants par Classe')

# Scatter plot multidimensionnel
scatter = axes[1,2].scatter(df['ratio_montant_solde'], df['nb_trans_jour'], 
                           c=df['is_fraud'], cmap='RdYlBu', alpha=0.6)
axes[1,2].set_title('üéØ Ratio Montant/Solde vs Nb Trans')
axes[1,2].set_xlabel('Ratio Montant/Solde')
axes[1,2].set_ylabel('Nb Trans Jour')

# Matrice de corr√©lation
features_corr = ['montant_log', 'nb_trans_jour', 'solde_compte', 'age_compte_jours', 'ratio_montant_solde']
corr_matrix = df[features_corr + ['is_fraud']].corr()
im = axes[2,0].imshow(corr_matrix, cmap='coolwarm', aspect='auto')
axes[2,0].set_xticks(range(len(corr_matrix.columns)))
axes[2,0].set_yticks(range(len(corr_matrix.columns)))
axes[2,0].set_xticklabels(corr_matrix.columns, rotation=45)
axes[2,0].set_yticklabels(corr_matrix.columns)
axes[2,0].set_title('üîó Matrice de Corr√©lation')

# Distribution des ratios
axes[2,1].hist(df[df['is_fraud']==0]['ratio_montant_moyenne'], bins=50, alpha=0.7, label='Normal', density=True)
axes[2,1].hist(df[df['is_fraud']==1]['ratio_montant_moyenne'], bins=50, alpha=0.7, label='Fraude', density=True)
axes[2,1].set_title('üìä Ratio Montant/Moyenne')
axes[2,1].legend()

# Analyse temporelle
night_fraud = df.groupby('is_night')['is_fraud'].mean()
axes[2,2].bar(['Jour', 'Nuit'], [night_fraud[0]*100, night_fraud[1]*100], 
             color=['lightblue', 'darkblue'], alpha=0.7)
axes[2,2].set_title('üåô Fraude Jour vs Nuit')
axes[2,2].set_ylabel('Taux de fraude (%)')

plt.tight_layout()
plt.show()

# 3. PR√âPARATION AVANC√âE DES DONN√âES
print("\\n‚öôÔ∏è Pr√©paration des donn√©es pour la mod√©lisation...")

# S√©lection des features
features = ['montant_log', 'heure', 'jour_semaine', 'nb_trans_jour', 'nb_trans_semaine',
           'solde_compte', 'age_compte_jours', 'merchant_category', 'pays_merchant',
           'ratio_montant_solde', 'ratio_montant_moyenne', 'is_weekend', 'is_night',
           'freq_trans_day_high']

X = df[features].copy()
y = df['is_fraud'].copy()

print(f"üìä Features s√©lectionn√©es: {len(features)}")
print(f"üéØ Distribution des classes: {y.value_counts().to_dict()}")

# Division temporelle (important pour √©viter le data leakage)
# Simuler une division temporelle
split_index = int(0.7 * len(df))
X_temp_train, X_temp_test = X[:split_index], X[split_index:]
y_temp_train, y_temp_test = y[:split_index], y[split_index:]

# Division train/validation sur la partie temporelle d'entra√Ænement
X_train, X_val, y_train, y_val = train_test_split(
    X_temp_train, y_temp_train, test_size=0.2, random_state=42, stratify=y_temp_train
)

print(f"üìà Train: {len(X_train)} (fraude: {y_train.sum()}/{len(y_train)})")
print(f"üìä Validation: {len(X_val)} (fraude: {y_val.sum()}/{len(y_val)})")
print(f"üéØ Test temporel: {len(X_temp_test)} (fraude: {y_temp_test.sum()}/{len(y_temp_test)})")

# Standardisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_temp_test_scaled = scaler.transform(X_temp_test)

# 4. GESTION DU D√âS√âQUILIBRE
print("\\n‚öñÔ∏è Gestion du d√©s√©quilibre des classes...")

# SMOTE pour l'over-sampling
smote = SMOTE(random_state=42, k_neighbors=3)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"‚úÖ Apr√®s SMOTE: {len(X_train_smote)} √©chantillons")
print(f"üìä Distribution: {pd.Series(y_train_smote).value_counts().to_dict()}")

# 5. MOD√âLISATION AVEC MULTIPLES ALGORITHMES
print("\\nü§ñ Test de multiple algorithmes de d√©tection...")

models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, 
                                          class_weight='balanced'),
    'Random Forest SMOTE': RandomForestClassifier(n_estimators=100, random_state=42),
    'Isolation Forest': IsolationForest(contamination=0.002, random_state=42),
    'One-Class SVM': OneClassSVM(nu=0.002, kernel='rbf')
}

results = {}

for name, model in models.items():
    print(f"\\nüîß Entra√Ænement {name}...")
    
    if name == 'Random Forest SMOTE':
        # Utiliser les donn√©es SMOTE
        model.fit(X_train_smote, y_train_smote)
        y_pred_val = model.predict(X_val_scaled)
        y_pred_proba_val = model.predict_proba(X_val_scaled)[:, 1]
        
    elif name in ['Isolation Forest', 'One-Class SVM']:
        # Algorithmes non-supervis√©s - entra√Æner sur donn√©es normales uniquement
        X_train_normal = X_train_scaled[y_train == 0]
        model.fit(X_train_normal)
        y_pred_val = model.predict(X_val_scaled)
        y_pred_val = [1 if x == -1 else 0 for x in y_pred_val]  # Conversion -1/1 vers 0/1
        y_pred_proba_val = model.decision_function(X_val_scaled)
        # Normaliser les scores de d√©cision pour avoir des pseudo-probabilit√©s
        y_pred_proba_val = (y_pred_proba_val - y_pred_proba_val.min()) / (y_pred_proba_val.max() - y_pred_proba_val.min())
        
    else:
        # Random Forest standard
        model.fit(X_train_scaled, y_train)
        y_pred_val = model.predict(X_val_scaled)
        y_pred_proba_val = model.predict_proba(X_val_scaled)[:, 1]
    
    # M√©triques
    if name not in ['Isolation Forest', 'One-Class SVM']:
        auc_score = roc_auc_score(y_val, y_pred_proba_val)
        avg_precision = average_precision_score(y_val, y_pred_proba_val)
    else:
        auc_score = roc_auc_score(y_val, y_pred_proba_val)
        avg_precision = average_precision_score(y_val, y_pred_proba_val)
    
    results[name] = {
        'model': model,
        'predictions': y_pred_val,
        'probabilities': y_pred_proba_val,
        'auc': auc_score,
        'avg_precision': avg_precision
    }
    
    print(f"   üéØ AUC: {auc_score:.3f}")
    print(f"   üìä Average Precision: {avg_precision:.3f}")
    
    # Rapport de classification
    print(f"   üìã Classification Report:")
    print(classification_report(y_val, y_pred_val, target_names=['Normal', 'Fraude']))

# 6. S√âLECTION DU MEILLEUR MOD√àLE
best_model_name = max(results.keys(), key=lambda x: results[x]['auc'])
best_model = results[best_model_name]['model']

print(f"\\nüèÜ MEILLEUR MOD√àLE: {best_model_name}")
print(f"üéØ AUC: {results[best_model_name]['auc']:.3f}")
print(f"üìä Average Precision: {results[best_model_name]['avg_precision']:.3f}")

# 7. ANALYSE APPROFONDIE DU MEILLEUR MOD√àLE
print("\\nüìä Analyse approfondie du meilleur mod√®le...")

best_pred = results[best_model_name]['predictions']
best_proba = results[best_model_name]['probabilities']

# Matrice de confusion
cm = confusion_matrix(y_val, best_pred)
print("\\nüéØ Matrice de confusion:")
print(cm)

# Visualisations avanc√©es
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle(f'üìà Analyse du Mod√®le {best_model_name}', fontsize=14, fontweight='bold')

# ROC Curve
fpr, tpr, _ = roc_curve(y_val, best_proba)
axes[0,0].plot(fpr, tpr, linewidth=2, label=f'AUC = {results[best_model_name]["auc"]:.3f}')
axes[0,0].plot([0, 1], [0, 1], 'k--', alpha=0.5)
axes[0,0].set_xlabel('Taux de Faux Positifs')
axes[0,0].set_ylabel('Taux de Vrais Positifs')
axes[0,0].set_title('üìà Courbe ROC')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_val, best_proba)
axes[0,1].plot(recall, precision, linewidth=2, label=f'AP = {results[best_model_name]["avg_precision"]:.3f}')
axes[0,1].set_xlabel('Rappel')
axes[0,1].set_ylabel('Pr√©cision')
axes[0,1].set_title('üéØ Courbe Pr√©cision-Rappel')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# Distribution des scores
axes[1,0].hist(best_proba[y_val == 0], bins=50, alpha=0.7, label='Normal', density=True)
axes[1,0].hist(best_proba[y_val == 1], bins=50, alpha=0.7, label='Fraude', density=True)
axes[1,0].set_xlabel('Score de Fraude')
axes[1,0].set_ylabel('Densit√©')
axes[1,0].set_title('üìä Distribution des Scores')
axes[1,0].legend()

# Matrice de confusion heatmap
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])
axes[1,1].set_title('üéØ Matrice de Confusion')
axes[1,1].set_xlabel('Pr√©diction')
axes[1,1].set_ylabel('R√©alit√©')

plt.tight_layout()
plt.show()

# 8. OPTIMISATION DU SEUIL
print("\\nüéØ Optimisation du seuil de d√©cision...")

# Calcul des m√©triques pour diff√©rents seuils
thresholds = np.arange(0.1, 0.9, 0.05)
metrics_by_threshold = []

for threshold in thresholds:
    y_pred_thresh = (best_proba >= threshold).astype(int)
    
    tn, fp, fn, tp = confusion_matrix(y_val, y_pred_thresh).ravel()
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
    
    metrics_by_threshold.append({
        'threshold': threshold,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'fpr': false_positive_rate
    })

metrics_df = pd.DataFrame(metrics_by_threshold)

# Trouver le seuil optimal (maximiser F1 ou selon crit√®re m√©tier)
optimal_threshold = metrics_df.loc[metrics_df['f1'].idxmax(), 'threshold']
print(f"üéØ Seuil optimal (F1): {optimal_threshold:.2f}")

# Appliquer le seuil optimal
y_pred_optimal = (best_proba >= optimal_threshold).astype(int)
print("\\nüìã Performance avec seuil optimal:")
print(classification_report(y_val, y_pred_optimal, target_names=['Normal', 'Fraude']))

# 9. TEST SUR DONN√âES TEMPORELLES
print("\\nüïê Test sur donn√©es temporelles (simulation production)...")

if best_model_name == 'Random Forest SMOTE':
    y_pred_temporal = best_model.predict(X_temp_test_scaled)
    y_proba_temporal = best_model.predict_proba(X_temp_test_scaled)[:, 1]
elif best_model_name in ['Isolation Forest', 'One-Class SVM']:
    y_pred_temporal_raw = best_model.predict(X_temp_test_scaled)
    y_pred_temporal = [1 if x == -1 else 0 for x in y_pred_temporal_raw]
    y_proba_temporal = best_model.decision_function(X_temp_test_scaled)
    y_proba_temporal = (y_proba_temporal - y_proba_temporal.min()) / (y_proba_temporal.max() - y_proba_temporal.min())
else:
    y_pred_temporal = best_model.predict(X_temp_test_scaled)
    y_proba_temporal = best_model.predict_proba(X_temp_test_scaled)[:, 1]

# Appliquer le seuil optimal
y_pred_temporal_optimal = (y_proba_temporal >= optimal_threshold).astype(int)

print("üìä Performance sur donn√©es temporelles:")
print(classification_report(y_temp_test, y_pred_temporal_optimal, target_names=['Normal', 'Fraude']))

temporal_auc = roc_auc_score(y_temp_test, y_proba_temporal)
print(f"üéØ AUC temporel: {temporal_auc:.3f}")

print("\\n‚úÖ SYST√àME DE D√âTECTION DE FRAUDE OP√âRATIONNEL!")
print("üöÄ Pr√™t pour le d√©ploiement en temps r√©el avec monitoring!")`}
        />
      </div>

      {/* Quiz avanc√©s */}
      <div className="grid grid-cols-1 lg:grid-cols-2 gap-8">
        <QuizCard
          question="Dans un probl√®me de classification avec des classes tr√®s d√©s√©quilibr√©es (99% classe A, 1% classe B), quelle strat√©gie d'√©valuation est la plus appropri√©e ?"
          options={[
            "Utiliser uniquement l'accuracy globale",
            "Se concentrer sur la pr√©cision de la classe majoritaire",
            "Utiliser l'AUC-ROC et l'Average Precision avec validation stratifi√©e",
            "√âquilibrer artificiellement le dataset de test"
          ]}
          correctAnswer={2}
          explanation="Pour les classes d√©s√©quilibr√©es, l'AUC-ROC et l'Average Precision sont les m√©triques les plus robustes car elles √©valuent la capacit√© du mod√®le √† distinguer les classes ind√©pendamment du seuil de d√©cision. La validation stratifi√©e preserve la proportion des classes dans chaque fold."
          difficulty="difficile"
        />

        <QuizCard
          question="Qu'est-ce qui caract√©rise le mieux le surapprentissage (overfitting) dans un mod√®le de Machine Learning ?"
          options={[
            "Le mod√®le a une faible pr√©cision sur les donn√©es d'entra√Ænement et de test",
            "Le mod√®le a une haute pr√©cision sur l'entra√Ænement mais faible sur le test",
            "Le mod√®le prend trop de temps √† s'entra√Æner",
            "Le mod√®le a besoin de plus de donn√©es d'entra√Ænement"
          ]}
          correctAnswer={1}
          explanation="Le surapprentissage se caract√©rise par un mod√®le qui 'm√©morise' les donn√©es d'entra√Ænement au lieu d'apprendre des patterns g√©n√©ralisables. Il obtient donc d'excellents r√©sultats sur les donn√©es qu'il a vues (entra√Ænement) mais √©choue sur de nouvelles donn√©es (test). C'est le signe d'un mod√®le trop complexe pour la quantit√© de donn√©es disponible."
          difficulty="moyen"
        />
      </div>

      <EducationalCard title="üéØ Projet Guid√© : Syst√®me de Recommandation E-commerce" type="exercice">
        <div className="space-y-8">
          <div className="text-center">
            <p className="text-xl font-medium mb-4">
              üöÄ Cr√©ons ensemble un syst√®me de recommandation complet pour une plateforme e-commerce !
            </p>
            <p className="text-gray-600">
              Projet fil rouge avec donn√©es r√©elles, d√©fis techniques et impact business
            </p>
          </div>
          
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
            <div className="bg-gradient-to-br from-blue-50 to-blue-100 p-6 rounded-xl border-2 border-blue-200 hover:shadow-lg transition-all duration-300">
              <div className="text-2xl mb-3">üìä</div>
              <h4 className="font-bold text-blue-800 mb-2">√âtape 1 : Analyse des Donn√©es</h4>
              <div className="text-sm space-y-2">
                <p><strong>Donn√©es :</strong></p>
                <ul className="list-disc pl-4 space-y-1">
                  <li>1M+ interactions utilisateur-produit</li>
                  <li>100k utilisateurs, 50k produits</li>
                  <li>M√©tadonn√©es : cat√©gories, prix, marques</li>
                  <li>Historique temporel sur 2 ans</li>
                </ul>
              </div>
            </div>
            
            <div className="bg-gradient-to-br from-green-50 to-green-100 p-6 rounded-xl border-2 border-green-200 hover:shadow-lg transition-all duration-300">
              <div className="text-2xl mb-3">üîß</div>
              <h4 className="font-bold text-green-800 mb-2">√âtape 2 : Feature Engineering</h4>
              <div className="text-sm space-y-2">
                <p><strong>Features avanc√©es :</strong></p>
                <ul className="list-disc pl-4 space-y-1">
                  <li>Embeddings produits (Word2Vec)</li>
                  <li>Patterns temporels saisonniers</li>
                  <li>Similarit√© collaborative</li>
                  <li>Profils comportementaux</li>
                </ul>
              </div>
            </div>
            
            <div className="bg-gradient-to-br from-purple-50 to-purple-100 p-6 rounded-xl border-2 border-purple-200 hover:shadow-lg transition-all duration-300">
              <div className="text-2xl mb-3">ü§ñ</div>
              <h4 className="font-bold text-purple-800 mb-2">√âtape 3 : Mod√©lisation</h4>
              <div className="text-sm space-y-2">
                <p><strong>Approches :</strong></p>
                <ul className="list-disc pl-4 space-y-1">
                  <li>Matrix Factorization (SVD++)</li>
                  <li>Deep Learning (Neural CF)</li>
                  <li>Ensemble hybride</li>
                  <li>Real-time serving</li>
                </ul>
              </div>
            </div>
            
            <div className="bg-gradient-to-br from-red-50 to-red-100 p-6 rounded-xl border-2 border-red-200 hover:shadow-lg transition-all duration-300">
              <div className="text-2xl mb-3">üìà</div>
              <h4 className="font-bold text-red-800 mb-2">√âtape 4 : √âvaluation Business</h4>
              <div className="text-sm space-y-2">
                <p><strong>M√©triques :</strong></p>
                <ul className="list-disc pl-4 space-y-1">
                  <li>CTR (Click-Through Rate)</li>
                  <li>Conversion rate</li>
                  <li>Revenue lift</li>
                  <li>A/B testing</li>
                </ul>
              </div>
            </div>
          </div>
          
          <div className="bg-gradient-to-r from-indigo-100 to-purple-100 p-8 rounded-xl border-2 border-indigo-200">
            <h4 className="font-bold text-indigo-800 mb-6 text-xl text-center">üéØ D√©fis Techniques √† R√©soudre</h4>
            
            <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
              <div className="bg-white p-6 rounded-lg shadow-lg">
                <h5 className="font-bold text-red-600 mb-3">üö® Cold Start Problem</h5>
                <p className="text-sm mb-3">Comment recommander √† un nouvel utilisateur sans historique ?</p>
                <div className="bg-red-50 p-3 rounded">
                  <p className="text-xs"><strong>Solution :</strong> Utiliser les m√©tadonn√©es, donn√©es d√©mographiques et popularity-based recommendations</p>
                </div>
              </div>
              
              <div className="bg-white p-6 rounded-lg shadow-lg">
                <h5 className="font-bold text-blue-600 mb-3">‚ö° Scalabilit√©</h5>
                <p className="text-sm mb-3">Servir 10k+ requ√™tes/seconde en temps r√©el ?</p>
                <div className="bg-blue-50 p-3 rounded">
                  <p className="text-xs"><strong>Solution :</strong> Pre-computing, caching, approximate algorithms (LSH)</p>
                </div>
              </div>
              
              <div className="bg-white p-6 rounded-lg shadow-lg">
                <h5 className="font-bold text-green-600 mb-3">üé≠ Diversit√© vs Pr√©cision</h5>
                <p className="text-sm mb-3">√âquilibrer recommendations pr√©cises et d√©couverte ?</p>
                <div className="bg-green-50 p-3 rounded">
                  <p className="text-xs"><strong>Solution :</strong> Multi-objective optimization, exploration vs exploitation</p>
                </div>
              </div>
              
              <div className="bg-white p-6 rounded-lg shadow-lg">
                <h5 className="font-bold text-purple-600 mb-3">üìä Donn√©es Sparse</h5>
                <p className="text-sm mb-3">99%+ de la matrice user-item est vide ?</p>
                <div className="bg-purple-50 p-3 rounded">
                  <p className="text-xs"><strong>Solution :</strong> Matrix factorization, implicit feedback, side information</p>
                </div>
              </div>
            </div>
          </div>
          
          <div className="bg-gradient-to-r from-yellow-100 to-orange-100 p-8 rounded-xl border-2 border-yellow-200">
            <h4 className="font-bold text-orange-800 mb-4 text-center">üéä Challenge Bonus : Impact Business</h4>
            <div className="grid grid-cols-1 md:grid-cols-3 gap-4 text-center">
              <div className="bg-white p-6 rounded-lg">
                <div className="text-3xl font-bold text-green-600">+23%</div>
                <p className="text-sm text-gray-600">Augmentation des ventes</p>
              </div>
              <div className="bg-white p-6 rounded-lg">
                <div className="text-3xl font-bold text-blue-600">+35%</div>
                <p className="text-sm text-gray-600">Engagement utilisateur</p>
              </div>
              <div className="bg-white p-6 rounded-lg">
                <div className="text-3xl font-bold text-purple-600">‚Ç¨2.1M</div>
                <p className="text-sm text-gray-600">Revenue suppl√©mentaire annuel</p>
              </div>
            </div>
            <p className="text-center mt-4 text-sm text-gray-700">
              <strong>Objectif :</strong> Impl√©menter et d√©ployer un syst√®me qui g√©n√®re un ROI mesurable ! üöÄ
            </p>
          </div>
        </div>
      </EducationalCard>

      <EducationalCard title="üõ†Ô∏è Ressources et Outils Professionnels" type="rappel">
        <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
          <div className="bg-white p-6 rounded-xl border hover:shadow-lg transition-all duration-300">
            <h4 className="font-bold text-blue-600 mb-3 flex items-center gap-2">
              <Code className="h-5 w-5" />
              Notebooks Jupyter
            </h4>
            <p className="text-sm text-gray-600 mb-3">Environnement interactif pour l'exp√©rimentation</p>
            <div className="flex items-center gap-2">
              <ExternalLink className="h-4 w-4 text-blue-500" />
              <a href="https://jupyter.org" className="text-blue-500 text-sm hover:underline">jupyter.org</a>
            </div>
          </div>
          
          <div className="bg-white p-6 rounded-xl border hover:shadow-lg transition-all duration-300">
            <h4 className="font-bold text-green-600 mb-3 flex items-center gap-2">
              <Target className="h-5 w-5" />
              MLflow
            </h4>
            <p className="text-sm text-gray-600 mb-3">Suivi et gestion des exp√©riences ML</p>
            <div className="flex items-center gap-2">
              <ExternalLink className="h-4 w-4 text-green-500" />
              <a href="https://mlflow.org" className="text-green-500 text-sm hover:underline">mlflow.org</a>
            </div>
          </div>
          
          <div className="bg-white p-6 rounded-xl border hover:shadow-lg transition-all duration-300">
            <h4 className="font-bold text-purple-600 mb-3 flex items-center gap-2">
              <Lightbulb className="h-5 w-5" />
              TensorBoard
            </h4>
            <p className="text-sm text-gray-600 mb-3">Visualisation des m√©triques d'entra√Ænement</p>
            <div className="flex items-center gap-2">
              <ExternalLink className="h-4 w-4 text-purple-500" />
              <a href="https://tensorboard.dev" className="text-purple-500 text-sm hover:underline">tensorboard.dev</a>
            </div>
          </div>
        </div>
      </EducationalCard>
    </section>
  );
};

export default PracticalExercisesSection;
