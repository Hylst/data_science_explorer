/**
 * Statistical Methods and Concepts
 * Core statistical concepts used in data science
 */

import { GlossaryEntry } from './types';

export const statisticsTerms: GlossaryEntry[] = [
  {
    term: "Statistiques",
    description: "Les Statistiques constituent la science fondamentale qui transforme les donn√©es brutes en connaissances exploitables. Imaginez un traducteur universel : les statistiques traduisent le **langage** des donn√©es en insights compr√©hensibles pour la prise de d√©cision. Cette discipline englobe quatre piliers essentiels : la **collecte** (comment obtenir des donn√©es repr√©sentatives), l'**analyse** (application de m√©thodes math√©matiques), l'**interpr√©tation** (donner du sens aux r√©sultats), et la **pr√©sentation** (communiquer efficacement les findings). Les statistiques nous permettent de naviguer dans l'incertitude, de distinguer les signaux du bruit, et de faire des pr√©dictions fiables. Elles constituent le socle math√©matique de la data science, fournissant les outils pour tester des hypoth√®ses, quantifier la confiance dans nos conclusions, et g√©n√©raliser des observations d'√©chantillons √† des populations enti√®res. Des sondages d'opinion aux essais cliniques, des analyses de march√© aux pr√©visions m√©t√©orologiques, les statistiques sont omnipr√©sentes dans notre soci√©t√© moderne.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Population vs √âchantillon",
    description: "Cette distinction fondamentale est comme la diff√©rence entre photographier une foule enti√®re versus prendre un instantan√© repr√©sentatif. La **Population** repr√©sente l'ensemble complet et exhaustif de tous les √©l√©ments, individus, ou observations qui nous int√©ressent dans notre √©tude (par exemple, tous les citoyens fran√ßais, toutes les entreprises du CAC 40, ou tous les patients atteints d'une maladie sp√©cifique). L'**√âchantillon** est un sous-ensemble soigneusement s√©lectionn√© de cette population, choisi pour √™tre repr√©sentatif et permettre des inf√©rences valides. La qualit√© de l'√©chantillonnage est cruciale : un √©chantillon biais√© peut conduire √† des conclusions erron√©es. Les m√©thodes d'√©chantillonnage incluent l'√©chantillonnage al√©atoire simple, stratifi√©, ou par grappes. La **taille d'√©chantillon** influence directement la pr√©cision des estimations : plus l'√©chantillon est grand, plus nos conclusions sont fiables, mais les co√ªts augmentent. Cette distinction est essentielle car √©tudier une population enti√®re est souvent impossible (co√ªt, temps, accessibilit√©), d'o√π l'importance de ma√Ætriser les techniques d'√©chantillonnage pour g√©n√©raliser nos r√©sultats.",
    category: "statistiques",
    icon: "Users"
  },
  {
    term: "Moyenne (Mean)",
    description: "La moyenne arithm√©tique est comme le 'centre de gravit√©' de vos donn√©es. Calcul√©e en additionnant toutes les valeurs et en divisant par le nombre d'observations (Œ£x/n), elle repr√©sente la valeur typique autour de laquelle les donn√©es gravitent. **Avantages** : facile √† calculer, utilise toutes les donn√©es, base de nombreux tests statistiques. **Inconv√©nients** : tr√®s sensible aux valeurs aberrantes (outliers). Par exemple, si 9 personnes gagnent 30k‚Ç¨ et une 10√®me gagne 300k‚Ç¨, la moyenne (57k‚Ç¨) ne repr√©sente pas bien le groupe. **Applications** : calcul de performances moyennes, analyses financi√®res, contr√¥le qualit√©. **Variantes** : moyenne pond√©r√©e (certaines valeurs comptent plus), moyenne g√©om√©trique (pour les taux de croissance), moyenne harmonique (pour les vitesses). La moyenne est la mesure de tendance centrale la plus utilis√©e en statistiques inf√©rentielles et constitue la base de concepts avanc√©s comme la variance et l'√©cart-type.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "M√©diane (Median)",
    description: "La m√©diane est la 'valeur du milieu' qui divise vos donn√©es en deux moiti√©s √©gales, comme un m√©diateur qui s√©pare √©quitablement deux groupes. Pour la calculer : triez les donn√©es par ordre croissant, puis prenez la valeur centrale (si n impair) ou la moyenne des deux valeurs centrales (si n pair). **Avantage majeur** : robuste aux valeurs aberrantes - elle r√©siste aux extr√™mes. Dans l'exemple pr√©c√©dent (9 personnes √† 30k‚Ç¨, 1 √† 300k‚Ç¨), la m√©diane reste 30k‚Ç¨, plus repr√©sentative. **Applications pratiques** : salaires (m√©diane plus repr√©sentative que moyenne), prix immobiliers, scores de satisfaction. **Interpr√©tation** : 50% des observations sont inf√©rieures √† la m√©diane, 50% sup√©rieures. **Comparaison avec la moyenne** : si m√©diane < moyenne, distribution asym√©trique vers la droite (queue positive) ; si m√©diane > moyenne, asym√©trie vers la gauche. La m√©diane est essentielle en statistiques descriptives et particuli√®rement utile pour les donn√©es √©conomiques et sociales.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Mode",
    description: "Le mode est la 'star' de vos donn√©es - la valeur qui appara√Æt le plus fr√©quemment, comme la chanson la plus jou√©e sur une playlist. **Identification** : comptez la fr√©quence de chaque valeur, le mode est celle avec le maximum d'occurrences. **Types de distributions** : unimodale (un seul mode), bimodale (deux modes), multimodale (plusieurs modes), ou amodale (pas de mode clair). **Particularit√©s** : seule mesure de tendance centrale applicable aux donn√©es qualitatives (couleur pr√©f√©r√©e, marque favorite). **Applications** : √©tudes de march√© (produit le plus vendu), contr√¥le qualit√© (d√©faut le plus fr√©quent), analyses d√©mographiques (√¢ge modal). **Avantages** : facile √† identifier visuellement, r√©sistant aux valeurs aberrantes, applicable √† tous types de donn√©es. **Limites** : peut ne pas exister ou √™tre multiple, moins utilis√© en statistiques inf√©rentielles. **Exemple pratique** : dans une enqu√™te sur les tailles de chaussures, si la taille 42 revient 15 fois (plus que toute autre), c'est le mode. Le mode compl√®te utilement la moyenne et la m√©diane pour une description compl√®te de la distribution.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Variance",
    description: "La variance mesure √† quel point vos donn√©es sont 'dispers√©es' autour de la moyenne, comme mesurer l'√©talement d'un groupe de personnes autour d'un point de rassemblement. **Calcul** : moyenne des carr√©s des √©carts √† la moyenne : Var(X) = Œ£(xi - Œº)¬≤/n (population) ou Œ£(xi - xÃÑ)¬≤/(n-1) (√©chantillon). **Pourquoi √©lever au carr√© ?** Cela √©vite que les √©carts positifs et n√©gatifs s'annulent, et donne plus de poids aux grandes d√©viations. **Interpr√©tation** : variance faible = donn√©es concentr√©es pr√®s de la moyenne (groupe homog√®ne) ; variance √©lev√©e = donn√©es tr√®s dispers√©es (groupe h√©t√©rog√®ne). **Probl√®me pratique** : l'unit√© est le carr√© de l'unit√© originale (si les donn√©es sont en euros, la variance est en euros¬≤), ce qui complique l'interpr√©tation. **Applications** : finance (mesure du risque d'un investissement), contr√¥le qualit√© (consistance d'un processus), recherche (variabilit√© entre sujets). La variance est fondamentale en statistiques car elle quantifie l'incertitude et sert de base √† de nombreux tests statistiques et mod√®les pr√©dictifs.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "√âcart-type (Standard Deviation)",
    description: "L'√©cart-type est la 'version lisible' de la variance - sa racine carr√©e qui remet les unit√©s dans leur forme originale. Si la variance est comme mesurer une surface, l'√©cart-type est comme mesurer une distance. **Calcul** : œÉ = ‚àöVariance. **Avantage majeur** : m√™me unit√© que les donn√©es originales, donc directement interpr√©table. **R√®gle empirique (loi normale)** : environ 68% des donn√©es se trouvent √† ¬±1œÉ de la moyenne, 95% √† ¬±2œÉ, 99.7% √† ¬±3œÉ. **Applications pratiques** : en finance, un √©cart-type de 15% sur les rendements d'une action indique sa volatilit√© ; en production, un √©cart-type faible indique un processus stable. **Comparaisons** : permet de comparer la variabilit√© entre diff√©rents datasets ou variables. **√âcart-type vs √©tendue** : l'√©cart-type utilise toutes les donn√©es (plus robuste) tandis que l'√©tendue ne consid√®re que min et max (sensible aux outliers). **Standardisation** : l'√©cart-type permet de cr√©er des scores Z pour comparer des valeurs de distributions diff√©rentes. C'est l'une des mesures les plus importantes en statistiques descriptives et inf√©rentielles.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Distribution normale (Gaussian)",
    description: "La distribution normale est la 'reine' des distributions statistiques, ressemblant √† une cloche parfaitement sym√©trique o√π la plupart des valeurs se concentrent au centre et diminuent graduellement vers les extr√™mes. **Analogie** : la r√©partition des tailles dans une population - peu de personnes tr√®s petites ou tr√®s grandes, la majorit√© autour de la moyenne. **Param√®tres** : enti√®rement d√©finie par sa moyenne Œº (centre) et son √©cart-type œÉ (largeur). **Propri√©t√©s remarquables** : sym√©trie parfaite, moyenne = m√©diane = mode, aires sous la courbe d√©finies (68-95-99.7 rule). **Ubiquit√© naturelle** : erreurs de mesure, caract√©ristiques biologiques, ph√©nom√®nes sociaux - le **Th√©or√®me Central Limite** explique pourquoi tant de ph√©nom√®nes suivent cette loi. **Applications** : tests statistiques (t-test, ANOVA), intervalles de confiance, contr√¥le qualit√© (Six Sigma), finance (mod√®les de risque). **Standardisation** : toute normale peut √™tre transform√©e en normale standard (Œº=0, œÉ=1) via Z = (X-Œº)/œÉ. **Importance historique** : d√©couverte par Gauss et Laplace, fondement de la statistique moderne. **Reconnaissance** : si vos donn√©es forment une courbe en cloche, vous pouvez appliquer de puissants outils statistiques param√©triques.",
    category: "statistiques",
    icon: "TrendingUp"
  },
  {
    term: "Probabilit√©",
    description: "La probabilit√© quantifie l'incertitude et mesure nos 'chances' qu'un √©v√©nement se r√©alise, comme un barom√®tre de la vraisemblance qui oscille entre l'impossible (0) et le certain (1). **Analogie** : pr√©dire la m√©t√©o - 0% = soleil garanti, 100% = pluie certaine, 70% = probablement pluvieux. **√âchelle** : toujours entre 0 et 1 (ou 0% et 100%), o√π 0.5 = √©quiprobable (pile ou face). **Interpr√©tations** : 1) **Fr√©quentiste** (r√©p√©tition infinie d'exp√©riences), 2) **Subjective** (degr√© de croyance personnel), 3) **Classique** (cas favorables/cas possibles). **R√®gles fondamentales** : P(A) + P(non-A) = 1, P(A ou B) = P(A) + P(B) - P(A et B), P(A et B) = P(A) √ó P(B|A). **Applications** : jeux de hasard, assurance (calcul des primes), m√©decine (diagnostic), finance (gestion des risques), machine learning (classification probabiliste). **Distributions** : uniforme (d√© √©quilibr√©), binomiale (succ√®s/√©chec), normale (ph√©nom√®nes naturels). **Th√©or√®me de Bayes** : mise √† jour des probabilit√©s avec nouvelles informations. **Impact** : fondement de la statistique inf√©rentielle, de l'IA probabiliste, et de la prise de d√©cision sous incertitude.",
    category: "statistiques",
    icon: "Percent"
  },
  {
    term: "Test d'hypoth√®se",
    description: "Le test d'hypoth√®se est comme un **proc√®s judiciaire** pour vos donn√©es - une proc√©dure rigoureuse qui d√©termine si une affirmation sur une population est cr√©dible ou doit √™tre rejet√©e. **Processus** : 1) Formuler l'**hypoth√®se nulle H‚ÇÄ** (status quo, 'pas d'effet') et l'**hypoth√®se alternative H‚ÇÅ** (ce qu'on veut prouver), 2) Choisir un **seuil de signification Œ±** (g√©n√©ralement 5%), 3) Calculer une **statistique de test** √† partir des donn√©es, 4) D√©terminer la **p-value**, 5) **D√©cision** : rejeter H‚ÇÄ si p < Œ±. **Analogie juridique** : H‚ÇÄ = 'innocent jusqu'√† preuve du contraire', les donn√©es sont les preuves, Œ± est le niveau de preuve requis, la p-value mesure la force des preuves contre l'innocence. **Types courants** : test t (comparaison de moyennes), test du œá¬≤ (ind√©pendance), ANOVA (comparaison multiple). **Erreurs possibles** : Type I (faux positif - condamner un innocent), Type II (faux n√©gatif - acquitter un coupable). **Applications** : essais cliniques (efficacit√© d'un m√©dicament), A/B testing (performance de versions), contr√¥le qualit√© (conformit√© aux standards). **Puissance** : probabilit√© de d√©tecter un effet r√©el. Les tests d'hypoth√®se sont le pilier de la recherche scientifique et de la prise de d√©cision bas√©e sur les donn√©es.",
    category: "statistiques",
    icon: "CheckCircle"
  },
  {
    term: "P-value",
    description: "La p-value est le **'niveau de surprise'** de vos donn√©es - elle mesure √† quel point vos r√©sultats observ√©s seraient improbables si l'hypoth√®se nulle √©tait vraie. **Analogie** : imaginez que vous soup√ßonnez qu'une pi√®ce est truqu√©e. Vous la lancez 100 fois et obtenez 70 faces. La p-value r√©pond √† : 'Si la pi√®ce √©tait √©quitable, quelle est la probabilit√© d'obtenir 70 faces ou plus par pur hasard ?' **Interpr√©tation** : p-value faible (< 0.05) = r√©sultats tr√®s surprenants sous H‚ÇÄ, donc on rejette H‚ÇÄ ; p-value √©lev√©e = r√©sultats pas surprenants, on ne rejette pas H‚ÇÄ. **Malentendus courants** : la p-value N'EST PAS la probabilit√© que H‚ÇÄ soit vraie, ni la probabilit√© de se tromper. **Calcul** : aire sous la courbe de distribution de la statistique de test, au-del√† de la valeur observ√©e. **Seuils conventionnels** : p < 0.05 (significatif), p < 0.01 (tr√®s significatif), p < 0.001 (hautement significatif). **Critiques** : probl√®me des comparaisons multiples, p-hacking (manipulation des analyses pour obtenir p < 0.05), sur-interpr√©tation des seuils arbitraires. **Alternatives** : intervalles de confiance, approche bay√©sienne, taille d'effet. La p-value reste un outil central mais doit √™tre interpr√©t√©e avec prudence et contexte.",
    category: "statistiques",
    icon: "Percent"
  },
  {
    term: "Intervalle de confiance",
    description: "L'intervalle de confiance est comme un **\'filet de s√©curit√© statistique\'** - une plage de valeurs qui a de bonnes chances de capturer le vrai param√®tre de population que nous cherchons √† estimer. **Analogie** : imaginez que vous essayez d\'attraper un poisson (le vrai param√®tre) avec un filet (l\'intervalle). Un filet plus large (niveau de confiance plus √©lev√©) a plus de chances d\'attraper le poisson, mais est moins pr√©cis. **Construction** : Estimation ¬± Marge d\'erreur, o√π la marge d\'erreur d√©pend du niveau de confiance souhait√© et de la variabilit√© des donn√©es. **Interpr√©tation correcte** : \'Si nous r√©p√©tions cette √©tude 100 fois, environ 95 intervalles sur 100 contiendraient le vrai param√®tre\' (pour un IC √† 95%). **Malentendu fr√©quent** : ce n\'est PAS \'il y a 95% de chances que le vrai param√®tre soit dans cet intervalle\' - le param√®tre est fixe, c\'est l\'intervalle qui varie d\'√©chantillon en √©chantillon. **Niveaux courants** : 90% (¬±1.645œÉ), 95% (¬±1.96œÉ), 99% (¬±2.576œÉ). **Applications** : sondages politiques (\'candidat X : 52% ¬±3%\'), essais cliniques (efficacit√© d\'un traitement), contr√¥le qualit√© (limites de tol√©rance). **Largeur** : d√©pend de la taille d\'√©chantillon (plus grand √©chantillon = intervalle plus √©troit) et de la variabilit√© des donn√©es. Les intervalles de confiance fournissent plus d\'information que les tests d\'hypoth√®se car ils quantifient l\'incertitude.",
    category: "statistiques",
    icon: "Target"
  },
  {
    term: "Corr√©lation",
    description: "La corr√©lation mesure √† quel point deux variables 'dansent ensemble' - si elles bougent dans la m√™me direction, en opposition, ou de mani√®re ind√©pendante. **Analogie** : observer deux danseurs - parfaitement synchronis√©s (corr√©lation +1), en opposition parfaite (-1), ou dansant ind√©pendamment (0). **Coefficient de Pearson** : mesure standard entre -1 et +1, o√π |r| proche de 1 indique une relation lin√©aire forte. **Interpr√©tation** : r > 0 (relation positive - quand X augmente, Y augmente), r < 0 (relation n√©gative - quand X augmente, Y diminue), r ‚âà 0 (pas de relation lin√©aire). **R√®gles empiriques** : |r| < 0.3 (faible), 0.3-0.7 (mod√©r√©e), > 0.7 (forte). **Types** : Pearson (lin√©aire), Spearman (monotone), Kendall (rang). **Applications** : finance (diversification de portefeuille), marketing (prix vs demande), sant√© (facteurs de risque), m√©t√©o (temp√©rature vs pression). **Pi√®ges** : corr√©lation ‚â† causalit√© ! Deux variables peuvent √™tre corr√©l√©es par hasard ou via une troisi√®me variable cach√©e. **Visualisation** : nuage de points (scatter plot) r√©v√®le la nature de la relation. **Importance** : base de la r√©gression, analyse factorielle, et d√©tection de multicolin√©arit√©.",
    category: "statistiques",
    icon: "GitBranch"
  },
  {
    term: "R√©gression",
    description: "La r√©gression est comme **'dessiner la meilleure ligne'** √† travers un nuage de points pour capturer la relation entre variables et faire des pr√©dictions. **Analogie** : imaginez que vous essayez de pr√©dire le prix d'une maison (Y) bas√© sur sa superficie (X). La r√©gression trouve la ligne qui passe 'au plus pr√®s' de tous les points (maisons) pour minimiser les erreurs de pr√©diction. **Types principaux** : **Lin√©aire simple** (Y = a + bX, une seule variable explicative), **Multiple** (plusieurs variables : prix = f(superficie, chambres, quartier)), **Polynomiale** (relations courbes), **Logistique** (pour variables binaires). **M√©thode des moindres carr√©s** : trouve la ligne qui minimise la somme des carr√©s des r√©sidus (erreurs de pr√©diction). **Hypoth√®ses cl√©s** : lin√©arit√©, ind√©pendance des erreurs, homosc√©dasticit√© (variance constante), normalit√© des r√©sidus. **√âvaluation** : R¬≤ (pourcentage de variance expliqu√©e), RMSE (erreur moyenne), analyse des r√©sidus. **Applications** : finance (mod√®les de pricing), marketing (impact publicitaire), √©conomie (√©lasticit√© prix-demande), sciences (relations dose-effet). **Interpr√©tation** : les coefficients indiquent l'impact d'une unit√© d'augmentation de X sur Y. **Extensions** : r√©gression ridge/lasso (r√©gularisation), r√©gression robuste (r√©sistante aux outliers). La r√©gression est l'un des outils les plus utilis√©s en data science pour comprendre et pr√©dire.",
    category: "statistiques",
    icon: "TrendingUp"
  },
  {
    term: "Statistiques bay√©siennes (Bayesian Statistics)",
    description: "Les statistiques bay√©siennes fonctionnent comme un **d√©tective qui met √† jour ses hypoth√®ses** √† chaque nouvel indice d√©couvert - elles permettent d'incorporer syst√©matiquement de nouvelles preuves pour affiner nos croyances. **Philosophie r√©volutionnaire** : contrairement √† l'approche fr√©quentiste (probabilit√© = fr√©quence √† long terme), l'approche bay√©sienne traite la probabilit√© comme un **degr√© de croyance** qui √©volue avec l'information. **Th√©or√®me de Bayes** : P(H|E) = P(E|H) √ó P(H) / P(E), o√π P(H|E) est la probabilit√© a posteriori (croyance mise √† jour), P(H) la probabilit√© a priori (croyance initiale), P(E|H) la vraisemblance (compatibilit√© des donn√©es avec l'hypoth√®se). **Analogie m√©dicale** : un m√©decin commence avec une probabilit√© a priori qu'un patient ait une maladie (bas√©e sur l'√¢ge, ant√©c√©dents), puis met √† jour cette probabilit√© apr√®s chaque test (sympt√¥mes, analyses). **Processus it√©ratif** : Prior ‚Üí Donn√©es ‚Üí Posterior, o√π le posterior d'aujourd'hui devient le prior de demain. **Avantages** : incorporation naturelle de connaissances pr√©alables, quantification compl√®te de l'incertitude, pr√©dictions probabilistes, gestion √©l√©gante de petits √©chantillons. **Applications** : diagnostic m√©dical, spam filtering, recommandations personnalis√©es, A/B testing, finance (gestion de risque). **Outils** : MCMC (√©chantillonnage), Stan/PyMC (logiciels), r√©seaux bay√©siens. **D√©fis** : choix du prior (subjectivit√©), complexit√© computationnelle, courbe d'apprentissage. **Renaissance moderne** : avec la puissance de calcul actuelle, les m√©thodes bay√©siennes connaissent un essor majeur en IA et data science.",
    category: "statistiques",
    icon: "RefreshCw"
  },
  {
    term: "Quantiles/Percentiles/Quartiles",
    description: "Les quantiles fonctionnent comme des **'lignes de d√©marcation'** qui divisent vos donn√©es en tranches √©gales, √† la mani√®re d'un couteau qui d√©coupe un g√¢teau en parts de taille identique. **Principe** : au lieu de regarder les valeurs absolues, on s'int√©resse aux positions relatives dans la distribution. **Percentiles** : divisent les donn√©es en 100 parts √©gales - le 75√®me percentile signifie que 75% des observations sont inf√©rieures √† cette valeur. **Analogie scolaire** : si vous √™tes au 90√®me percentile d'un examen, vous avez fait mieux que 90% des √©tudiants. **Quartiles** : cas sp√©cial qui divise en 4 parts √©gales : Q1 (25√®me percentile), Q2 (m√©diane, 50√®me percentile), Q3 (75√®me percentile). **Calcul pratique** : triez les donn√©es, puis trouvez les valeurs aux positions k√ó(n+1)/100 pour le k√®me percentile. **Applications cruciales** : 1) **Boxplots** (visualisation des quartiles et outliers), 2) **Benchmarking** (performance relative), 3) **D√©tection d'anomalies** (valeurs au-del√† de Q3 + 1.5√óIQR), 4) **Segmentation** (diviser clients en groupes). **Espace interquartile (IQR)** : Q3 - Q1, mesure robuste de dispersion r√©sistante aux outliers. **Avantages** : interpr√©tation intuitive, robustesse aux valeurs extr√™mes, applicable √† toute distribution. **Exemples concrets** : salaires (m√©diane plus repr√©sentative), temps de r√©ponse web (95√®me percentile pour SLA), croissance d'enfants (courbes de percentiles). **Diff√©rence cl√©** : contrairement √† la moyenne/√©cart-type, les quantiles ne font aucune hypoth√®se sur la forme de la distribution.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Erreurs de type I et de type II (Type I & II Errors)",
    description: "Type I : faux positifs (rejeter une hypoth√®se nulle vraie). Type II : faux n√©gatifs (accepter une hypoth√®se nulle fausse).",
    category: "statistiques",
    icon: "AlertTriangle"
  },
  {
    term: "Cha√Ænes de Markov Monte Carlo (MCMC)",
    description: "Classe d'algorithmes pour l'√©chantillonnage √† partir d'une distribution de probabilit√© complexe, utilis√©e en statistique bay√©sienne et apprentissage automatique.",
    category: "statistiques",
    icon: "GitBranch"
  },
  {
    term: "Mod√®les de Markov cach√©s (Hidden Markov Models - HMM)",
    description: "Mod√®le statistique dans lequel le syst√®me mod√©lis√© est suppos√© √™tre un processus de Markov avec des √©tats non observ√©s (cach√©s). Utilis√© en reconnaissance vocale, bioinformatique et finance.",
    category: "statistiques",
    icon: "Eye"
  },
  {
    term: "Analyse de survie (Survival Analysis)",
    description: "**La science du temps qui reste !** Comme un m√©decin qui pr√©dit l'esp√©rance de vie d'un patient ou un ing√©nieur qui estime la dur√©e de vie d'une machine, l'analyse de survie mod√©lise le temps jusqu'√† ce qu'un √©v√©nement critique se produise.\n\n**‚è∞ Analogie M√©dicale :**\nImaginez suivre 1000 patients atteints d'une maladie : certains gu√©rissent rapidement, d'autres vivent des ann√©es, quelques-uns quittent l'√©tude. L'analyse de survie extrait des insights m√™me avec ces donn√©es 'incompl√®tes'.\n\n**üéØ Concepts Fondamentaux :**\n\n**Fonction de Survie S(t) :**\n- Probabilit√© de survivre au-del√† du temps t\n- S(t) = P(T > t) o√π T = temps de survie\n- D√©croissante de 1 (t=0) vers 0 (t=‚àû)\n\n**Fonction de Risque h(t) :**\n- Taux instantan√© de d√©faillance au temps t\n- h(t) = lim[P(t ‚â§ T < t+Œît | T ‚â• t)] / Œît\n- Peut augmenter, diminuer, ou rester constant\n\n**üö® D√©fi de la Censure :**\n\n**Types de Censure :**\n‚Ä¢ **Droite** : √âv√©nement non observ√© √† la fin de l'√©tude\n‚Ä¢ **Gauche** : √âv√©nement d√©j√† survenu au d√©but\n‚Ä¢ **Intervalle** : √âv√©nement dans une p√©riode connue\n‚Ä¢ **Informative** : Censure li√©e au risque d'√©v√©nement\n\n**Impact Critique :**\n- Ignorer la censure ‚Üí Biais majeurs\n- Sous-estimation des temps de survie\n- Conclusions erron√©es sur l'efficacit√©\n\n**üìä M√©thodes Classiques :**\n\n**Estimateur de Kaplan-Meier :**\n- Estimation non-param√©trique de S(t)\n- Courbes de survie en escalier\n- Intervalles de confiance\n- Test du log-rank pour comparaisons\n\n**Mod√®le de Cox (Proportional Hazards) :**\n- h(t|x) = h‚ÇÄ(t) √ó exp(Œ≤x)\n- Semi-param√©trique (pas d'hypoth√®se sur h‚ÇÄ)\n- Hazard Ratios pour interpr√©ter les effets\n- Standard en recherche m√©dicale\n\n**Mod√®les Param√©triques :**\n- **Weibull** : Risque monotone (croissant/d√©croissant)\n- **Exponentiel** : Risque constant\n- **Log-normal** : Risque en cloche\n- **Gamma g√©n√©ralis√©** : Tr√®s flexible\n\n**üéØ Applications Diversifi√©es :**\n\n**M√©decine & Sant√© :**\n- Essais cliniques (survie patients)\n- √âpid√©miologie (progression maladie)\n- Pharmacovigilance (effets secondaires)\n\n**Business & Marketing :**\n- **Customer Churn** : Temps avant d√©sabonnement\n- **CLV** : Customer Lifetime Value\n- **R√©tention** : Dur√©e d'engagement client\n\n**Ing√©nierie & Fiabilit√© :**\n- Dur√©e de vie des composants\n- Maintenance pr√©dictive\n- Analyse des pannes syst√®me\n\n**Finance :**\n- D√©faut de cr√©dit\n- Dur√©e des investissements\n- Risque de march√©\n\n**üõ†Ô∏è Outils Modernes :**\n\n**Packages R :**\n- `survival` : Fonctions de base\n- `survminer` : Visualisations √©l√©gantes\n- `flexsurv` : Mod√®les flexibles\n\n**Python :**\n- `lifelines` : Complet et intuitif\n- `scikit-survival` : Int√©gration sklearn\n- `pycox` : Deep learning pour survie\n\n**üìà Extensions Avanc√©es :**\n\n**Mod√®les Multi-√©tats :**\n- Transitions entre √©tats multiples\n- Maladie ‚Üí R√©mission ‚Üí Rechute ‚Üí D√©c√®s\n\n**Survie Concurrente :**\n- Risques comp√©titifs multiples\n- D√©c√®s par cancer vs autres causes\n\n**Machine Learning :**\n- **Random Survival Forest** : Ensembles d'arbres\n- **DeepSurv** : R√©seaux de neurones\n- **DeepHit** : Risques concurrents\n\n**‚ö° M√©triques d'√âvaluation :**\n- **C-index** : Concordance (√©quivalent AUC)\n- **Brier Score** : Erreur de pr√©diction temporelle\n- **IBS** : Integrated Brier Score\n- **Time-dependent AUC** : Performance temporelle\n\n**üí° Insights Strat√©giques :**\nNetflix utilise l'analyse de survie pour pr√©dire le churn avec 85% de pr√©cision, permettant des interventions cibl√©es qui r√©duisent l'attrition de 23%. En m√©decine, elle guide 90% des d√©cisions th√©rapeutiques en oncologie.",
    category: "statistiques",
    icon: "Clock"
  }
];