/**
 * Statistical Methods and Concepts
 * Core statistical concepts used in data science
 */

import { GlossaryEntry } from './types';

export const statisticsTerms: GlossaryEntry[] = [
  {
    term: "Statistiques",
    description: "Les Statistiques constituent la science fondamentale qui transforme les donn√©es brutes en connaissances exploitables. Imaginez un traducteur universel : les statistiques traduisent le **langage** des donn√©es en insights compr√©hensibles pour la prise de d√©cision. Cette discipline englobe quatre piliers essentiels : la **collecte** (comment obtenir des donn√©es repr√©sentatives), l'**analyse** (application de m√©thodes math√©matiques), l'**interpr√©tation** (donner du sens aux r√©sultats), et la **pr√©sentation** (communiquer efficacement les findings). Les statistiques nous permettent de naviguer dans l'incertitude, de distinguer les signaux du bruit, et de faire des pr√©dictions fiables. Elles constituent le socle math√©matique de la data science, fournissant les outils pour tester des hypoth√®ses, quantifier la confiance dans nos conclusions, et g√©n√©raliser des observations d'√©chantillons √† des populations enti√®res. Des sondages d'opinion aux essais cliniques, des analyses de march√© aux pr√©visions m√©t√©orologiques, les statistiques sont omnipr√©sentes dans notre soci√©t√© moderne.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Population vs √âchantillon",
    description: "Cette distinction fondamentale est comme la diff√©rence entre photographier une foule enti√®re versus prendre un instantan√© repr√©sentatif. La **Population** repr√©sente l'ensemble complet et exhaustif de tous les √©l√©ments, individus, ou observations qui nous int√©ressent dans notre √©tude (par exemple, tous les citoyens fran√ßais, toutes les entreprises du CAC 40, ou tous les patients atteints d'une maladie sp√©cifique). L'**√âchantillon** est un sous-ensemble soigneusement s√©lectionn√© de cette population, choisi pour √™tre repr√©sentatif et permettre des inf√©rences valides. La qualit√© de l'√©chantillonnage est cruciale : un √©chantillon biais√© peut conduire √† des conclusions erron√©es. Les m√©thodes d'√©chantillonnage incluent l'√©chantillonnage al√©atoire simple, stratifi√©, ou par grappes. La **taille d'√©chantillon** influence directement la pr√©cision des estimations : plus l'√©chantillon est grand, plus nos conclusions sont fiables, mais les co√ªts augmentent. Cette distinction est essentielle car √©tudier une population enti√®re est souvent impossible (co√ªt, temps, accessibilit√©), d'o√π l'importance de ma√Ætriser les techniques d'√©chantillonnage pour g√©n√©raliser nos r√©sultats.",
    category: "statistiques",
    icon: "Users"
  },
  {
    term: "Moyenne (Mean)",
    description: "La moyenne arithm√©tique est comme le 'centre de gravit√©' de vos donn√©es. Calcul√©e en additionnant toutes les valeurs et en divisant par le nombre d'observations (Œ£x/n), elle repr√©sente la valeur typique autour de laquelle les donn√©es gravitent. **Avantages** : facile √† calculer, utilise toutes les donn√©es, base de nombreux tests statistiques. **Inconv√©nients** : tr√®s sensible aux valeurs aberrantes (outliers). Par exemple, si 9 personnes gagnent 30k‚Ç¨ et une 10√®me gagne 300k‚Ç¨, la moyenne (57k‚Ç¨) ne repr√©sente pas bien le groupe. **Applications** : calcul de performances moyennes, analyses financi√®res, contr√¥le qualit√©. **Variantes** : moyenne pond√©r√©e (certaines valeurs comptent plus), moyenne g√©om√©trique (pour les taux de croissance), moyenne harmonique (pour les vitesses). La moyenne est la mesure de tendance centrale la plus utilis√©e en statistiques inf√©rentielles et constitue la base de concepts avanc√©s comme la variance et l'√©cart-type.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "M√©diane (Median)",
    description: "La m√©diane est la 'valeur du milieu' qui divise vos donn√©es en deux moiti√©s √©gales, comme un m√©diateur qui s√©pare √©quitablement deux groupes. Pour la calculer : triez les donn√©es par ordre croissant, puis prenez la valeur centrale (si n impair) ou la moyenne des deux valeurs centrales (si n pair). **Avantage majeur** : robuste aux valeurs aberrantes - elle r√©siste aux extr√™mes. Dans l'exemple pr√©c√©dent (9 personnes √† 30k‚Ç¨, 1 √† 300k‚Ç¨), la m√©diane reste 30k‚Ç¨, plus repr√©sentative. **Applications pratiques** : salaires (m√©diane plus repr√©sentative que moyenne), prix immobiliers, scores de satisfaction. **Interpr√©tation** : 50% des observations sont inf√©rieures √† la m√©diane, 50% sup√©rieures. **Comparaison avec la moyenne** : si m√©diane < moyenne, distribution asym√©trique vers la droite (queue positive) ; si m√©diane > moyenne, asym√©trie vers la gauche. La m√©diane est essentielle en statistiques descriptives et particuli√®rement utile pour les donn√©es √©conomiques et sociales.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Mode",
    description: "Le mode est la 'star' de vos donn√©es - la valeur qui appara√Æt le plus fr√©quemment, comme la chanson la plus jou√©e sur une playlist. **Identification** : comptez la fr√©quence de chaque valeur, le mode est celle avec le maximum d'occurrences. **Types de distributions** : unimodale (un seul mode), bimodale (deux modes), multimodale (plusieurs modes), ou amodale (pas de mode clair). **Particularit√©s** : seule mesure de tendance centrale applicable aux donn√©es qualitatives (couleur pr√©f√©r√©e, marque favorite). **Applications** : √©tudes de march√© (produit le plus vendu), contr√¥le qualit√© (d√©faut le plus fr√©quent), analyses d√©mographiques (√¢ge modal). **Avantages** : facile √† identifier visuellement, r√©sistant aux valeurs aberrantes, applicable √† tous types de donn√©es. **Limites** : peut ne pas exister ou √™tre multiple, moins utilis√© en statistiques inf√©rentielles. **Exemple pratique** : dans une enqu√™te sur les tailles de chaussures, si la taille 42 revient 15 fois (plus que toute autre), c'est le mode. Le mode compl√®te utilement la moyenne et la m√©diane pour une description compl√®te de la distribution.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Variance",
    description: "La variance mesure √† quel point vos donn√©es sont 'dispers√©es' autour de la moyenne, comme mesurer l'√©talement d'un groupe de personnes autour d'un point de rassemblement. **Calcul** : moyenne des carr√©s des √©carts √† la moyenne : Var(X) = Œ£(xi - Œº)¬≤/n (population) ou Œ£(xi - xÃÑ)¬≤/(n-1) (√©chantillon). **Pourquoi √©lever au carr√© ?** Cela √©vite que les √©carts positifs et n√©gatifs s'annulent, et donne plus de poids aux grandes d√©viations. **Interpr√©tation** : variance faible = donn√©es concentr√©es pr√®s de la moyenne (groupe homog√®ne) ; variance √©lev√©e = donn√©es tr√®s dispers√©es (groupe h√©t√©rog√®ne). **Probl√®me pratique** : l'unit√© est le carr√© de l'unit√© originale (si les donn√©es sont en euros, la variance est en euros¬≤), ce qui complique l'interpr√©tation. **Applications** : finance (mesure du risque d'un investissement), contr√¥le qualit√© (consistance d'un processus), recherche (variabilit√© entre sujets). La variance est fondamentale en statistiques car elle quantifie l'incertitude et sert de base √† de nombreux tests statistiques et mod√®les pr√©dictifs.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "√âcart-type (Standard Deviation)",
    description: "L'√©cart-type est la 'version lisible' de la variance - sa racine carr√©e qui remet les unit√©s dans leur forme originale. Si la variance est comme mesurer une surface, l'√©cart-type est comme mesurer une distance. **Calcul** : œÉ = ‚àöVariance. **Avantage majeur** : m√™me unit√© que les donn√©es originales, donc directement interpr√©table. **R√®gle empirique (loi normale)** : environ 68% des donn√©es se trouvent √† ¬±1œÉ de la moyenne, 95% √† ¬±2œÉ, 99.7% √† ¬±3œÉ. **Applications pratiques** : en finance, un √©cart-type de 15% sur les rendements d'une action indique sa volatilit√© ; en production, un √©cart-type faible indique un processus stable. **Comparaisons** : permet de comparer la variabilit√© entre diff√©rents datasets ou variables. **√âcart-type vs √©tendue** : l'√©cart-type utilise toutes les donn√©es (plus robuste) tandis que l'√©tendue ne consid√®re que min et max (sensible aux outliers). **Standardisation** : l'√©cart-type permet de cr√©er des scores Z pour comparer des valeurs de distributions diff√©rentes. C'est l'une des mesures les plus importantes en statistiques descriptives et inf√©rentielles.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Distribution normale (Gaussian)",
    description: "La distribution normale est la 'reine' des distributions statistiques, ressemblant √† une cloche parfaitement sym√©trique o√π la plupart des valeurs se concentrent au centre et diminuent graduellement vers les extr√™mes. **Analogie** : la r√©partition des tailles dans une population - peu de personnes tr√®s petites ou tr√®s grandes, la majorit√© autour de la moyenne. **Param√®tres** : enti√®rement d√©finie par sa moyenne Œº (centre) et son √©cart-type œÉ (largeur). **Propri√©t√©s remarquables** : sym√©trie parfaite, moyenne = m√©diane = mode, aires sous la courbe d√©finies (68-95-99.7 rule). **Ubiquit√© naturelle** : erreurs de mesure, caract√©ristiques biologiques, ph√©nom√®nes sociaux - le **Th√©or√®me Central Limite** explique pourquoi tant de ph√©nom√®nes suivent cette loi. **Applications** : tests statistiques (t-test, ANOVA), intervalles de confiance, contr√¥le qualit√© (Six Sigma), finance (mod√®les de risque). **Standardisation** : toute normale peut √™tre transform√©e en normale standard (Œº=0, œÉ=1) via Z = (X-Œº)/œÉ. **Importance historique** : d√©couverte par Gauss et Laplace, fondement de la statistique moderne. **Reconnaissance** : si vos donn√©es forment une courbe en cloche, vous pouvez appliquer de puissants outils statistiques param√©triques.",
    category: "statistiques",
    icon: "TrendingUp"
  },
  {
    term: "Probabilit√©",
    description: "La probabilit√© quantifie l'incertitude et mesure nos 'chances' qu'un √©v√©nement se r√©alise, comme un barom√®tre de la vraisemblance qui oscille entre l'impossible (0) et le certain (1). **Analogie** : pr√©dire la m√©t√©o - 0% = soleil garanti, 100% = pluie certaine, 70% = probablement pluvieux. **√âchelle** : toujours entre 0 et 1 (ou 0% et 100%), o√π 0.5 = √©quiprobable (pile ou face). **Interpr√©tations** : 1) **Fr√©quentiste** (r√©p√©tition infinie d'exp√©riences), 2) **Subjective** (degr√© de croyance personnel), 3) **Classique** (cas favorables/cas possibles). **R√®gles fondamentales** : P(A) + P(non-A) = 1, P(A ou B) = P(A) + P(B) - P(A et B), P(A et B) = P(A) √ó P(B|A). **Applications** : jeux de hasard, assurance (calcul des primes), m√©decine (diagnostic), finance (gestion des risques), machine learning (classification probabiliste). **Distributions** : uniforme (d√© √©quilibr√©), binomiale (succ√®s/√©chec), normale (ph√©nom√®nes naturels). **Th√©or√®me de Bayes** : mise √† jour des probabilit√©s avec nouvelles informations. **Impact** : fondement de la statistique inf√©rentielle, de l'IA probabiliste, et de la prise de d√©cision sous incertitude.",
    category: "statistiques",
    icon: "Percent"
  },
  {
    term: "Test d'hypoth√®se",
    description: "Le test d'hypoth√®se est comme un **proc√®s judiciaire** pour vos donn√©es - une proc√©dure rigoureuse qui d√©termine si une affirmation sur une population est cr√©dible ou doit √™tre rejet√©e. **Processus** : 1) Formuler l'**hypoth√®se nulle H‚ÇÄ** (status quo, 'pas d'effet') et l'**hypoth√®se alternative H‚ÇÅ** (ce qu'on veut prouver), 2) Choisir un **seuil de signification Œ±** (g√©n√©ralement 5%), 3) Calculer une **statistique de test** √† partir des donn√©es, 4) D√©terminer la **p-value**, 5) **D√©cision** : rejeter H‚ÇÄ si p < Œ±. **Analogie juridique** : H‚ÇÄ = 'innocent jusqu'√† preuve du contraire', les donn√©es sont les preuves, Œ± est le niveau de preuve requis, la p-value mesure la force des preuves contre l'innocence. **Types courants** : test t (comparaison de moyennes), test du œá¬≤ (ind√©pendance), ANOVA (comparaison multiple). **Erreurs possibles** : Type I (faux positif - condamner un innocent), Type II (faux n√©gatif - acquitter un coupable). **Applications** : essais cliniques (efficacit√© d'un m√©dicament), A/B testing (performance de versions), contr√¥le qualit√© (conformit√© aux standards). **Puissance** : probabilit√© de d√©tecter un effet r√©el. Les tests d'hypoth√®se sont le pilier de la recherche scientifique et de la prise de d√©cision bas√©e sur les donn√©es.",
    category: "statistiques",
    icon: "CheckCircle"
  },
  {
    term: "P-value",
    description: "La p-value est le **'niveau de surprise'** de vos donn√©es - elle mesure √† quel point vos r√©sultats observ√©s seraient improbables si l'hypoth√®se nulle √©tait vraie. **Analogie** : imaginez que vous soup√ßonnez qu'une pi√®ce est truqu√©e. Vous la lancez 100 fois et obtenez 70 faces. La p-value r√©pond √† : 'Si la pi√®ce √©tait √©quitable, quelle est la probabilit√© d'obtenir 70 faces ou plus par pur hasard ?' **Interpr√©tation** : p-value faible (< 0.05) = r√©sultats tr√®s surprenants sous H‚ÇÄ, donc on rejette H‚ÇÄ ; p-value √©lev√©e = r√©sultats pas surprenants, on ne rejette pas H‚ÇÄ. **Malentendus courants** : la p-value N'EST PAS la probabilit√© que H‚ÇÄ soit vraie, ni la probabilit√© de se tromper. **Calcul** : aire sous la courbe de distribution de la statistique de test, au-del√† de la valeur observ√©e. **Seuils conventionnels** : p < 0.05 (significatif), p < 0.01 (tr√®s significatif), p < 0.001 (hautement significatif). **Critiques** : probl√®me des comparaisons multiples, p-hacking (manipulation des analyses pour obtenir p < 0.05), sur-interpr√©tation des seuils arbitraires. **Alternatives** : intervalles de confiance, approche bay√©sienne, taille d'effet. La p-value reste un outil central mais doit √™tre interpr√©t√©e avec prudence et contexte.",
    category: "statistiques",
    icon: "Percent"
  },
  {
    term: "Intervalle de confiance",
    description: "L'intervalle de confiance est comme un **\'filet de s√©curit√© statistique\'** - une plage de valeurs qui a de bonnes chances de capturer le vrai param√®tre de population que nous cherchons √† estimer. **Analogie** : imaginez que vous essayez d\'attraper un poisson (le vrai param√®tre) avec un filet (l\'intervalle). Un filet plus large (niveau de confiance plus √©lev√©) a plus de chances d\'attraper le poisson, mais est moins pr√©cis. **Construction** : Estimation ¬± Marge d\'erreur, o√π la marge d\'erreur d√©pend du niveau de confiance souhait√© et de la variabilit√© des donn√©es. **Interpr√©tation correcte** : \'Si nous r√©p√©tions cette √©tude 100 fois, environ 95 intervalles sur 100 contiendraient le vrai param√®tre\' (pour un IC √† 95%). **Malentendu fr√©quent** : ce n\'est PAS \'il y a 95% de chances que le vrai param√®tre soit dans cet intervalle\' - le param√®tre est fixe, c\'est l\'intervalle qui varie d\'√©chantillon en √©chantillon. **Niveaux courants** : 90% (¬±1.645œÉ), 95% (¬±1.96œÉ), 99% (¬±2.576œÉ). **Applications** : sondages politiques (\'candidat X : 52% ¬±3%\'), essais cliniques (efficacit√© d\'un traitement), contr√¥le qualit√© (limites de tol√©rance). **Largeur** : d√©pend de la taille d\'√©chantillon (plus grand √©chantillon = intervalle plus √©troit) et de la variabilit√© des donn√©es. Les intervalles de confiance fournissent plus d\'information que les tests d\'hypoth√®se car ils quantifient l\'incertitude.",
    category: "statistiques",
    icon: "Target"
  },
  {
    term: "Corr√©lation",
    description: "La corr√©lation mesure √† quel point deux variables 'dansent ensemble' - si elles bougent dans la m√™me direction, en opposition, ou de mani√®re ind√©pendante. **Analogie** : observer deux danseurs - parfaitement synchronis√©s (corr√©lation +1), en opposition parfaite (-1), ou dansant ind√©pendamment (0). **Coefficient de Pearson** : mesure standard entre -1 et +1, o√π |r| proche de 1 indique une relation lin√©aire forte. **Interpr√©tation** : r > 0 (relation positive - quand X augmente, Y augmente), r < 0 (relation n√©gative - quand X augmente, Y diminue), r ‚âà 0 (pas de relation lin√©aire). **R√®gles empiriques** : |r| < 0.3 (faible), 0.3-0.7 (mod√©r√©e), > 0.7 (forte). **Types** : Pearson (lin√©aire), Spearman (monotone), Kendall (rang). **Applications** : finance (diversification de portefeuille), marketing (prix vs demande), sant√© (facteurs de risque), m√©t√©o (temp√©rature vs pression). **Pi√®ges** : corr√©lation ‚â† causalit√© ! Deux variables peuvent √™tre corr√©l√©es par hasard ou via une troisi√®me variable cach√©e. **Visualisation** : nuage de points (scatter plot) r√©v√®le la nature de la relation. **Importance** : base de la r√©gression, analyse factorielle, et d√©tection de multicolin√©arit√©.",
    category: "statistiques",
    icon: "GitBranch"
  },
  {
    term: "R√©gression",
    description: "La r√©gression est comme **'dessiner la meilleure ligne'** √† travers un nuage de points pour capturer la relation entre variables et faire des pr√©dictions. **Analogie** : imaginez que vous essayez de pr√©dire le prix d'une maison (Y) bas√© sur sa superficie (X). La r√©gression trouve la ligne qui passe 'au plus pr√®s' de tous les points (maisons) pour minimiser les erreurs de pr√©diction. **Types principaux** : **Lin√©aire simple** (Y = a + bX, une seule variable explicative), **Multiple** (plusieurs variables : prix = f(superficie, chambres, quartier)), **Polynomiale** (relations courbes), **Logistique** (pour variables binaires). **M√©thode des moindres carr√©s** : trouve la ligne qui minimise la somme des carr√©s des r√©sidus (erreurs de pr√©diction). **Hypoth√®ses cl√©s** : lin√©arit√©, ind√©pendance des erreurs, homosc√©dasticit√© (variance constante), normalit√© des r√©sidus. **√âvaluation** : R¬≤ (pourcentage de variance expliqu√©e), RMSE (erreur moyenne), analyse des r√©sidus. **Applications** : finance (mod√®les de pricing), marketing (impact publicitaire), √©conomie (√©lasticit√© prix-demande), sciences (relations dose-effet). **Interpr√©tation** : les coefficients indiquent l'impact d'une unit√© d'augmentation de X sur Y. **Extensions** : r√©gression ridge/lasso (r√©gularisation), r√©gression robuste (r√©sistante aux outliers). La r√©gression est l'un des outils les plus utilis√©s en data science pour comprendre et pr√©dire.",
    category: "statistiques",
    icon: "TrendingUp"
  },
  {
    term: "Statistiques bay√©siennes (Bayesian Statistics)",
    description: "Les statistiques bay√©siennes fonctionnent comme un **d√©tective qui met √† jour ses hypoth√®ses** √† chaque nouvel indice d√©couvert - elles permettent d'incorporer syst√©matiquement de nouvelles preuves pour affiner nos croyances. **Philosophie r√©volutionnaire** : contrairement √† l'approche fr√©quentiste (probabilit√© = fr√©quence √† long terme), l'approche bay√©sienne traite la probabilit√© comme un **degr√© de croyance** qui √©volue avec l'information. **Th√©or√®me de Bayes** : P(H|E) = P(E|H) √ó P(H) / P(E), o√π P(H|E) est la probabilit√© a posteriori (croyance mise √† jour), P(H) la probabilit√© a priori (croyance initiale), P(E|H) la vraisemblance (compatibilit√© des donn√©es avec l'hypoth√®se). **Analogie m√©dicale** : un m√©decin commence avec une probabilit√© a priori qu'un patient ait une maladie (bas√©e sur l'√¢ge, ant√©c√©dents), puis met √† jour cette probabilit√© apr√®s chaque test (sympt√¥mes, analyses). **Processus it√©ratif** : Prior ‚Üí Donn√©es ‚Üí Posterior, o√π le posterior d'aujourd'hui devient le prior de demain. **Avantages** : incorporation naturelle de connaissances pr√©alables, quantification compl√®te de l'incertitude, pr√©dictions probabilistes, gestion √©l√©gante de petits √©chantillons. **Applications** : diagnostic m√©dical, spam filtering, recommandations personnalis√©es, A/B testing, finance (gestion de risque). **Outils** : MCMC (√©chantillonnage), Stan/PyMC (logiciels), r√©seaux bay√©siens. **D√©fis** : choix du prior (subjectivit√©), complexit√© computationnelle, courbe d'apprentissage. **Renaissance moderne** : avec la puissance de calcul actuelle, les m√©thodes bay√©siennes connaissent un essor majeur en IA et data science.",
    category: "statistiques",
    icon: "RefreshCw"
  },
  {
    term: "Quantiles/Percentiles/Quartiles",
    description: "Les quantiles fonctionnent comme des **'lignes de d√©marcation'** qui divisent vos donn√©es en tranches √©gales, √† la mani√®re d'un couteau qui d√©coupe un g√¢teau en parts de taille identique. **Principe** : au lieu de regarder les valeurs absolues, on s'int√©resse aux positions relatives dans la distribution. **Percentiles** : divisent les donn√©es en 100 parts √©gales - le 75√®me percentile signifie que 75% des observations sont inf√©rieures √† cette valeur. **Analogie scolaire** : si vous √™tes au 90√®me percentile d'un examen, vous avez fait mieux que 90% des √©tudiants. **Quartiles** : cas sp√©cial qui divise en 4 parts √©gales : Q1 (25√®me percentile), Q2 (m√©diane, 50√®me percentile), Q3 (75√®me percentile). **Calcul pratique** : triez les donn√©es, puis trouvez les valeurs aux positions k√ó(n+1)/100 pour le k√®me percentile. **Applications cruciales** : 1) **Boxplots** (visualisation des quartiles et outliers), 2) **Benchmarking** (performance relative), 3) **D√©tection d'anomalies** (valeurs au-del√† de Q3 + 1.5√óIQR), 4) **Segmentation** (diviser clients en groupes). **Espace interquartile (IQR)** : Q3 - Q1, mesure robuste de dispersion r√©sistante aux outliers. **Avantages** : interpr√©tation intuitive, robustesse aux valeurs extr√™mes, applicable √† toute distribution. **Exemples concrets** : salaires (m√©diane plus repr√©sentative), temps de r√©ponse web (95√®me percentile pour SLA), croissance d'enfants (courbes de percentiles). **Diff√©rence cl√©** : contrairement √† la moyenne/√©cart-type, les quantiles ne font aucune hypoth√®se sur la forme de la distribution.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Erreurs de type I et de type II (Type I & II Errors)",
    description: "Les erreurs de Type I et II sont comme les **'erreurs judiciaires'** des statistiques - elles repr√©sentent les deux fa√ßons dont nous pouvons nous tromper lors d'un test d'hypoth√®se. **Analogie juridique** : imaginez un proc√®s o√π l'accus√© est soit innocent (H‚ÇÄ vraie) soit coupable (H‚ÇÄ fausse). **Erreur de Type I (Œ±)** : condamner un innocent - rejeter H‚ÇÄ alors qu'elle est vraie (faux positif). C'est comme d√©clarer qu'un m√©dicament est efficace alors qu'il ne l'est pas. **Probabilit√©** : Œ± = P(rejeter H‚ÇÄ | H‚ÇÄ vraie), g√©n√©ralement fix√©e √† 5%. **Erreur de Type II (Œ≤)** : acquitter un coupable - accepter H‚ÇÄ alors qu'elle est fausse (faux n√©gatif). C'est comme ne pas d√©tecter l'efficacit√© d'un m√©dicament qui fonctionne r√©ellement. **Probabilit√©** : Œ≤ = P(accepter H‚ÇÄ | H‚ÇÄ fausse). **Puissance statistique** : 1-Œ≤, probabilit√© de d√©tecter un effet r√©el. **Trade-off fondamental** : r√©duire Œ± augmente Œ≤ et vice-versa - on ne peut pas minimiser les deux simultan√©ment sans augmenter la taille d'√©chantillon. **Applications critiques** : m√©decine (diagnostic), contr√¥le qualit√© (d√©fauts), s√©curit√© (d√©tection de menaces). **Cons√©quences** : Type I peut conduire √† des d√©cisions co√ªteuses bas√©es sur de fausses preuves ; Type II peut faire rater des opportunit√©s importantes. **Facteurs d'influence** : taille d'√©chantillon (plus grand = moins d'erreurs), taille d'effet (effet plus grand = moins d'erreur Type II), variabilit√© des donn√©es. **Strat√©gies de mitigation** : calcul de puissance a priori, tests adaptatifs, approches bay√©siennes. **Contexte moderne** : avec le Big Data, l'erreur Type I devient critique (probl√®me des comparaisons multiples), n√©cessitant des corrections comme Bonferroni ou FDR (False Discovery Rate).",
    category: "statistiques",
    icon: "AlertTriangle"
  },
  {
    term: "Cha√Ænes de Markov Monte Carlo (MCMC)",
    description: "**üé≤ L'Art de l'Exploration Probabiliste !**\n\nComme un explorateur m√©thodique qui d√©couvre un territoire inconnu en suivant des r√®gles pr√©cises, MCMC r√©volutionne l'√©chantillonnage de distributions complexes en cr√©ant une cha√Æne d'√©tats o√π chaque √©tape d√©pend uniquement de la pr√©c√©dente, permettant d'explorer efficacement des espaces probabilistes de haute dimension.\n\n**üó∫Ô∏è Analogie de l'Explorateur :**\nImaginez un explorateur dans une r√©gion montagneuse brumeuse. Il ne peut voir que sa position actuelle et les environs imm√©diats. Pour cartographier la r√©gion, il suit une r√®gle simple : √† chaque √©tape, il propose un nouveau lieu √† visiter bas√© sur sa position actuelle, et d√©cide d'y aller selon certains crit√®res. Apr√®s des milliers d'√©tapes, son parcours r√©v√®le la topographie compl√®te !\n\n**‚öôÔ∏è Fondements Th√©oriques :**\n\n**Propri√©t√© de Markov :**\n```\nP(X_{t+1} | X_t, X_{t-1}, ..., X_0) = P(X_{t+1} | X_t)\n```\n*L'avenir ne d√©pend que du pr√©sent, pas du pass√©*\n\n**Cha√Æne de Markov :**\n- **√âtats** : Valeurs possibles des param√®tres\n- **Transitions** : Probabilit√©s de passage entre √©tats\n- **Stationnarit√©** : Distribution limite invariante\n- **Ergodicit√©** : Convergence vers la distribution cible\n\n**Th√©or√®me Fondamental :**\nSi la cha√Æne est irr√©ductible et ap√©riodique, alors :\n```\nlim_{n‚Üí‚àû} (1/n) Œ£ f(X_i) = E_œÄ[f(X)]\n```\n*La moyenne empirique converge vers l'esp√©rance th√©orique*\n\n**üéØ Algorithmes Principaux :**\n\n**Metropolis-Hastings :**\n```\nAlgorithme Metropolis-Hastings:\n1. √âtat actuel : x_t\n2. Proposer : x' ~ q(x'|x_t)\n3. Calculer ratio : Œ± = min(1, [œÄ(x')q(x_t|x')] / [œÄ(x_t)q(x'|x_t)])\n4. Accepter x' avec probabilit√© Œ±\n5. Sinon garder x_t\n```\n\n**Avantages :**\n- **Universalit√©** : Fonctionne pour toute distribution\n- **Simplicit√©** : Facile √† impl√©menter\n- **Flexibilit√©** : Nombreuses variantes possibles\n\n**Gibbs Sampling :**\n```\nAlgorithme de Gibbs:\nPour chaque variable X_i:\n  X_i^{(t+1)} ~ P(X_i | X_{-i}^{(t+1)}, X_{-i}^{(t)})\n```\n\n**Conditions d'Application :**\n- **Conditionnelles Connues** : Distributions conditionnelles calculables\n- **Sampling Direct** : √âchantillonnage direct possible\n- **Efficacit√©** : Convergence souvent plus rapide\n\n**Hamiltonian Monte Carlo (HMC) :**\n```\nDynamique Hamiltonienne:\ndq/dt = ‚àÇH/‚àÇp\ndp/dt = -‚àÇH/‚àÇq\n\nH(q,p) = U(q) + K(p)\nU(q) = -log œÄ(q)  # √ânergie potentielle\nK(p) = p¬≤/2m      # √ânergie cin√©tique\n```\n\n**R√©volution HMC :**\n- **Gradient Information** : Utilise les gradients de la log-densit√©\n- **Exploration Efficace** : √âvite la marche al√©atoire\n- **Haute Dimension** : Excellent pour espaces complexes\n- **Stan/PyMC** : Impl√©mentations modernes\n\n**üî¨ Applications en Machine Learning :**\n\n**Inf√©rence Bay√©sienne :**\n```\nMod√®le Bay√©sien:\nP(Œ∏|D) ‚àù P(D|Œ∏) √ó P(Œ∏)\n\n√âchantillonnage MCMC:\nŒ∏^{(1)}, Œ∏^{(2)}, ..., Œ∏^{(N)} ~ P(Œ∏|D)\n\nEstimation:\nE[Œ∏|D] ‚âà (1/N) Œ£ Œ∏^{(i)}\n```\n\n**R√©seaux de Neurones Bay√©siens :**\n- **Incertitude** : Distribution sur les poids\n- **Regularization** : Priors sur les param√®tres\n- **Calibration** : Pr√©dictions avec intervalles de confiance\n- **Robustesse** : R√©sistance √† l'overfitting\n\n**Mod√®les Graphiques :**\n- **Variables Latentes** : √âchantillonnage des √©tats cach√©s\n- **Topic Models** : LDA, allocation de sujets\n- **Collaborative Filtering** : Factorisation matricielle\n- **Social Networks** : Mod√®les de communaut√©s\n\n**üß† Deep Learning et MCMC :**\n\n**Bayesian Neural Networks :**\n```python\n# PyMC3 Example\nwith pm.Model() as model:\n    # Priors sur les poids\n    w1 = pm.Normal('w1', 0, 1, shape=(input_dim, hidden_dim))\n    w2 = pm.Normal('w2', 0, 1, shape=(hidden_dim, output_dim))\n    \n    # Forward pass\n    hidden = pm.math.tanh(pm.math.dot(X, w1))\n    output = pm.math.dot(hidden, w2)\n    \n    # Likelihood\n    y_obs = pm.Normal('y_obs', output, sigma, observed=y)\n    \n    # MCMC Sampling\n    trace = pm.sample(2000, tune=1000)\n```\n\n**Variational Inference vs MCMC :**\n- **VI** : Approximation rapide mais biais√©e\n- **MCMC** : √âchantillonnage exact mais co√ªteux\n- **Hybrid** : VI pour initialisation, MCMC pour raffinement\n\n**üé® Variantes Avanc√©es :**\n\n**Parallel Tempering :**\n```\nTemp√©ratures : T‚ÇÅ < T‚ÇÇ < ... < T‚Çñ\nDistributions : œÄ_i(x) ‚àù [œÄ(x)]^{1/T_i}\n\n√âchanges entre cha√Ænes :\nŒ± = min(1, exp[(1/T_i - 1/T_j)(U(x_j) - U(x_i))])\n```\n\n**Avantages :**\n- **Multimodalit√©** : Exploration de modes multiples\n- **Convergence** : Plus rapide vers stationnarit√©\n- **Robustesse** : Moins sensible √† l'initialisation\n\n**Adaptive MCMC :**\n- **Covariance Adaptation** : Ajustement automatique des propositions\n- **Step Size Tuning** : Optimisation du taux d'acceptation\n- **Dual Averaging** : Algorithmes d'adaptation robustes\n\n**Reversible Jump MCMC :**\n- **Model Selection** : Saut entre mod√®les de dimensions diff√©rentes\n- **Variable Selection** : Inclusion/exclusion de variables\n- **Complexity Control** : Balance biais-variance automatique\n\n**üìä Diagnostics et Convergence :**\n\n**Trace Plots :**\n- **Mixing** : Exploration efficace de l'espace\n- **Stationarity** : Stabilit√© de la distribution\n- **Autocorrelation** : Ind√©pendance des √©chantillons\n\n**Gelman-Rubin Statistic (RÃÇ) :**\n```\nRÃÇ = ‚àö[(n-1)/n + (1/n)(B/W)]\n\nB = Variance entre cha√Ænes\nW = Variance intra-cha√Ænes\n\nConvergence si RÃÇ < 1.1\n```\n\n**Effective Sample Size (ESS) :**\n```\nESS = N / (1 + 2Œ£œÅ‚Çñ)\n\nœÅ‚Çñ = Autocorr√©lation au lag k\nN = Nombre total d'√©chantillons\n```\n\n**üöÄ Applications Sectorielles :**\n\n**Finance Quantitative :**\n- **Risk Management** : Mod√®les de volatilit√© stochastique\n- **Portfolio Optimization** : Incertitude sur les param√®tres\n- **Credit Risk** : Mod√®les de d√©faut hi√©rarchiques\n- **Derivatives Pricing** : Mod√®les complexes multi-facteurs\n\n**Bioinformatique :**\n- **Phylog√©n√©tique** : Reconstruction d'arbres √©volutifs\n- **G√©nomique** : Association g√©notype-ph√©notype\n- **√âpid√©miologie** : Mod√®les de propagation\n- **Drug Discovery** : Mod√©lisation mol√©culaire\n\n**Sciences Sociales :**\n- **√âconom√©trie** : Mod√®les hi√©rarchiques\n- **Psychom√©trie** : Th√©orie de r√©ponse √† l'item\n- **D√©mographie** : Projections de population\n- **Marketing** : Mod√®les de choix discret\n\n**üîß Impl√©mentation Moderne :**\n\n**Stan (C++) :**\n```stan\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\n```\n\n**PyMC (Python) :**\n```python\nwith pm.Model() as model:\n    alpha = pm.Normal('alpha', 0, 10)\n    beta = pm.Normal('beta', 0, 10)\n    sigma = pm.HalfNormal('sigma', 5)\n    \n    mu = alpha + beta * x\n    y_obs = pm.Normal('y_obs', mu, sigma, observed=y)\n    \n    trace = pm.sample(2000, return_inferencedata=True)\n```\n\n**JAGS (R) :**\n```r\nlibrary(rjags)\n\nmodel_string <- \"\n  model {\n    for (i in 1:N) {\n      y[i] ~ dnorm(mu[i], tau)\n      mu[i] <- alpha + beta * x[i]\n    }\n    alpha ~ dnorm(0, 0.01)\n    beta ~ dnorm(0, 0.01)\n    tau ~ dgamma(0.01, 0.01)\n  }\"\n```\n\n**‚ö° Optimisations Modernes :**\n\n**GPU Acceleration :**\n- **CuPy/JAX** : Calculs parall√®les massifs\n- **TensorFlow Probability** : Int√©gration deep learning\n- **Numpyro** : MCMC sur GPU avec JAX\n\n**Automatic Differentiation :**\n- **Gradients Exacts** : Plus de d√©riv√©es num√©riques\n- **HMC Efficace** : Exploration optimale\n- **NUTS** : No-U-Turn Sampler automatique\n\n**üö® D√©fis et Solutions :**\n\n**Haute Dimension :**\n- **Curse of Dimensionality** : Exploration inefficace\n- **Solution** : HMC, Riemannian MCMC\n- **Preconditioning** : Transformation d'espace\n\n**Multimodalit√© :**\n- **Mode Switching** : Difficult√© √† changer de mode\n- **Solution** : Parallel Tempering, Annealed Importance\n- **Initialization** : D√©marrage multiple\n\n**Computational Cost :**\n- **Likelihood Evaluation** : Co√ªt par it√©ration\n- **Solution** : Approximate Bayesian Computation\n- **Subsampling** : Mini-batch MCMC\n\n**üåü R√©volution et Impact :**\nMCMC a r√©volutionn√© la statistique bay√©sienne en rendant praticable l'inf√©rence sur des mod√®les complexes impossibles √† r√©soudre analytiquement. Avec l'essor du machine learning, MCMC devient essentiel pour quantifier l'incertitude, permettant une IA plus robuste et interpr√©table. L'int√©gration avec l'automatic differentiation et le calcul GPU ouvre de nouvelles fronti√®res pour l'inf√©rence bay√©sienne √† grande √©chelle.",
    category: "statistiques",
    icon: "GitBranch"
  },
  {
    term: "Mod√®les de Markov cach√©s (Hidden Markov Models - HMM)",
    description: "Mod√®le statistique dans lequel le syst√®me mod√©lis√© est suppos√© √™tre un processus de Markov avec des √©tats non observ√©s (cach√©s). Utilis√© en reconnaissance vocale, bioinformatique et finance.",
    category: "statistiques",
    icon: "Eye"
  },
  {
    term: "Analyse de survie (Survival Analysis)",
    description: "**La science du temps qui reste !** Comme un m√©decin qui pr√©dit l'esp√©rance de vie d'un patient ou un ing√©nieur qui estime la dur√©e de vie d'une machine, l'analyse de survie mod√©lise le temps jusqu'√† ce qu'un √©v√©nement critique se produise.\n\n**‚è∞ Analogie M√©dicale :**\nImaginez suivre 1000 patients atteints d'une maladie : certains gu√©rissent rapidement, d'autres vivent des ann√©es, quelques-uns quittent l'√©tude. L'analyse de survie extrait des insights m√™me avec ces donn√©es 'incompl√®tes'.\n\n**üéØ Concepts Fondamentaux :**\n\n**Fonction de Survie S(t) :**\n- Probabilit√© de survivre au-del√† du temps t\n- S(t) = P(T > t) o√π T = temps de survie\n- D√©croissante de 1 (t=0) vers 0 (t=‚àû)\n\n**Fonction de Risque h(t) :**\n- Taux instantan√© de d√©faillance au temps t\n- h(t) = lim[P(t ‚â§ T < t+Œît | T ‚â• t)] / Œît\n- Peut augmenter, diminuer, ou rester constant\n\n**üö® D√©fi de la Censure :**\n\n**Types de Censure :**\n‚Ä¢ **Droite** : √âv√©nement non observ√© √† la fin de l'√©tude\n‚Ä¢ **Gauche** : √âv√©nement d√©j√† survenu au d√©but\n‚Ä¢ **Intervalle** : √âv√©nement dans une p√©riode connue\n‚Ä¢ **Informative** : Censure li√©e au risque d'√©v√©nement\n\n**Impact Critique :**\n- Ignorer la censure ‚Üí Biais majeurs\n- Sous-estimation des temps de survie\n- Conclusions erron√©es sur l'efficacit√©\n\n**üìä M√©thodes Classiques :**\n\n**Estimateur de Kaplan-Meier :**\n- Estimation non-param√©trique de S(t)\n- Courbes de survie en escalier\n- Intervalles de confiance\n- Test du log-rank pour comparaisons\n\n**Mod√®le de Cox (Proportional Hazards) :**\n- h(t|x) = h‚ÇÄ(t) √ó exp(Œ≤x)\n- Semi-param√©trique (pas d'hypoth√®se sur h‚ÇÄ)\n- Hazard Ratios pour interpr√©ter les effets\n- Standard en recherche m√©dicale\n\n**Mod√®les Param√©triques :**\n- **Weibull** : Risque monotone (croissant/d√©croissant)\n- **Exponentiel** : Risque constant\n- **Log-normal** : Risque en cloche\n- **Gamma g√©n√©ralis√©** : Tr√®s flexible\n\n**üéØ Applications Diversifi√©es :**\n\n**M√©decine & Sant√© :**\n- Essais cliniques (survie patients)\n- √âpid√©miologie (progression maladie)\n- Pharmacovigilance (effets secondaires)\n\n**Business & Marketing :**\n- **Customer Churn** : Temps avant d√©sabonnement\n- **CLV** : Customer Lifetime Value\n- **R√©tention** : Dur√©e d'engagement client\n\n**Ing√©nierie & Fiabilit√© :**\n- Dur√©e de vie des composants\n- Maintenance pr√©dictive\n- Analyse des pannes syst√®me\n\n**Finance :**\n- D√©faut de cr√©dit\n- Dur√©e des investissements\n- Risque de march√©\n\n**üõ†Ô∏è Outils Modernes :**\n\n**Packages R :**\n- `survival` : Fonctions de base\n- `survminer` : Visualisations √©l√©gantes\n- `flexsurv` : Mod√®les flexibles\n\n**Python :**\n- `lifelines` : Complet et intuitif\n- `scikit-survival` : Int√©gration sklearn\n- `pycox` : Deep learning pour survie\n\n**üìà Extensions Avanc√©es :**\n\n**Mod√®les Multi-√©tats :**\n- Transitions entre √©tats multiples\n- Maladie ‚Üí R√©mission ‚Üí Rechute ‚Üí D√©c√®s\n\n**Survie Concurrente :**\n- Risques comp√©titifs multiples\n- D√©c√®s par cancer vs autres causes\n\n**Machine Learning :**\n- **Random Survival Forest** : Ensembles d'arbres\n- **DeepSurv** : R√©seaux de neurones\n- **DeepHit** : Risques concurrents\n\n**‚ö° M√©triques d'√âvaluation :**\n- **C-index** : Concordance (√©quivalent AUC)\n- **Brier Score** : Erreur de pr√©diction temporelle\n- **IBS** : Integrated Brier Score\n- **Time-dependent AUC** : Performance temporelle\n\n**üí° Insights Strat√©giques :**\nNetflix utilise l'analyse de survie pour pr√©dire le churn avec 85% de pr√©cision, permettant des interventions cibl√©es qui r√©duisent l'attrition de 23%. En m√©decine, elle guide 90% des d√©cisions th√©rapeutiques en oncologie.",
    category: "statistiques",
    icon: "Clock"
  }
];