/**
 * Statistical Methods and Concepts
 * Core statistical concepts used in data science
 */

import { GlossaryEntry } from './types';

export const statisticsTerms: GlossaryEntry[] = [
  {
    term: "Statistiques",
    description: "Les Statistiques constituent la science fondamentale qui transforme les données brutes en connaissances exploitables. Imaginez un traducteur universel : les statistiques traduisent le **langage** des données en insights compréhensibles pour la prise de décision. Cette discipline englobe quatre piliers essentiels : la **collecte** (comment obtenir des données représentatives), l'**analyse** (application de méthodes mathématiques), l'**interprétation** (donner du sens aux résultats), et la **présentation** (communiquer efficacement les findings). Les statistiques nous permettent de naviguer dans l'incertitude, de distinguer les signaux du bruit, et de faire des prédictions fiables. Elles constituent le socle mathématique de la data science, fournissant les outils pour tester des hypothèses, quantifier la confiance dans nos conclusions, et généraliser des observations d'échantillons à des populations entières. Des sondages d'opinion aux essais cliniques, des analyses de marché aux prévisions météorologiques, les statistiques sont omniprésentes dans notre société moderne.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Population vs Échantillon",
    description: "Cette distinction fondamentale est comme la différence entre photographier une foule entière versus prendre un instantané représentatif. La **Population** représente l'ensemble complet et exhaustif de tous les éléments, individus, ou observations qui nous intéressent dans notre étude (par exemple, tous les citoyens français, toutes les entreprises du CAC 40, ou tous les patients atteints d'une maladie spécifique). L'**Échantillon** est un sous-ensemble soigneusement sélectionné de cette population, choisi pour être représentatif et permettre des inférences valides. La qualité de l'échantillonnage est cruciale : un échantillon biaisé peut conduire à des conclusions erronées. Les méthodes d'échantillonnage incluent l'échantillonnage aléatoire simple, stratifié, ou par grappes. La **taille d'échantillon** influence directement la précision des estimations : plus l'échantillon est grand, plus nos conclusions sont fiables, mais les coûts augmentent. Cette distinction est essentielle car étudier une population entière est souvent impossible (coût, temps, accessibilité), d'où l'importance de maîtriser les techniques d'échantillonnage pour généraliser nos résultats.",
    category: "statistiques",
    icon: "Users"
  },
  {
    term: "Moyenne (Mean)",
    description: "La moyenne arithmétique est comme le 'centre de gravité' de vos données. Calculée en additionnant toutes les valeurs et en divisant par le nombre d'observations (Σx/n), elle représente la valeur typique autour de laquelle les données gravitent. **Avantages** : facile à calculer, utilise toutes les données, base de nombreux tests statistiques. **Inconvénients** : très sensible aux valeurs aberrantes (outliers). Par exemple, si 9 personnes gagnent 30k€ et une 10ème gagne 300k€, la moyenne (57k€) ne représente pas bien le groupe. **Applications** : calcul de performances moyennes, analyses financières, contrôle qualité. **Variantes** : moyenne pondérée (certaines valeurs comptent plus), moyenne géométrique (pour les taux de croissance), moyenne harmonique (pour les vitesses). La moyenne est la mesure de tendance centrale la plus utilisée en statistiques inférentielles et constitue la base de concepts avancés comme la variance et l'écart-type.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Médiane (Median)",
    description: "La médiane est la 'valeur du milieu' qui divise vos données en deux moitiés égales, comme un médiateur qui sépare équitablement deux groupes. Pour la calculer : triez les données par ordre croissant, puis prenez la valeur centrale (si n impair) ou la moyenne des deux valeurs centrales (si n pair). **Avantage majeur** : robuste aux valeurs aberrantes - elle résiste aux extrêmes. Dans l'exemple précédent (9 personnes à 30k€, 1 à 300k€), la médiane reste 30k€, plus représentative. **Applications pratiques** : salaires (médiane plus représentative que moyenne), prix immobiliers, scores de satisfaction. **Interprétation** : 50% des observations sont inférieures à la médiane, 50% supérieures. **Comparaison avec la moyenne** : si médiane < moyenne, distribution asymétrique vers la droite (queue positive) ; si médiane > moyenne, asymétrie vers la gauche. La médiane est essentielle en statistiques descriptives et particulièrement utile pour les données économiques et sociales.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Mode",
    description: "Le mode est la 'star' de vos données - la valeur qui apparaît le plus fréquemment, comme la chanson la plus jouée sur une playlist. **Identification** : comptez la fréquence de chaque valeur, le mode est celle avec le maximum d'occurrences. **Types de distributions** : unimodale (un seul mode), bimodale (deux modes), multimodale (plusieurs modes), ou amodale (pas de mode clair). **Particularités** : seule mesure de tendance centrale applicable aux données qualitatives (couleur préférée, marque favorite). **Applications** : études de marché (produit le plus vendu), contrôle qualité (défaut le plus fréquent), analyses démographiques (âge modal). **Avantages** : facile à identifier visuellement, résistant aux valeurs aberrantes, applicable à tous types de données. **Limites** : peut ne pas exister ou être multiple, moins utilisé en statistiques inférentielles. **Exemple pratique** : dans une enquête sur les tailles de chaussures, si la taille 42 revient 15 fois (plus que toute autre), c'est le mode. Le mode complète utilement la moyenne et la médiane pour une description complète de la distribution.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Variance",
    description: "La variance mesure à quel point vos données sont 'dispersées' autour de la moyenne, comme mesurer l'étalement d'un groupe de personnes autour d'un point de rassemblement. **Calcul** : moyenne des carrés des écarts à la moyenne : Var(X) = Σ(xi - μ)²/n (population) ou Σ(xi - x̄)²/(n-1) (échantillon). **Pourquoi élever au carré ?** Cela évite que les écarts positifs et négatifs s'annulent, et donne plus de poids aux grandes déviations. **Interprétation** : variance faible = données concentrées près de la moyenne (groupe homogène) ; variance élevée = données très dispersées (groupe hétérogène). **Problème pratique** : l'unité est le carré de l'unité originale (si les données sont en euros, la variance est en euros²), ce qui complique l'interprétation. **Applications** : finance (mesure du risque d'un investissement), contrôle qualité (consistance d'un processus), recherche (variabilité entre sujets). La variance est fondamentale en statistiques car elle quantifie l'incertitude et sert de base à de nombreux tests statistiques et modèles prédictifs.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Écart-type (Standard Deviation)",
    description: "L'écart-type est la 'version lisible' de la variance - sa racine carrée qui remet les unités dans leur forme originale. Si la variance est comme mesurer une surface, l'écart-type est comme mesurer une distance. **Calcul** : σ = √Variance. **Avantage majeur** : même unité que les données originales, donc directement interprétable. **Règle empirique (loi normale)** : environ 68% des données se trouvent à ±1σ de la moyenne, 95% à ±2σ, 99.7% à ±3σ. **Applications pratiques** : en finance, un écart-type de 15% sur les rendements d'une action indique sa volatilité ; en production, un écart-type faible indique un processus stable. **Comparaisons** : permet de comparer la variabilité entre différents datasets ou variables. **Écart-type vs étendue** : l'écart-type utilise toutes les données (plus robuste) tandis que l'étendue ne considère que min et max (sensible aux outliers). **Standardisation** : l'écart-type permet de créer des scores Z pour comparer des valeurs de distributions différentes. C'est l'une des mesures les plus importantes en statistiques descriptives et inférentielles.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Distribution normale (Gaussian)",
    description: "La distribution normale est la 'reine' des distributions statistiques, ressemblant à une cloche parfaitement symétrique où la plupart des valeurs se concentrent au centre et diminuent graduellement vers les extrêmes. **Analogie** : la répartition des tailles dans une population - peu de personnes très petites ou très grandes, la majorité autour de la moyenne. **Paramètres** : entièrement définie par sa moyenne μ (centre) et son écart-type σ (largeur). **Propriétés remarquables** : symétrie parfaite, moyenne = médiane = mode, aires sous la courbe définies (68-95-99.7 rule). **Ubiquité naturelle** : erreurs de mesure, caractéristiques biologiques, phénomènes sociaux - le **Théorème Central Limite** explique pourquoi tant de phénomènes suivent cette loi. **Applications** : tests statistiques (t-test, ANOVA), intervalles de confiance, contrôle qualité (Six Sigma), finance (modèles de risque). **Standardisation** : toute normale peut être transformée en normale standard (μ=0, σ=1) via Z = (X-μ)/σ. **Importance historique** : découverte par Gauss et Laplace, fondement de la statistique moderne. **Reconnaissance** : si vos données forment une courbe en cloche, vous pouvez appliquer de puissants outils statistiques paramétriques.",
    category: "statistiques",
    icon: "TrendingUp"
  },
  {
    term: "Probabilité",
    description: "La probabilité quantifie l'incertitude et mesure nos 'chances' qu'un événement se réalise, comme un baromètre de la vraisemblance qui oscille entre l'impossible (0) et le certain (1). **Analogie** : prédire la météo - 0% = soleil garanti, 100% = pluie certaine, 70% = probablement pluvieux. **Échelle** : toujours entre 0 et 1 (ou 0% et 100%), où 0.5 = équiprobable (pile ou face). **Interprétations** : 1) **Fréquentiste** (répétition infinie d'expériences), 2) **Subjective** (degré de croyance personnel), 3) **Classique** (cas favorables/cas possibles). **Règles fondamentales** : P(A) + P(non-A) = 1, P(A ou B) = P(A) + P(B) - P(A et B), P(A et B) = P(A) × P(B|A). **Applications** : jeux de hasard, assurance (calcul des primes), médecine (diagnostic), finance (gestion des risques), machine learning (classification probabiliste). **Distributions** : uniforme (dé équilibré), binomiale (succès/échec), normale (phénomènes naturels). **Théorème de Bayes** : mise à jour des probabilités avec nouvelles informations. **Impact** : fondement de la statistique inférentielle, de l'IA probabiliste, et de la prise de décision sous incertitude.",
    category: "statistiques",
    icon: "Percent"
  },
  {
    term: "Test d'hypothèse",
    description: "Le test d'hypothèse est comme un **procès judiciaire** pour vos données - une procédure rigoureuse qui détermine si une affirmation sur une population est crédible ou doit être rejetée. **Processus** : 1) Formuler l'**hypothèse nulle H₀** (status quo, 'pas d'effet') et l'**hypothèse alternative H₁** (ce qu'on veut prouver), 2) Choisir un **seuil de signification α** (généralement 5%), 3) Calculer une **statistique de test** à partir des données, 4) Déterminer la **p-value**, 5) **Décision** : rejeter H₀ si p < α. **Analogie juridique** : H₀ = 'innocent jusqu'à preuve du contraire', les données sont les preuves, α est le niveau de preuve requis, la p-value mesure la force des preuves contre l'innocence. **Types courants** : test t (comparaison de moyennes), test du χ² (indépendance), ANOVA (comparaison multiple). **Erreurs possibles** : Type I (faux positif - condamner un innocent), Type II (faux négatif - acquitter un coupable). **Applications** : essais cliniques (efficacité d'un médicament), A/B testing (performance de versions), contrôle qualité (conformité aux standards). **Puissance** : probabilité de détecter un effet réel. Les tests d'hypothèse sont le pilier de la recherche scientifique et de la prise de décision basée sur les données.",
    category: "statistiques",
    icon: "CheckCircle"
  },
  {
    term: "P-value",
    description: "La p-value est le **'niveau de surprise'** de vos données - elle mesure à quel point vos résultats observés seraient improbables si l'hypothèse nulle était vraie. **Analogie** : imaginez que vous soupçonnez qu'une pièce est truquée. Vous la lancez 100 fois et obtenez 70 faces. La p-value répond à : 'Si la pièce était équitable, quelle est la probabilité d'obtenir 70 faces ou plus par pur hasard ?' **Interprétation** : p-value faible (< 0.05) = résultats très surprenants sous H₀, donc on rejette H₀ ; p-value élevée = résultats pas surprenants, on ne rejette pas H₀. **Malentendus courants** : la p-value N'EST PAS la probabilité que H₀ soit vraie, ni la probabilité de se tromper. **Calcul** : aire sous la courbe de distribution de la statistique de test, au-delà de la valeur observée. **Seuils conventionnels** : p < 0.05 (significatif), p < 0.01 (très significatif), p < 0.001 (hautement significatif). **Critiques** : problème des comparaisons multiples, p-hacking (manipulation des analyses pour obtenir p < 0.05), sur-interprétation des seuils arbitraires. **Alternatives** : intervalles de confiance, approche bayésienne, taille d'effet. La p-value reste un outil central mais doit être interprétée avec prudence et contexte.",
    category: "statistiques",
    icon: "Percent"
  },
  {
    term: "Intervalle de confiance",
    description: "L'intervalle de confiance est comme un **\'filet de sécurité statistique\'** - une plage de valeurs qui a de bonnes chances de capturer le vrai paramètre de population que nous cherchons à estimer. **Analogie** : imaginez que vous essayez d\'attraper un poisson (le vrai paramètre) avec un filet (l\'intervalle). Un filet plus large (niveau de confiance plus élevé) a plus de chances d\'attraper le poisson, mais est moins précis. **Construction** : Estimation ± Marge d\'erreur, où la marge d\'erreur dépend du niveau de confiance souhaité et de la variabilité des données. **Interprétation correcte** : \'Si nous répétions cette étude 100 fois, environ 95 intervalles sur 100 contiendraient le vrai paramètre\' (pour un IC à 95%). **Malentendu fréquent** : ce n\'est PAS \'il y a 95% de chances que le vrai paramètre soit dans cet intervalle\' - le paramètre est fixe, c\'est l\'intervalle qui varie d\'échantillon en échantillon. **Niveaux courants** : 90% (±1.645σ), 95% (±1.96σ), 99% (±2.576σ). **Applications** : sondages politiques (\'candidat X : 52% ±3%\'), essais cliniques (efficacité d\'un traitement), contrôle qualité (limites de tolérance). **Largeur** : dépend de la taille d\'échantillon (plus grand échantillon = intervalle plus étroit) et de la variabilité des données. Les intervalles de confiance fournissent plus d\'information que les tests d\'hypothèse car ils quantifient l\'incertitude.",
    category: "statistiques",
    icon: "Target"
  },
  {
    term: "Corrélation",
    description: "La corrélation mesure à quel point deux variables 'dansent ensemble' - si elles bougent dans la même direction, en opposition, ou de manière indépendante. **Analogie** : observer deux danseurs - parfaitement synchronisés (corrélation +1), en opposition parfaite (-1), ou dansant indépendamment (0). **Coefficient de Pearson** : mesure standard entre -1 et +1, où |r| proche de 1 indique une relation linéaire forte. **Interprétation** : r > 0 (relation positive - quand X augmente, Y augmente), r < 0 (relation négative - quand X augmente, Y diminue), r ≈ 0 (pas de relation linéaire). **Règles empiriques** : |r| < 0.3 (faible), 0.3-0.7 (modérée), > 0.7 (forte). **Types** : Pearson (linéaire), Spearman (monotone), Kendall (rang). **Applications** : finance (diversification de portefeuille), marketing (prix vs demande), santé (facteurs de risque), météo (température vs pression). **Pièges** : corrélation ≠ causalité ! Deux variables peuvent être corrélées par hasard ou via une troisième variable cachée. **Visualisation** : nuage de points (scatter plot) révèle la nature de la relation. **Importance** : base de la régression, analyse factorielle, et détection de multicolinéarité.",
    category: "statistiques",
    icon: "GitBranch"
  },
  {
    term: "Régression",
    description: "La régression est comme **'dessiner la meilleure ligne'** à travers un nuage de points pour capturer la relation entre variables et faire des prédictions. **Analogie** : imaginez que vous essayez de prédire le prix d'une maison (Y) basé sur sa superficie (X). La régression trouve la ligne qui passe 'au plus près' de tous les points (maisons) pour minimiser les erreurs de prédiction. **Types principaux** : **Linéaire simple** (Y = a + bX, une seule variable explicative), **Multiple** (plusieurs variables : prix = f(superficie, chambres, quartier)), **Polynomiale** (relations courbes), **Logistique** (pour variables binaires). **Méthode des moindres carrés** : trouve la ligne qui minimise la somme des carrés des résidus (erreurs de prédiction). **Hypothèses clés** : linéarité, indépendance des erreurs, homoscédasticité (variance constante), normalité des résidus. **Évaluation** : R² (pourcentage de variance expliquée), RMSE (erreur moyenne), analyse des résidus. **Applications** : finance (modèles de pricing), marketing (impact publicitaire), économie (élasticité prix-demande), sciences (relations dose-effet). **Interprétation** : les coefficients indiquent l'impact d'une unité d'augmentation de X sur Y. **Extensions** : régression ridge/lasso (régularisation), régression robuste (résistante aux outliers). La régression est l'un des outils les plus utilisés en data science pour comprendre et prédire.",
    category: "statistiques",
    icon: "TrendingUp"
  },
  {
    term: "Statistiques bayésiennes (Bayesian Statistics)",
    description: "Les statistiques bayésiennes fonctionnent comme un **détective qui met à jour ses hypothèses** à chaque nouvel indice découvert - elles permettent d'incorporer systématiquement de nouvelles preuves pour affiner nos croyances. **Philosophie révolutionnaire** : contrairement à l'approche fréquentiste (probabilité = fréquence à long terme), l'approche bayésienne traite la probabilité comme un **degré de croyance** qui évolue avec l'information. **Théorème de Bayes** : P(H|E) = P(E|H) × P(H) / P(E), où P(H|E) est la probabilité a posteriori (croyance mise à jour), P(H) la probabilité a priori (croyance initiale), P(E|H) la vraisemblance (compatibilité des données avec l'hypothèse). **Analogie médicale** : un médecin commence avec une probabilité a priori qu'un patient ait une maladie (basée sur l'âge, antécédents), puis met à jour cette probabilité après chaque test (symptômes, analyses). **Processus itératif** : Prior → Données → Posterior, où le posterior d'aujourd'hui devient le prior de demain. **Avantages** : incorporation naturelle de connaissances préalables, quantification complète de l'incertitude, prédictions probabilistes, gestion élégante de petits échantillons. **Applications** : diagnostic médical, spam filtering, recommandations personnalisées, A/B testing, finance (gestion de risque). **Outils** : MCMC (échantillonnage), Stan/PyMC (logiciels), réseaux bayésiens. **Défis** : choix du prior (subjectivité), complexité computationnelle, courbe d'apprentissage. **Renaissance moderne** : avec la puissance de calcul actuelle, les méthodes bayésiennes connaissent un essor majeur en IA et data science.",
    category: "statistiques",
    icon: "RefreshCw"
  },
  {
    term: "Quantiles/Percentiles/Quartiles",
    description: "Les quantiles fonctionnent comme des **'lignes de démarcation'** qui divisent vos données en tranches égales, à la manière d'un couteau qui découpe un gâteau en parts de taille identique. **Principe** : au lieu de regarder les valeurs absolues, on s'intéresse aux positions relatives dans la distribution. **Percentiles** : divisent les données en 100 parts égales - le 75ème percentile signifie que 75% des observations sont inférieures à cette valeur. **Analogie scolaire** : si vous êtes au 90ème percentile d'un examen, vous avez fait mieux que 90% des étudiants. **Quartiles** : cas spécial qui divise en 4 parts égales : Q1 (25ème percentile), Q2 (médiane, 50ème percentile), Q3 (75ème percentile). **Calcul pratique** : triez les données, puis trouvez les valeurs aux positions k×(n+1)/100 pour le kème percentile. **Applications cruciales** : 1) **Boxplots** (visualisation des quartiles et outliers), 2) **Benchmarking** (performance relative), 3) **Détection d'anomalies** (valeurs au-delà de Q3 + 1.5×IQR), 4) **Segmentation** (diviser clients en groupes). **Espace interquartile (IQR)** : Q3 - Q1, mesure robuste de dispersion résistante aux outliers. **Avantages** : interprétation intuitive, robustesse aux valeurs extrêmes, applicable à toute distribution. **Exemples concrets** : salaires (médiane plus représentative), temps de réponse web (95ème percentile pour SLA), croissance d'enfants (courbes de percentiles). **Différence clé** : contrairement à la moyenne/écart-type, les quantiles ne font aucune hypothèse sur la forme de la distribution.",
    category: "statistiques",
    icon: "BarChart3"
  },
  {
    term: "Erreurs de type I et de type II (Type I & II Errors)",
    description: "Les erreurs de Type I et II sont comme les **'erreurs judiciaires'** des statistiques - elles représentent les deux façons dont nous pouvons nous tromper lors d'un test d'hypothèse. **Analogie juridique** : imaginez un procès où l'accusé est soit innocent (H₀ vraie) soit coupable (H₀ fausse). **Erreur de Type I (α)** : condamner un innocent - rejeter H₀ alors qu'elle est vraie (faux positif). C'est comme déclarer qu'un médicament est efficace alors qu'il ne l'est pas. **Probabilité** : α = P(rejeter H₀ | H₀ vraie), généralement fixée à 5%. **Erreur de Type II (β)** : acquitter un coupable - accepter H₀ alors qu'elle est fausse (faux négatif). C'est comme ne pas détecter l'efficacité d'un médicament qui fonctionne réellement. **Probabilité** : β = P(accepter H₀ | H₀ fausse). **Puissance statistique** : 1-β, probabilité de détecter un effet réel. **Trade-off fondamental** : réduire α augmente β et vice-versa - on ne peut pas minimiser les deux simultanément sans augmenter la taille d'échantillon. **Applications critiques** : médecine (diagnostic), contrôle qualité (défauts), sécurité (détection de menaces). **Conséquences** : Type I peut conduire à des décisions coûteuses basées sur de fausses preuves ; Type II peut faire rater des opportunités importantes. **Facteurs d'influence** : taille d'échantillon (plus grand = moins d'erreurs), taille d'effet (effet plus grand = moins d'erreur Type II), variabilité des données. **Stratégies de mitigation** : calcul de puissance a priori, tests adaptatifs, approches bayésiennes. **Contexte moderne** : avec le Big Data, l'erreur Type I devient critique (problème des comparaisons multiples), nécessitant des corrections comme Bonferroni ou FDR (False Discovery Rate).",
    category: "statistiques",
    icon: "AlertTriangle"
  },
  {
    term: "Chaînes de Markov Monte Carlo (MCMC)",
    description: "**🎲 L'Art de l'Exploration Probabiliste !**\n\nComme un explorateur méthodique qui découvre un territoire inconnu en suivant des règles précises, MCMC révolutionne l'échantillonnage de distributions complexes en créant une chaîne d'états où chaque étape dépend uniquement de la précédente, permettant d'explorer efficacement des espaces probabilistes de haute dimension.\n\n**🗺️ Analogie de l'Explorateur :**\nImaginez un explorateur dans une région montagneuse brumeuse. Il ne peut voir que sa position actuelle et les environs immédiats. Pour cartographier la région, il suit une règle simple : à chaque étape, il propose un nouveau lieu à visiter basé sur sa position actuelle, et décide d'y aller selon certains critères. Après des milliers d'étapes, son parcours révèle la topographie complète !\n\n**⚙️ Fondements Théoriques :**\n\n**Propriété de Markov :**\n```\nP(X_{t+1} | X_t, X_{t-1}, ..., X_0) = P(X_{t+1} | X_t)\n```\n*L'avenir ne dépend que du présent, pas du passé*\n\n**Chaîne de Markov :**\n- **États** : Valeurs possibles des paramètres\n- **Transitions** : Probabilités de passage entre états\n- **Stationnarité** : Distribution limite invariante\n- **Ergodicité** : Convergence vers la distribution cible\n\n**Théorème Fondamental :**\nSi la chaîne est irréductible et apériodique, alors :\n```\nlim_{n→∞} (1/n) Σ f(X_i) = E_π[f(X)]\n```\n*La moyenne empirique converge vers l'espérance théorique*\n\n**🎯 Algorithmes Principaux :**\n\n**Metropolis-Hastings :**\n```\nAlgorithme Metropolis-Hastings:\n1. État actuel : x_t\n2. Proposer : x' ~ q(x'|x_t)\n3. Calculer ratio : α = min(1, [π(x')q(x_t|x')] / [π(x_t)q(x'|x_t)])\n4. Accepter x' avec probabilité α\n5. Sinon garder x_t\n```\n\n**Avantages :**\n- **Universalité** : Fonctionne pour toute distribution\n- **Simplicité** : Facile à implémenter\n- **Flexibilité** : Nombreuses variantes possibles\n\n**Gibbs Sampling :**\n```\nAlgorithme de Gibbs:\nPour chaque variable X_i:\n  X_i^{(t+1)} ~ P(X_i | X_{-i}^{(t+1)}, X_{-i}^{(t)})\n```\n\n**Conditions d'Application :**\n- **Conditionnelles Connues** : Distributions conditionnelles calculables\n- **Sampling Direct** : Échantillonnage direct possible\n- **Efficacité** : Convergence souvent plus rapide\n\n**Hamiltonian Monte Carlo (HMC) :**\n```\nDynamique Hamiltonienne:\ndq/dt = ∂H/∂p\ndp/dt = -∂H/∂q\n\nH(q,p) = U(q) + K(p)\nU(q) = -log π(q)  # Énergie potentielle\nK(p) = p²/2m      # Énergie cinétique\n```\n\n**Révolution HMC :**\n- **Gradient Information** : Utilise les gradients de la log-densité\n- **Exploration Efficace** : Évite la marche aléatoire\n- **Haute Dimension** : Excellent pour espaces complexes\n- **Stan/PyMC** : Implémentations modernes\n\n**🔬 Applications en Machine Learning :**\n\n**Inférence Bayésienne :**\n```\nModèle Bayésien:\nP(θ|D) ∝ P(D|θ) × P(θ)\n\nÉchantillonnage MCMC:\nθ^{(1)}, θ^{(2)}, ..., θ^{(N)} ~ P(θ|D)\n\nEstimation:\nE[θ|D] ≈ (1/N) Σ θ^{(i)}\n```\n\n**Réseaux de Neurones Bayésiens :**\n- **Incertitude** : Distribution sur les poids\n- **Regularization** : Priors sur les paramètres\n- **Calibration** : Prédictions avec intervalles de confiance\n- **Robustesse** : Résistance à l'overfitting\n\n**Modèles Graphiques :**\n- **Variables Latentes** : Échantillonnage des états cachés\n- **Topic Models** : LDA, allocation de sujets\n- **Collaborative Filtering** : Factorisation matricielle\n- **Social Networks** : Modèles de communautés\n\n**🧠 Deep Learning et MCMC :**\n\n**Bayesian Neural Networks :**\n```python\n# PyMC3 Example\nwith pm.Model() as model:\n    # Priors sur les poids\n    w1 = pm.Normal('w1', 0, 1, shape=(input_dim, hidden_dim))\n    w2 = pm.Normal('w2', 0, 1, shape=(hidden_dim, output_dim))\n    \n    # Forward pass\n    hidden = pm.math.tanh(pm.math.dot(X, w1))\n    output = pm.math.dot(hidden, w2)\n    \n    # Likelihood\n    y_obs = pm.Normal('y_obs', output, sigma, observed=y)\n    \n    # MCMC Sampling\n    trace = pm.sample(2000, tune=1000)\n```\n\n**Variational Inference vs MCMC :**\n- **VI** : Approximation rapide mais biaisée\n- **MCMC** : Échantillonnage exact mais coûteux\n- **Hybrid** : VI pour initialisation, MCMC pour raffinement\n\n**🎨 Variantes Avancées :**\n\n**Parallel Tempering :**\n```\nTempératures : T₁ < T₂ < ... < Tₖ\nDistributions : π_i(x) ∝ [π(x)]^{1/T_i}\n\nÉchanges entre chaînes :\nα = min(1, exp[(1/T_i - 1/T_j)(U(x_j) - U(x_i))])\n```\n\n**Avantages :**\n- **Multimodalité** : Exploration de modes multiples\n- **Convergence** : Plus rapide vers stationnarité\n- **Robustesse** : Moins sensible à l'initialisation\n\n**Adaptive MCMC :**\n- **Covariance Adaptation** : Ajustement automatique des propositions\n- **Step Size Tuning** : Optimisation du taux d'acceptation\n- **Dual Averaging** : Algorithmes d'adaptation robustes\n\n**Reversible Jump MCMC :**\n- **Model Selection** : Saut entre modèles de dimensions différentes\n- **Variable Selection** : Inclusion/exclusion de variables\n- **Complexity Control** : Balance biais-variance automatique\n\n**📊 Diagnostics et Convergence :**\n\n**Trace Plots :**\n- **Mixing** : Exploration efficace de l'espace\n- **Stationarity** : Stabilité de la distribution\n- **Autocorrelation** : Indépendance des échantillons\n\n**Gelman-Rubin Statistic (R̂) :**\n```\nR̂ = √[(n-1)/n + (1/n)(B/W)]\n\nB = Variance entre chaînes\nW = Variance intra-chaînes\n\nConvergence si R̂ < 1.1\n```\n\n**Effective Sample Size (ESS) :**\n```\nESS = N / (1 + 2Σρₖ)\n\nρₖ = Autocorrélation au lag k\nN = Nombre total d'échantillons\n```\n\n**🚀 Applications Sectorielles :**\n\n**Finance Quantitative :**\n- **Risk Management** : Modèles de volatilité stochastique\n- **Portfolio Optimization** : Incertitude sur les paramètres\n- **Credit Risk** : Modèles de défaut hiérarchiques\n- **Derivatives Pricing** : Modèles complexes multi-facteurs\n\n**Bioinformatique :**\n- **Phylogénétique** : Reconstruction d'arbres évolutifs\n- **Génomique** : Association génotype-phénotype\n- **Épidémiologie** : Modèles de propagation\n- **Drug Discovery** : Modélisation moléculaire\n\n**Sciences Sociales :**\n- **Économétrie** : Modèles hiérarchiques\n- **Psychométrie** : Théorie de réponse à l'item\n- **Démographie** : Projections de population\n- **Marketing** : Modèles de choix discret\n\n**🔧 Implémentation Moderne :**\n\n**Stan (C++) :**\n```stan\ndata {\n  int<lower=0> N;\n  vector[N] x;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  real beta;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(alpha + beta * x, sigma);\n}\n```\n\n**PyMC (Python) :**\n```python\nwith pm.Model() as model:\n    alpha = pm.Normal('alpha', 0, 10)\n    beta = pm.Normal('beta', 0, 10)\n    sigma = pm.HalfNormal('sigma', 5)\n    \n    mu = alpha + beta * x\n    y_obs = pm.Normal('y_obs', mu, sigma, observed=y)\n    \n    trace = pm.sample(2000, return_inferencedata=True)\n```\n\n**JAGS (R) :**\n```r\nlibrary(rjags)\n\nmodel_string <- \"\n  model {\n    for (i in 1:N) {\n      y[i] ~ dnorm(mu[i], tau)\n      mu[i] <- alpha + beta * x[i]\n    }\n    alpha ~ dnorm(0, 0.01)\n    beta ~ dnorm(0, 0.01)\n    tau ~ dgamma(0.01, 0.01)\n  }\"\n```\n\n**⚡ Optimisations Modernes :**\n\n**GPU Acceleration :**\n- **CuPy/JAX** : Calculs parallèles massifs\n- **TensorFlow Probability** : Intégration deep learning\n- **Numpyro** : MCMC sur GPU avec JAX\n\n**Automatic Differentiation :**\n- **Gradients Exacts** : Plus de dérivées numériques\n- **HMC Efficace** : Exploration optimale\n- **NUTS** : No-U-Turn Sampler automatique\n\n**🚨 Défis et Solutions :**\n\n**Haute Dimension :**\n- **Curse of Dimensionality** : Exploration inefficace\n- **Solution** : HMC, Riemannian MCMC\n- **Preconditioning** : Transformation d'espace\n\n**Multimodalité :**\n- **Mode Switching** : Difficulté à changer de mode\n- **Solution** : Parallel Tempering, Annealed Importance\n- **Initialization** : Démarrage multiple\n\n**Computational Cost :**\n- **Likelihood Evaluation** : Coût par itération\n- **Solution** : Approximate Bayesian Computation\n- **Subsampling** : Mini-batch MCMC\n\n**🌟 Révolution et Impact :**\nMCMC a révolutionné la statistique bayésienne en rendant praticable l'inférence sur des modèles complexes impossibles à résoudre analytiquement. Avec l'essor du machine learning, MCMC devient essentiel pour quantifier l'incertitude, permettant une IA plus robuste et interprétable. L'intégration avec l'automatic differentiation et le calcul GPU ouvre de nouvelles frontières pour l'inférence bayésienne à grande échelle.",
    category: "statistiques",
    icon: "GitBranch"
  },
  {
    term: "Modèles de Markov cachés (Hidden Markov Models - HMM)",
    description: "Modèle statistique dans lequel le système modélisé est supposé être un processus de Markov avec des états non observés (cachés). Utilisé en reconnaissance vocale, bioinformatique et finance.",
    category: "statistiques",
    icon: "Eye"
  },
  {
    term: "Analyse de survie (Survival Analysis)",
    description: "**La science du temps qui reste !** Comme un médecin qui prédit l'espérance de vie d'un patient ou un ingénieur qui estime la durée de vie d'une machine, l'analyse de survie modélise le temps jusqu'à ce qu'un événement critique se produise.\n\n**⏰ Analogie Médicale :**\nImaginez suivre 1000 patients atteints d'une maladie : certains guérissent rapidement, d'autres vivent des années, quelques-uns quittent l'étude. L'analyse de survie extrait des insights même avec ces données 'incomplètes'.\n\n**🎯 Concepts Fondamentaux :**\n\n**Fonction de Survie S(t) :**\n- Probabilité de survivre au-delà du temps t\n- S(t) = P(T > t) où T = temps de survie\n- Décroissante de 1 (t=0) vers 0 (t=∞)\n\n**Fonction de Risque h(t) :**\n- Taux instantané de défaillance au temps t\n- h(t) = lim[P(t ≤ T < t+Δt | T ≥ t)] / Δt\n- Peut augmenter, diminuer, ou rester constant\n\n**🚨 Défi de la Censure :**\n\n**Types de Censure :**\n• **Droite** : Événement non observé à la fin de l'étude\n• **Gauche** : Événement déjà survenu au début\n• **Intervalle** : Événement dans une période connue\n• **Informative** : Censure liée au risque d'événement\n\n**Impact Critique :**\n- Ignorer la censure → Biais majeurs\n- Sous-estimation des temps de survie\n- Conclusions erronées sur l'efficacité\n\n**📊 Méthodes Classiques :**\n\n**Estimateur de Kaplan-Meier :**\n- Estimation non-paramétrique de S(t)\n- Courbes de survie en escalier\n- Intervalles de confiance\n- Test du log-rank pour comparaisons\n\n**Modèle de Cox (Proportional Hazards) :**\n- h(t|x) = h₀(t) × exp(βx)\n- Semi-paramétrique (pas d'hypothèse sur h₀)\n- Hazard Ratios pour interpréter les effets\n- Standard en recherche médicale\n\n**Modèles Paramétriques :**\n- **Weibull** : Risque monotone (croissant/décroissant)\n- **Exponentiel** : Risque constant\n- **Log-normal** : Risque en cloche\n- **Gamma généralisé** : Très flexible\n\n**🎯 Applications Diversifiées :**\n\n**Médecine & Santé :**\n- Essais cliniques (survie patients)\n- Épidémiologie (progression maladie)\n- Pharmacovigilance (effets secondaires)\n\n**Business & Marketing :**\n- **Customer Churn** : Temps avant désabonnement\n- **CLV** : Customer Lifetime Value\n- **Rétention** : Durée d'engagement client\n\n**Ingénierie & Fiabilité :**\n- Durée de vie des composants\n- Maintenance prédictive\n- Analyse des pannes système\n\n**Finance :**\n- Défaut de crédit\n- Durée des investissements\n- Risque de marché\n\n**🛠️ Outils Modernes :**\n\n**Packages R :**\n- `survival` : Fonctions de base\n- `survminer` : Visualisations élégantes\n- `flexsurv` : Modèles flexibles\n\n**Python :**\n- `lifelines` : Complet et intuitif\n- `scikit-survival` : Intégration sklearn\n- `pycox` : Deep learning pour survie\n\n**📈 Extensions Avancées :**\n\n**Modèles Multi-états :**\n- Transitions entre états multiples\n- Maladie → Rémission → Rechute → Décès\n\n**Survie Concurrente :**\n- Risques compétitifs multiples\n- Décès par cancer vs autres causes\n\n**Machine Learning :**\n- **Random Survival Forest** : Ensembles d'arbres\n- **DeepSurv** : Réseaux de neurones\n- **DeepHit** : Risques concurrents\n\n**⚡ Métriques d'Évaluation :**\n- **C-index** : Concordance (équivalent AUC)\n- **Brier Score** : Erreur de prédiction temporelle\n- **IBS** : Integrated Brier Score\n- **Time-dependent AUC** : Performance temporelle\n\n**💡 Insights Stratégiques :**\nNetflix utilise l'analyse de survie pour prédire le churn avec 85% de précision, permettant des interventions ciblées qui réduisent l'attrition de 23%. En médecine, elle guide 90% des décisions thérapeutiques en oncologie.",
    category: "statistiques",
    icon: "Clock"
  }
];