/**
 * Model Evaluation and Metrics
 * Performance metrics, validation techniques, and model assessment methods
 */

import { GlossaryEntry } from './types';

export const evaluationTerms: GlossaryEntry[] = [
  {
    term: "√âvaluation de mod√®les (Model Evaluation)",
    description: "Processus d'assessment des performances d'un mod√®le ML en utilisant diverses m√©triques et techniques de validation pour d√©terminer sa qualit√© et capacit√© de g√©n√©ralisation.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Matrice de confusion (Confusion Matrix)",
    description: "La matrice de confusion est comme le bulletin scolaire d√©taill√© d'un mod√®le de classification - elle r√©v√®le exactement o√π il excelle et o√π il √©choue ! **Anatomie visuelle** : tableau 2x2 (binaire) ou n√ón (multiclasse) croisant pr√©dictions vs r√©alit√©. **Les 4 cases magiques** (binaire) : **TP** (Vrais Positifs) = 'Bravo, bien vu !', **TN** (Vrais N√©gatifs) = 'Correct, rien √† signaler', **FP** (Faux Positifs) = 'Fausse alerte !', **FN** (Faux N√©gatifs) = 'Rat√©, c'√©tait important !'. **Analogie m√©dicale** : diagnostic de maladie - FP = patient sain diagnostiqu√© malade (stress inutile), FN = patient malade non d√©tect√© (danger !). **Lecture intuitive** : diagonale = succ√®s, hors-diagonale = erreurs. Plus la diagonale est 'chaude' et les c√¥t√©s 'froids', mieux c'est ! **Insights pr√©cieux** : r√©v√®le les **confusions sp√©cifiques** (classe A confondue avec B), **d√©s√©quilibres** de performance, **patterns d'erreurs**. **Calculs d√©riv√©s** : toutes les m√©triques importantes (pr√©cision, rappel, F1, accuracy) se calculent √† partir d'elle. **Visualisation** : heatmap color√©e pour identifier rapidement les probl√®mes. **Cas multiclasse** : matrice n√ón r√©v√©lant les confusions entre toutes les paires de classes. La matrice de confusion transforme des chiffres abstraits en diagnostic visuel actionnable !",
    category: "evaluation",
    icon: "Grid3x3"
  },
  {
    term: "Pr√©cision (Precision)",
    description: "La pr√©cision r√©pond √† la question cruciale : 'Quand mon mod√®le dit OUI, √† quelle fr√©quence a-t-il raison ?' - c'est la mesure de la fiabilit√© de ses pr√©dictions positives ! **Formule simple** : Pr√©cision = TP/(TP+FP) = Vrais Positifs / Tous les Positifs pr√©dits. **Analogie m√©dicale** : sur 100 patients diagnostiqu√©s 'malades', combien le sont vraiment ? Une pr√©cision de 90% = 90 vrais malades, 10 fausses alertes. **Analogie spam** : sur 100 emails class√©s 'spam', combien sont vraiment du spam ? **Quand privil√©gier la pr√©cision** : co√ªt √©lev√© des **faux positifs** - diagnostic m√©dical grave, recommandations produits, d√©tection de fraude (√©viter d'emb√™ter les clients honn√™tes). **Trade-off fondamental** : augmenter la pr√©cision (√™tre plus s√©lectif) peut r√©duire le rappel (rater des vrais cas). **Exemple concret** : d√©tecteur de tumeurs avec pr√©cision 95% = sur 100 'tumeurs d√©tect√©es', 95 sont r√©elles, 5 sont des fausses alertes (stress inutile). **Interpr√©tation business** : pr√©cision √©lev√©e = confiance dans les alertes, moins de 'bruit', mais risque de rater des cas. **Pi√®ge classique** : pr√©cision parfaite (100%) facile √† obtenir en √©tant ultra-conservateur, mais au d√©triment du rappel. **Contexte d√©cisionnel** : pr√©f√©rer pr√©cision quand l'action suite √† une pr√©diction positive est co√ªteuse ou irr√©versible.",
    category: "evaluation",
    icon: "Target"
  },
  {
    term: "Rappel (Recall/Sensitivity)",
    description: "Le rappel r√©pond √† la question vitale : 'De tous les vrais cas positifs, combien mon mod√®le en a-t-il d√©tect√©s ?' - c'est la mesure de sa capacit√© √† ne rien laisser passer ! **Formule essentielle** : Rappel = TP/(TP+FN) = Vrais Positifs / Tous les Positifs r√©els. **Analogie s√©curitaire** : sur 100 vrais criminels, combien le syst√®me de surveillance en a-t-il rep√©r√©s ? Rappel 80% = 80 d√©tect√©s, 20 √©chappent ! **Analogie m√©dicale** : sur 100 patients r√©ellement malades, combien le test en d√©tecte-t-il ? **Quand privil√©gier le rappel** : co√ªt catastrophique des **faux n√©gatifs** - d√©tection de cancer, syst√®mes de s√©curit√©, alertes d'urgence (mieux vaut trop d'alertes que rater un danger). **Trade-off in√©vitable** : augmenter le rappel (√™tre moins s√©lectif) g√©n√®re souvent plus de faux positifs, r√©duisant la pr√©cision. **Exemple critique** : d√©tecteur d'incendie avec rappel 95% = d√©tecte 95% des vrais incendies, mais rate 5% (potentiellement catastrophique). **Synonymes** : Sensibilit√©, Taux de Vrais Positifs (TPR). **Interpr√©tation business** : rappel √©lev√© = s√©curit√© maximale, aucun cas important rat√©, mais plus de 'bruit'. **Contexte d√©cisionnel** : privil√©gier rappel quand rater un cas positif a des cons√©quences graves ou irr√©versibles. **Analogie filet** : rappel = taille des mailles du filet - plus fines (rappel √©lev√©) attrapent plus de poissons mais aussi plus de d√©chets.",
    category: "evaluation",
    icon: "Search"
  },
  {
    term: "F1-Score",
    description: "Le F1-Score est comme un diplomate qui n√©gocie la paix entre Pr√©cision et Rappel : il trouve le compromis parfait quand ces deux m√©triques rivales se disputent ! **Formule magique** : F1 = 2 √ó (Pr√©cision √ó Rappel) / (Pr√©cision + Rappel) = moyenne harmonique (plus stricte que moyenne arithm√©tique). **Pourquoi harmonique ?** : punit s√©v√®rement les d√©s√©quilibres - si Pr√©cision=90% et Rappel=10%, F1=18% (pas 50% !). Force l'√©quilibre ! **Analogie sportive** : comme noter un athl√®te sur sprint ET endurance - exceller dans un seul domaine ne suffit pas, il faut √™tre bon partout. **Cas d'usage parfait** : datasets d√©s√©quilibr√©s o√π l'accuracy est trompeuse (99% de classe majoritaire). **Exemple concret** : d√©tection de fraude - F1 √©lev√© = bon √©quilibre entre 'attraper les fraudeurs' (rappel) et '√©viter les fausses accusations' (pr√©cision). **Interpr√©tation** : F1=1.0 (parfait), F1=0.0 (catastrophique), F1>0.8 (g√©n√©ralement bon). **Avantage cl√©** : m√©trique unique qui r√©sume la performance globale sur la classe positive. **Limitation** : ignore les vrais n√©gatifs (pas toujours probl√©matique). **Variantes** : F2-Score (favorise rappel), F0.5-Score (favorise pr√©cision). **Analogie culinaire** : comme √©quilibrer sucr√©-sal√© - trop de l'un g√¢che le plat, l'harmonie fait la perfection. **Usage pratique** : m√©trique de r√©f√©rence pour comparer des mod√®les sur t√¢ches de classification binaire d√©s√©quilibr√©es.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Exactitude (Accuracy)",
    description: "**La m√©trique la plus intuitive mais la plus tra√Ætre !** L'exactitude est comme un thermom√®tre qui mesure la 'temp√©rature g√©n√©rale' de votre mod√®le - simple √† comprendre, mais qui peut masquer des probl√®mes graves.\n\n**üéØ Formule Ultra-Simple :**\nAccuracy = (TP + TN) / (TP + TN + FP + FN) = Pr√©dictions Correctes / Total des Pr√©dictions\n\n**üè´ Analogie Scolaire :**\nComme un pourcentage de bonnes r√©ponses √† un QCM : 85/100 questions correctes = 85% d'exactitude. Facile √† comprendre, rassurant... mais attention aux pi√®ges !\n\n**‚ö†Ô∏è Le Pi√®ge Mortel des Classes D√©s√©quilibr√©es :**\nImaginez d√©tecter une maladie rare (1% de la population) : un mod√®le 'idiot' qui dit toujours 'pas malade' aura 99% d'exactitude ! Impressionnant sur le papier, catastrophique en r√©alit√©.\n\n**üö® Exemple Concret du Pi√®ge :**\n- Dataset : 1000 emails (950 normaux, 50 spams)\n- Mod√®le paresseux : 'tout est normal' ‚Üí 95% d'exactitude\n- Probl√®me : 0% des spams d√©tect√©s !\n\n**‚úÖ Quand Utiliser l'Accuracy :**\n- Classes **√©quilibr√©es** (50/50 ou proche)\n- Co√ªt √©gal des erreurs (FP = FN)\n- Vue d'ensemble rapide des performances\n- Communication avec non-experts\n\n**‚ùå Quand l'√âviter :**\n- Classes tr√®s d√©s√©quilibr√©es\n- Co√ªt diff√©rent des types d'erreurs\n- D√©tection d'√©v√©nements rares\n- Applications critiques (m√©dical, s√©curit√©)\n\n**üîç Alternatives Plus Robustes :**\n- **F1-Score** : √©quilibre pr√©cision/rappel\n- **Balanced Accuracy** : moyenne des sensibilit√©s par classe\n- **Cohen's Kappa** : accord corrig√© du hasard\n- **AUC-ROC** : performance ind√©pendante du seuil\n\n**üí° R√®gle d'Or :**\nL'accuracy seule ne suffit JAMAIS - toujours l'accompagner d'autres m√©triques pour un diagnostic complet. C'est la m√©trique 'grand public' qui cache souvent la complexit√© r√©elle !",
    category: "evaluation",
    icon: "CheckCircle"
  },
  {
    term: "Sp√©cificit√© (Specificity)",
    description: "**Le gardien vigilant contre les fausses alertes !** La sp√©cificit√© mesure √† quel point votre mod√®le est dou√© pour dire 'NON' quand c'est vraiment NON - c'est l'art d'√©viter les faux positifs.\n\n**üéØ Formule Essentielle :**\nSp√©cificit√© = TN / (TN + FP) = Vrais N√©gatifs / Tous les N√©gatifs R√©els\n\n**üö® Question Cl√© :**\n'De tous les cas qui sont vraiment n√©gatifs, combien mon mod√®le les identifie-t-il correctement comme n√©gatifs ?'\n\n**üè• Analogie M√©dicale Parfaite :**\nTest de grossesse : sur 100 femmes NON enceintes, combien le test indique-t-il correctement 'n√©gatif' ? Sp√©cificit√© 95% = 95 r√©sultats corrects, 5 faux positifs (stress inutile !).\n\n**üîç Synonymes Importants :**\n- **Taux de Vrais N√©gatifs (TNR)**\n- **S√©lectivit√©**\n- **1 - Taux de Faux Positifs**\n\n**‚öñÔ∏è Dualit√© avec la Sensibilit√© :**\n- **Sensibilit√© (Rappel)** : 'Ne rien rater d'important'\n- **Sp√©cificit√©** : 'Ne pas crier au loup'\n- Trade-off in√©vitable : am√©liorer l'un d√©grade souvent l'autre\n\n**üéØ Cas d'Usage Critiques :**\n- **Screening m√©dical** : √©viter les fausses alertes co√ªteuses\n- **D√©tection de spam** : ne pas bloquer d'emails importants\n- **Syst√®mes de s√©curit√©** : r√©duire les fausses alarmes\n- **Contr√¥le qualit√©** : ne pas rejeter de bons produits\n\n**üìä Interpr√©tation Pratique :**\n- **Sp√©cificit√© > 95%** : Excellent, tr√®s peu de fausses alertes\n- **Sp√©cificit√© 80-95%** : Bon, acceptable pour la plupart des cas\n- **Sp√©cificit√© < 80%** : Probl√©matique, trop de faux positifs\n\n**‚ö†Ô∏è Pi√®ge Classique :**\nSp√©cificit√© parfaite (100%) facile √† obtenir en √©tant ultra-conservateur (tout classer n√©gatif), mais au d√©triment de la sensibilit√© !\n\n**üîÑ Relation avec ROC :**\nAxe X de la courbe ROC = 1 - Sp√©cificit√© = Taux de Faux Positifs. Plus la sp√©cificit√© est √©lev√©e, plus on est √† gauche sur la courbe.\n\n**üí° R√®gle Pratique :**\nPrivil√©gier la sp√©cificit√© quand le co√ªt d'une fausse alerte est √©lev√© (temps, argent, stress, ressources). C'est la m√©trique de la prudence et de la pr√©cision !",
    category: "evaluation",
    icon: "Shield"
  },
  {
    term: "Courbe ROC (ROC Curve)",
    description: "**üìà Le Tableau de Bord Universel de Classification !**\n\nComme un pilote qui surveille simultan√©ment vitesse et altitude, la courbe ROC r√©v√®le l'√©quilibre parfait entre sensibilit√© (d√©tecter les vrais positifs) et sp√©cificit√© (√©viter les faux positifs) √† travers tous les seuils possibles, offrant une vision panoramique des performances de votre mod√®le.\n\n**‚úàÔ∏è Analogie du Pilote :**\nImaginez un pilote ajustant ses instruments : trop sensible aux alertes (haute sensibilit√©) = beaucoup de fausses alarmes, pas assez sensible (haute sp√©cificit√©) = risque de rater des dangers r√©els. La courbe ROC cartographie ce dilemme √† chaque niveau de vigilance !\n\n**üìä Fondements Math√©matiques :**\n\n**Axes Fondamentaux :**\n```\nAxe X : Taux de Faux Positifs (FPR)\n      = FP / (FP + TN)\n      = 1 - Sp√©cificit√©\n      = \"Fausses Alarmes\"\n\nAxe Y : Taux de Vrais Positifs (TPR)\n      = TP / (TP + FN)\n      = Sensibilit√© = Rappel\n      = \"D√©tections R√©ussies\"\n```\n\n**Construction de la Courbe :**\n1. **Scores de Probabilit√©** : Mod√®le produit P(classe=1) ‚àà [0,1]\n2. **Seuils D√©croissants** : œÑ ‚àà [1, 0] par pas fins\n3. **Classification Binaire** : ≈∑ = 1 si P(y=1) ‚â• œÑ, sinon 0\n4. **Calcul M√©triques** : (FPR_œÑ, TPR_œÑ) pour chaque œÑ\n5. **Trac√©** : Points (FPR, TPR) reli√©s par segments\n\n**üé® Anatomie Visuelle :**\n\n**Points de R√©f√©rence :**\n- **Origine (0,0)** : Seuil = 1, tout class√© n√©gatif\n- **Coin (1,1)** : Seuil = 0, tout class√© positif\n- **Diagonale** : Performance al√©atoire (AUC = 0.5)\n- **Coin (0,1)** : Classificateur parfait (AUC = 1.0)\n\n**Forme Id√©ale :**\n```\nCaract√©ristiques :\n- Mont√©e rapide vers TPR = 1\n- Progression lente de FPR\n- Coude marqu√© vers (0,1)\n- Aire sous courbe maximale\n\nInterpr√©tation :\n- Excellent pouvoir discriminant\n- S√©paration claire des classes\n- Seuils optimaux √©vidents\n```\n\n**üîç Patterns d'Interpr√©tation :**\n\n**Courbe Concave (Bonne) :**\n```\nCaract√©ristiques :\n- Courbure vers le coin sup√©rieur gauche\n- Pente d√©croissante\n- AUC > 0.7\n\nInterpr√©tation :\n- Mod√®le discriminant\n- Trade-off favorable\n- Seuils exploitables\n```\n\n**Courbe Lin√©aire (Al√©atoire) :**\n```\nCaract√©ristiques :\n- Droite de (0,0) √† (1,1)\n- Pente constante = 1\n- AUC ‚âà 0.5\n\nInterpr√©tation :\n- Aucun pouvoir pr√©dictif\n- Performance al√©atoire\n- Mod√®le inutile\n```\n\n**Courbe Convexe (Probl√©matique) :**\n```\nCaract√©ristiques :\n- Courbure vers le coin inf√©rieur droit\n- AUC < 0.5\n- Performance invers√©e\n\nInterpr√©tation :\n- Mod√®le \"anti-pr√©dictif\"\n- Inverser les pr√©dictions am√©liore\n- Erreur de labellisation possible\n```\n\n**üìê M√©triques D√©riv√©es :**\n\n**AUC (Area Under Curve) :**\n```\nAUC = ‚à´‚ÇÄ¬π TPR(FPR) d(FPR)\n    = P(score(+) > score(-))\n    = Probabilit√© de ranking correct\n\nInterpr√©tation :\n- AUC = 1.0 : Classificateur parfait\n- AUC = 0.5 : Performance al√©atoire\n- AUC = 0.0 : Parfait mais invers√©\n```\n\n**Point Optimal (Youden's J) :**\n```\nJ = TPR - FPR = Sensibilit√© + Sp√©cificit√© - 1\nPoint Optimal = argmax_œÑ (TPR_œÑ - FPR_œÑ)\n\nDistance √† (0,1) :\nd = ‚àö[(1-TPR)¬≤ + FPR¬≤]\nPoint Optimal = argmin_œÑ d_œÑ\n```\n\n**Partial AUC :**\n```\npAUC = ‚à´‚ÇÄ^{FPR_max} TPR(FPR) d(FPR)\n     = AUC dans r√©gion sp√©cifique\n     = Utile pour contraintes m√©tier\n\nExemple :\npAUC‚ÇÄ.‚ÇÅ = Performance pour FPR ‚â§ 10%\n```\n\n**üöÄ Applications Critiques :**\n\n**Diagnostic M√©dical :**\n```\nContexte :\n- √âquilibrer sensibilit√©/sp√©cificit√©\n- Co√ªts diff√©rents FN vs FP\n- Seuils adaptatifs par pathologie\n\nOptimisation ROC :\n- Maximiser AUC globale\n- Contraintes sur FPR (< 5%)\n- Points op√©rationnels multiples\n```\n\n**D√©tection de Fraude :**\n```\nObjectifs :\n- D√©tecter fraudes (haute sensibilit√©)\n- Limiter fausses alertes (co√ªt op√©rationnel)\n- Adaptation temps r√©el\n\nStrat√©gie ROC :\n- pAUC pour FPR faible\n- Seuils dynamiques\n- Monitoring continu\n```\n\n**Syst√®mes de Recommandation :**\n```\nD√©fis :\n- Pr√©dire pr√©f√©rences utilisateur\n- √âviter recommandations inad√©quates\n- Personnalisation massive\n\nUsage ROC :\n- AUC par segment utilisateur\n- Calibration des scores\n- A/B testing des seuils\n```\n\n**üîß Impl√©mentation Pratique :**\n\n**Scikit-learn Complet :**\n```python\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Calcul de la courbe ROC\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Visualisation avanc√©e\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n\n# 1. Courbe ROC principale\nRocCurveDisplay.from_predictions(\n    y_true, y_scores, ax=ax1, name=f'ROC (AUC = {roc_auc:.3f})'\n)\nax1.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')\nax1.set_title('ROC Curve')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# 2. Point optimal (Youden's J)\nyouden_j = tpr - fpr\noptimal_idx = np.argmax(youden_j)\noptimal_threshold = thresholds[optimal_idx]\noptimal_fpr = fpr[optimal_idx]\noptimal_tpr = tpr[optimal_idx]\n\nax1.plot(optimal_fpr, optimal_tpr, 'ro', markersize=10, \n         label=f'Optimal (œÑ={optimal_threshold:.3f})')\nax1.legend()\n\n# 3. Distribution des scores\nscores_pos = y_scores[y_true == 1]\nscores_neg = y_scores[y_true == 0]\n\nax2.hist(scores_neg, bins=50, alpha=0.7, label='Negative', color='red')\nax2.hist(scores_pos, bins=50, alpha=0.7, label='Positive', color='blue')\nax2.axvline(optimal_threshold, color='green', linestyle='--', \n           label=f'Optimal Threshold')\nax2.set_xlabel('Prediction Score')\nax2.set_ylabel('Frequency')\nax2.set_title('Score Distributions')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# 4. M√©triques vs Seuil\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecisions = []\nrecalls = []\nf1_scores = []\nspecificities = []\n\nfor threshold in thresholds:\n    y_pred = (y_scores >= threshold).astype(int)\n    precisions.append(precision_score(y_true, y_pred, zero_division=0))\n    recalls.append(recall_score(y_true, y_pred, zero_division=0))\n    f1_scores.append(f1_score(y_true, y_pred, zero_division=0))\n    specificities.append(1 - fpr[np.where(thresholds == threshold)[0][0]])\n\nax3.plot(thresholds, precisions, label='Precision', color='blue')\nax3.plot(thresholds, recalls, label='Recall (TPR)', color='red')\nax3.plot(thresholds, f1_scores, label='F1-Score', color='green')\nax3.plot(thresholds, specificities, label='Specificity', color='orange')\nax3.axvline(optimal_threshold, color='black', linestyle='--', alpha=0.7)\nax3.set_xlabel('Threshold')\nax3.set_ylabel('Metric Value')\nax3.set_title('Metrics vs Threshold')\nax3.legend()\nax3.grid(True, alpha=0.3)\n\n# 5. Courbe ROC zoom√©e (r√©gion int√©ressante)\ninteresting_region = fpr <= 0.2  # Focus sur FPR faible\nax4.plot(fpr[interesting_region], tpr[interesting_region], 'b-', linewidth=2)\nax4.plot(optimal_fpr, optimal_tpr, 'ro', markersize=8)\nax4.set_xlim(0, 0.2)\nax4.set_ylim(0.8, 1.0)\nax4.set_xlabel('False Positive Rate')\nax4.set_ylabel('True Positive Rate')\nax4.set_title('ROC Curve - High Specificity Region')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Statistiques d√©taill√©es\nprint(f\"\"\"\nROC Analysis Summary:\n{'='*50}\nAUC Score: {roc_auc:.4f}\nOptimal Threshold: {optimal_threshold:.4f}\nOptimal TPR: {optimal_tpr:.4f}\nOptimal FPR: {optimal_fpr:.4f}\nYouden's J: {youden_j[optimal_idx]:.4f}\n\nAt Optimal Threshold:\nPrecision: {precisions[optimal_idx]:.4f}\nRecall: {recalls[optimal_idx]:.4f}\nF1-Score: {f1_scores[optimal_idx]:.4f}\nSpecificity: {specificities[optimal_idx]:.4f}\n\"\"\")\n```\n\n**Analyse Multi-Classes :**\n```python\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom itertools import cycle\n\n# Binarisation One-vs-Rest\ny_bin = label_binarize(y_true, classes=np.unique(y_true))\nn_classes = y_bin.shape[1]\n\n# Calcul ROC pour chaque classe\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_scores[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# ROC micro-average\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_bin.ravel(), y_scores.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n# ROC macro-average\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Visualisation\nfig, ax = plt.subplots(figsize=(10, 8))\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'red', 'green'])\n\nfor i, color in zip(range(n_classes), colors):\n    ax.plot(fpr[i], tpr[i], color=color, lw=2,\n            label=f'Class {i} (AUC = {roc_auc[i]:.2f})')\n\nax.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', lw=4,\n        label=f'Micro-avg (AUC = {roc_auc[\"micro\"]:.2f})')\n\nax.plot(fpr[\"macro\"], tpr[\"macro\"], color='navy', linestyle=':', lw=4,\n        label=f'Macro-avg (AUC = {roc_auc[\"macro\"]:.2f})')\n\nax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('Multi-class ROC Curves')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.show()\n```\n\n**üéØ Optimisation Avanc√©e :**\n\n**Seuil M√©tier-Orient√© :**\n```python\ndef business_optimal_threshold(y_true, y_scores, cost_fp=1, cost_fn=5):\n    \"\"\"\n    Trouve le seuil optimal bas√© sur les co√ªts m√©tier\n    cost_fp: co√ªt d'un faux positif\n    cost_fn: co√ªt d'un faux n√©gatif\n    \"\"\"\n    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n    \n    # Calcul du co√ªt total pour chaque seuil\n    costs = []\n    for i, threshold in enumerate(thresholds):\n        y_pred = (y_scores >= threshold).astype(int)\n        \n        # Matrice de confusion\n        tn = np.sum((y_true == 0) & (y_pred == 0))\n        fp = np.sum((y_true == 0) & (y_pred == 1))\n        fn = np.sum((y_true == 1) & (y_pred == 0))\n        tp = np.sum((y_true == 1) & (y_pred == 1))\n        \n        # Co√ªt total\n        total_cost = cost_fp * fp + cost_fn * fn\n        costs.append(total_cost)\n    \n    # Seuil optimal = co√ªt minimal\n    optimal_idx = np.argmin(costs)\n    optimal_threshold = thresholds[optimal_idx]\n    \n    return optimal_threshold, costs[optimal_idx], costs\n\n# Usage\noptimal_thresh, min_cost, all_costs = business_optimal_threshold(\n    y_true, y_scores, cost_fp=1, cost_fn=10\n)\n\nprint(f\"Seuil optimal m√©tier: {optimal_thresh:.3f}\")\nprint(f\"Co√ªt minimal: {min_cost}\")\n\n# Visualisation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# ROC avec point optimal m√©tier\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\noptimal_idx = np.where(thresholds == optimal_thresh)[0][0]\n\nax1.plot(fpr, tpr, 'b-', label=f'ROC (AUC = {auc(fpr, tpr):.3f})')\nax1.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=10,\n         label=f'Business Optimal')\nax1.plot([0, 1], [0, 1], 'k--', alpha=0.5)\nax1.set_xlabel('False Positive Rate')\nax1.set_ylabel('True Positive Rate')\nax1.set_title('ROC with Business Optimal Point')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Co√ªt vs Seuil\nax2.plot(thresholds, all_costs, 'g-', linewidth=2)\nax2.axvline(optimal_thresh, color='red', linestyle='--',\n           label=f'Optimal = {optimal_thresh:.3f}')\nax2.set_xlabel('Threshold')\nax2.set_ylabel('Total Business Cost')\nax2.set_title('Business Cost vs Threshold')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\n```\n\n**Analyse de Stabilit√© :**\n```python\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import resample\n\ndef roc_stability_analysis(X, y, model, n_bootstrap=100, cv_folds=5):\n    \"\"\"\n    Analyse la stabilit√© de la courbe ROC\n    \"\"\"\n    # Bootstrap AUC\n    bootstrap_aucs = []\n    for i in range(n_bootstrap):\n        X_boot, y_boot = resample(X, y, random_state=i)\n        model.fit(X_boot, y_boot)\n        y_scores = model.predict_proba(X)[:, 1]\n        fpr, tpr, _ = roc_curve(y, y_scores)\n        bootstrap_aucs.append(auc(fpr, tpr))\n    \n    # Cross-validation AUC\n    cv_aucs = cross_val_score(model, X, y, cv=StratifiedKFold(cv_folds), \n                             scoring='roc_auc')\n    \n    # Statistiques\n    results = {\n        'bootstrap_mean': np.mean(bootstrap_aucs),\n        'bootstrap_std': np.std(bootstrap_aucs),\n        'bootstrap_ci': np.percentile(bootstrap_aucs, [2.5, 97.5]),\n        'cv_mean': np.mean(cv_aucs),\n        'cv_std': np.std(cv_aucs),\n        'stability_score': 1 - np.std(bootstrap_aucs)  # Plus proche de 1 = plus stable\n    }\n    \n    return results, bootstrap_aucs, cv_aucs\n\n# Usage\nstability_results, boot_aucs, cv_aucs = roc_stability_analysis(\n    X, y, RandomForestClassifier(random_state=42)\n)\n\nprint(f\"\"\"\nStability Analysis:\n{'='*30}\nBootstrap AUC: {stability_results['bootstrap_mean']:.4f} ¬± {stability_results['bootstrap_std']:.4f}\nBootstrap 95% CI: [{stability_results['bootstrap_ci'][0]:.4f}, {stability_results['bootstrap_ci'][1]:.4f}]\nCV AUC: {stability_results['cv_mean']:.4f} ¬± {stability_results['cv_std']:.4f}\nStability Score: {stability_results['stability_score']:.4f}\n\"\"\")\n\n# Visualisation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Distribution Bootstrap\nax1.hist(boot_aucs, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nax1.axvline(stability_results['bootstrap_mean'], color='red', linestyle='--',\n           label=f\"Mean = {stability_results['bootstrap_mean']:.3f}\")\nax1.axvline(stability_results['bootstrap_ci'][0], color='orange', linestyle=':',\n           label=f\"95% CI\")\nax1.axvline(stability_results['bootstrap_ci'][1], color='orange', linestyle=':')\nax1.set_xlabel('AUC Score')\nax1.set_ylabel('Frequency')\nax1.set_title('Bootstrap AUC Distribution')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Comparaison Bootstrap vs CV\nax2.boxplot([boot_aucs, cv_aucs], labels=['Bootstrap', 'Cross-Validation'])\nax2.set_ylabel('AUC Score')\nax2.set_title('AUC Stability Comparison')\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\n```\n\n**üìä Visualisations Avanc√©es :**\n\n**ROC Interactive avec Plotly :**\n```python\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Donn√©es\nfpr, tpr, thresholds = roc_curve(y_true, y_scores)\nroc_auc = auc(fpr, tpr)\n\n# Subplot interactif\nfig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=('ROC Curve', 'Threshold Analysis', \n                   'Score Distribution', 'Confusion Matrix Heatmap'),\n    specs=[[{'type': 'scatter'}, {'type': 'scatter'}],\n           [{'type': 'histogram'}, {'type': 'heatmap'}]]\n)\n\n# ROC Curve\nfig.add_trace(\n    go.Scatter(\n        x=fpr, y=tpr,\n        mode='lines+markers',\n        name=f'ROC (AUC = {roc_auc:.3f})',\n        hovertemplate='FPR: %{x:.3f}<br>TPR: %{y:.3f}<extra></extra>',\n        line=dict(color='blue', width=3)\n    ),\n    row=1, col=1\n)\n\n# Ligne al√©atoire\nfig.add_trace(\n    go.Scatter(\n        x=[0, 1], y=[0, 1],\n        mode='lines',\n        name='Random',\n        line=dict(color='red', dash='dash'),\n        showlegend=False\n    ),\n    row=1, col=1\n)\n\n# Point optimal\nyouden_j = tpr - fpr\noptimal_idx = np.argmax(youden_j)\nfig.add_trace(\n    go.Scatter(\n        x=[fpr[optimal_idx]], y=[tpr[optimal_idx]],\n        mode='markers',\n        name='Optimal Point',\n        marker=dict(size=12, color='red', symbol='star'),\n        hovertemplate=f'Optimal<br>FPR: {fpr[optimal_idx]:.3f}<br>TPR: {tpr[optimal_idx]:.3f}<extra></extra>'\n    ),\n    row=1, col=1\n)\n\n# M√©triques vs Seuil\nfig.add_trace(\n    go.Scatter(\n        x=thresholds, y=tpr,\n        mode='lines',\n        name='TPR (Sensitivity)',\n        line=dict(color='green')\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=thresholds, y=1-fpr,\n        mode='lines',\n        name='TNR (Specificity)',\n        line=dict(color='orange')\n    ),\n    row=1, col=2\n)\n\n# Distribution des scores\nscores_pos = y_scores[y_true == 1]\nscores_neg = y_scores[y_true == 0]\n\nfig.add_trace(\n    go.Histogram(\n        x=scores_neg,\n        name='Negative Class',\n        opacity=0.7,\n        nbinsx=50,\n        marker_color='red'\n    ),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Histogram(\n        x=scores_pos,\n        name='Positive Class',\n        opacity=0.7,\n        nbinsx=50,\n        marker_color='blue'\n    ),\n    row=2, col=1\n)\n\n# Matrice de confusion au seuil optimal\ny_pred_optimal = (y_scores >= thresholds[optimal_idx]).astype(int)\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred_optimal)\n\nfig.add_trace(\n    go.Heatmap(\n        z=cm,\n        x=['Predicted Neg', 'Predicted Pos'],\n        y=['Actual Neg', 'Actual Pos'],\n        colorscale='Blues',\n        showscale=True,\n        text=cm,\n        texttemplate=\"%{text}\",\n        textfont={\"size\":16}\n    ),\n    row=2, col=2\n)\n\n# Mise √† jour des axes\nfig.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\nfig.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\nfig.update_xaxes(title_text=\"Threshold\", row=1, col=2)\nfig.update_yaxes(title_text=\"Rate\", row=1, col=2)\nfig.update_xaxes(title_text=\"Prediction Score\", row=2, col=1)\nfig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n\nfig.update_layout(\n    title='Interactive ROC Analysis Dashboard',\n    height=800,\n    showlegend=True\n)\n\nfig.show()\n```\n\n**üåü Impact et Applications Modernes :**\nLa courbe ROC reste l'outil de r√©f√©rence pour √©valuer les classificateurs binaires dans des contextes √©quilibr√©s. Elle guide les syst√®mes de scoring de cr√©dit (banques), la d√©tection de malwares (cybers√©curit√©), les tests diagnostiques (m√©decine), et l'optimisation des campagnes marketing (e-commerce). Son universalit√© et son interpr√©tation intuitive en font un standard incontournable, compl√©t√©e par la courbe Pr√©cision-Rappel pour les cas d√©s√©quilibr√©s.",
    category: "evaluation",
    icon: "TrendingUp"
  },
  {
    term: "AUC (Area Under Curve)",
    description: "**La mesure ultime de discrimination !** Comme un test m√©dical qui doit parfaitement s√©parer les malades des bien-portants, l'AUC quantifie la capacit√© d'un mod√®le √† distinguer entre les classes positives et n√©gatives sur l'ensemble du spectre de seuils possibles.\n\n**üìä Analogie G√©om√©trique :**\nImaginez la courbe ROC comme le profil d'une montagne : plus l'aire sous cette courbe est grande (proche de 1.0), plus le mod√®le est performant. Une AUC de 0.5 ressemble √† une ligne droite (performance al√©atoire), tandis qu'une AUC de 1.0 forme un carr√© parfait.\n\n**üéØ Interpr√©tation Intuitive :**\n\n**Signification Probabiliste :**\nL'AUC repr√©sente la probabilit√© qu'un mod√®le classe correctement un exemple positif choisi al√©atoirement plus haut qu'un exemple n√©gatif choisi al√©atoirement.\n\n**√âchelle de Performance :**\n‚Ä¢ **0.9 - 1.0** : Excellence (diagnostic m√©dical)\n‚Ä¢ **0.8 - 0.9** : Tr√®s bon (d√©tection fraude)\n‚Ä¢ **0.7 - 0.8** : Bon (marketing pr√©dictif)\n‚Ä¢ **0.6 - 0.7** : Moyen (am√©lioration n√©cessaire)\n‚Ä¢ **0.5 - 0.6** : Faible (√† peine mieux que le hasard)\n‚Ä¢ **< 0.5** : Pire que le hasard (inverser les pr√©dictions !)\n\n**üîç Construction Math√©matique :**\n\n**Courbe ROC :**\n- **Axe X** : Taux de Faux Positifs (1 - Sp√©cificit√©)\n- **Axe Y** : Taux de Vrais Positifs (Sensibilit√©)\n- **Points** : Performance √† diff√©rents seuils\n\n**Calcul de l'AUC :**\n```\nAUC = ‚à´‚ÇÄ¬π TPR(FPR) d(FPR)\n```\n\n**M√©thode Trap√©zo√Ødale :**\n- Approximation num√©rique par trap√®zes\n- Pr√©cision d√©pendante du nombre de seuils\n- Impl√©mentation standard dans sklearn\n\n**‚ö° Avantages Distinctifs :**\n\n**Invariance au Seuil :**\n- √âvalue toutes les performances possibles\n- Pas besoin de choisir un seuil optimal\n- Vision globale du mod√®le\n\n**Invariance √† l'√âchelle :**\n- Mesure qualit√© du ranking, pas valeurs absolues\n- Robuste aux transformations monotones\n- Comparable entre mod√®les diff√©rents\n\n**üö® Limitations Critiques :**\n\n**Classes D√©s√©quilibr√©es :**\n- AUC peut √™tre optimiste\n- Privil√©gie la classe majoritaire\n- Pr√©f√©rer AUC-PR (Precision-Recall)\n\n**Interpr√©tation M√©tier :**\n- Pas directement li√©e aux co√ªts business\n- Ne refl√®te pas l'impact des erreurs\n- Compl√©ment n√©cessaire avec m√©triques m√©tier\n\n**üéØ Applications Sectorielles :**\n\n**M√©decine :**\n- **Diagnostic** : AUC > 0.95 pour tests critiques\n- **Screening** : Balance sensibilit√©/sp√©cificit√©\n- **Biomarqueurs** : Validation de nouveaux tests\n\n**Finance :**\n- **Cr√©dit** : Scoring de risque de d√©faut\n- **Fraude** : D√©tection transactions suspectes\n- **Trading** : Signaux d'achat/vente\n\n**Marketing :**\n- **Churn** : Pr√©diction d√©sabonnement\n- **Conversion** : Probabilit√© d'achat\n- **Segmentation** : Classification clients\n\n**üõ†Ô∏è Variantes Sp√©cialis√©es :**\n\n**AUC-PR (Precision-Recall) :**\n- Meilleure pour classes d√©s√©quilibr√©es\n- Focus sur la classe positive\n- Moins sensible aux vrais n√©gatifs\n\n**Partial AUC :**\n- AUC dans une r√©gion sp√©cifique\n- Utile pour contraintes m√©tier\n- Ex: FPR < 0.1 pour applications critiques\n\n**Multi-class AUC :**\n- **One-vs-Rest** : AUC moyenne par classe\n- **One-vs-One** : AUC pour chaque paire\n- **Macro/Micro averaging** : Strat√©gies d'agr√©gation\n\n**üìà Optimisation Pratique :**\n\n**Feature Engineering :**\n- S√©lection bas√©e sur AUC individuelle\n- Interactions augmentant la s√©parabilit√©\n- Transformations non-lin√©aires\n\n**Hyperparameter Tuning :**\n- Validation crois√©e avec AUC\n- Optimisation bay√©sienne\n- Early stopping bas√© sur AUC validation\n\n**üî¨ Tests Statistiques :**\n\n**Comparaison de Mod√®les :**\n- Test de DeLong pour AUC\n- Bootstrap pour intervalles de confiance\n- Correction de Bonferroni pour tests multiples\n\n**Significativit√© :**\n- p-value < 0.05 pour diff√©rence significative\n- Taille d'effet (diff√©rence d'AUC)\n- Puissance statistique du test\n\n**üí° Bonnes Pratiques :**\n- **Validation crois√©e** stratifi√©e\n- **Intervalles de confiance** syst√©matiques\n- **Comparaison** avec baseline simple\n- **Analyse** des courbes ROC compl√®tes\n- **Contexte m√©tier** toujours consid√©r√©\n\n**üìä Impact Mesurable :**\nGoogle am√©liore ses mod√®les publicitaires de 0.001 AUC par trimestre, g√©n√©rant des millions de revenus suppl√©mentaires. En m√©decine, une am√©lioration d'AUC de 0.05 peut sauver des milliers de vies.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Courbe Pr√©cision-Rappel",
    description: "**üéØ Le Radar de Performance pour Classes D√©s√©quilibr√©es !**\n\nComme un radar qui r√©v√®le les objets cach√©s dans le brouillard, la courbe Pr√©cision-Rappel illumine les performances r√©elles de votre mod√®le sur les classes minoritaires, l√† o√π la courbe ROC peut √™tre trompeusement optimiste.\n\n**üîç Analogie du D√©tective :**\nImaginez un d√©tective recherchant des criminels dans une foule. La **pr√©cision** mesure : \"Parmi tous ceux que j'ai arr√™t√©s, combien sont vraiment coupables ?\" Le **rappel** demande : \"Parmi tous les vrais criminels, combien ai-je r√©ussi √† attraper ?\" La courbe r√©v√®le ce dilemme √† chaque niveau de vigilance !\n\n**üìä Fondements Math√©matiques :**\n\n**D√©finitions Fondamentales :**\n```\nPr√©cision = TP / (TP + FP)\n          = Vrais Positifs / Pr√©dictions Positives\n          = \"Qualit√© des d√©tections\"\n\nRappel = TP / (TP + FN)\n       = Vrais Positifs / Positifs R√©els\n       = \"Compl√©tude des d√©tections\"\n```\n\n**Construction de la Courbe :**\n1. **Scores de Probabilit√©** : Mod√®le produit P(classe=1)\n2. **Seuils Variables** : œÑ ‚àà [0, 1] par pas fins\n3. **Classification** : ≈∑ = 1 si P(y=1) ‚â• œÑ, sinon 0\n4. **Calcul M√©triques** : (Pr√©cision_œÑ, Rappel_œÑ) pour chaque œÑ\n5. **Trac√©** : Rappel en X, Pr√©cision en Y\n\n**üé® Anatomie Visuelle :**\n\n**Forme Caract√©ristique :**\n- **D√©but** : (Rappel=0, Pr√©cision=1) - Seuil tr√®s √©lev√©\n- **Fin** : (Rappel=1, Pr√©cision=baseline) - Seuil tr√®s bas\n- **Tendance** : D√©croissance g√©n√©rale (trade-off)\n- **Aire** : Average Precision (AP)\n\n**Points Critiques :**\n```\nPoint Optimal : Maximum F1-Score\nF1 = 2 √ó (Pr√©cision √ó Rappel) / (Pr√©cision + Rappel)\n\nPoint d'√âquilibre : Pr√©cision = Rappel\nBreak-Even Point (BEP)\n\nSeuil M√©tier : Selon contraintes op√©rationnelles\n```\n\n**üîç Patterns d'Interpr√©tation :**\n\n**Courbe Id√©ale :**\n```\nCaract√©ristiques :\n- Reste proche de Pr√©cision = 1\n- Couvre tout l'espace Rappel [0,1]\n- Aire sous courbe (AP) proche de 1\n- D√©clin tardif et progressif\n\nInterpr√©tation :\n- Mod√®le excellent\n- S√©paration claire des classes\n- Peu de faux positifs\n```\n\n**Courbe D√©grad√©e :**\n```\nCaract√©ristiques :\n- Chute rapide de pr√©cision\n- Aire sous courbe faible\n- Proche de la ligne baseline\n- Oscillations importantes\n\nInterpr√©tation :\n- Mod√®le peu discriminant\n- Classes mal s√©par√©es\n- Beaucoup de faux positifs\n```\n\n**Courbe en Dents de Scie :**\n```\nCaract√©ristiques :\n- Variations abruptes\n- Pics et chutes altern√©s\n- Instabilit√© locale\n\nInterpr√©tation :\n- Dataset petit ou bruit√©\n- Mod√®le instable\n- Besoin de lissage\n```\n\n**üìê M√©triques D√©riv√©es :**\n\n**Average Precision (AP) :**\n```\nAP = Œ£(Rappel_n - Rappel_{n-1}) √ó Pr√©cision_n\n   = Aire sous la courbe PR\n   = R√©sum√© en un nombre [0,1]\n\nInterpr√©tation :\n- AP = 1 : Mod√®le parfait\n- AP = baseline : Mod√®le al√©atoire\n- AP > baseline : Mod√®le utile\n```\n\n**F1-Score Optimal :**\n```\nF1_max = max_œÑ [2 √ó P(œÑ) √ó R(œÑ) / (P(œÑ) + R(œÑ))]\n\nSeuil Optimal :\nœÑ_opt = argmax_œÑ F1(œÑ)\n\n√âquilibre Harmonique :\nMoyenne harmonique de Pr√©cision et Rappel\n```\n\n**Precision at K :**\n```\nP@K = Pr√©cision parmi les K premi√®res pr√©dictions\n    = M√©trique de ranking\n    = Important pour recommandations\n\nExemple :\nP@10 = 0.8 ‚Üí 8 vrais positifs dans le top 10\n```\n\n**üöÄ Applications Critiques :**\n\n**D√©tection d'Anomalies :**\n```\nContexte :\n- Classes tr√®s d√©s√©quilibr√©es (0.1% anomalies)\n- Co√ªt √©lev√© des faux n√©gatifs\n- ROC trompeusement optimiste\n\nStrat√©gie PR :\n- Focus sur le rappel √©lev√©\n- Pr√©cision acceptable selon co√ªt\n- Seuil adapt√© aux contraintes m√©tier\n```\n\n**Recherche d'Information :**\n```\nObjectif :\n- Retrouver documents pertinents\n- Minimiser documents non-pertinents\n- √âquilibrer exhaustivit√© et qualit√©\n\nM√©triques Cl√©s :\n- P@10, P@100 : Pr√©cision top r√©sultats\n- Rappel global : Couverture totale\n- F1 : √âquilibre optimal\n```\n\n**Diagnostic M√©dical :**\n```\nEnjeux :\n- D√©tecter maladies rares\n- √âviter faux n√©gatifs (danger)\n- Limiter faux positifs (co√ªt)\n\nOptimisation :\n- Rappel prioritaire (s√©curit√©)\n- Pr√©cision selon ressources\n- Seuils adaptatifs par pathologie\n```\n\n**üîß Impl√©mentation Pratique :**\n\n**Scikit-learn :**\n```python\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import PrecisionRecallDisplay\nimport matplotlib.pyplot as plt\n\n# Calcul de la courbe\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\nap_score = average_precision_score(y_true, y_scores)\n\n# Visualisation avanc√©e\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Courbe PR\nPrecisionRecallDisplay.from_predictions(\n    y_true, y_scores, ax=ax1, name=f'AP = {ap_score:.3f}'\n)\nax1.axhline(y=y_true.mean(), color='r', linestyle='--', \n           label=f'Baseline = {y_true.mean():.3f}')\nax1.set_title('Precision-Recall Curve')\nax1.legend()\n\n# F1-Score vs Seuil\nf1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\noptimal_idx = np.argmax(f1_scores)\noptimal_threshold = thresholds[optimal_idx]\n\nax2.plot(thresholds, f1_scores, 'b-', label='F1-Score')\nax2.axvline(x=optimal_threshold, color='r', linestyle='--', \n           label=f'Optimal œÑ = {optimal_threshold:.3f}')\nax2.axhline(y=f1_scores[optimal_idx], color='g', linestyle=':', \n           label=f'Max F1 = {f1_scores[optimal_idx]:.3f}')\nax2.set_xlabel('Threshold')\nax2.set_ylabel('F1-Score')\nax2.set_title('F1-Score vs Threshold')\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n**Analyse Multi-Classes :**\n```python\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import label_binarize\n\n# Binarisation pour multi-classes\ny_bin = label_binarize(y_true, classes=np.unique(y_true))\nn_classes = y_bin.shape[1]\n\n# Courbe PR par classe\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes = axes.ravel()\n\nfor i in range(min(n_classes, 4)):\n    precision, recall, _ = precision_recall_curve(y_bin[:, i], y_scores[:, i])\n    ap = average_precision_score(y_bin[:, i], y_scores[:, i])\n    \n    axes[i].plot(recall, precision, label=f'Class {i} (AP = {ap:.3f})')\n    axes[i].set_xlabel('Recall')\n    axes[i].set_ylabel('Precision')\n    axes[i].set_title(f'PR Curve - Class {i}')\n    axes[i].legend()\n    axes[i].grid(True)\n\nplt.tight_layout()\n```\n\n**üéØ Optimisation Avanc√©e :**\n\n**Seuil Adaptatif :**\n```python\ndef find_optimal_threshold(y_true, y_scores, metric='f1'):\n    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n    \n    if metric == 'f1':\n        f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n        optimal_idx = np.argmax(f1_scores)\n        return thresholds[optimal_idx], f1_scores[optimal_idx]\n    \n    elif metric == 'precision_at_recall':\n        # Pr√©cision maximale pour rappel >= 0.8\n        target_recall = 0.8\n        valid_indices = recall[:-1] >= target_recall\n        if np.any(valid_indices):\n            best_idx = np.argmax(precision[:-1][valid_indices])\n            return thresholds[valid_indices][best_idx]\n    \n    elif metric == 'recall_at_precision':\n        # Rappel maximal pour pr√©cision >= 0.9\n        target_precision = 0.9\n        valid_indices = precision[:-1] >= target_precision\n        if np.any(valid_indices):\n            best_idx = np.argmax(recall[:-1][valid_indices])\n            return thresholds[valid_indices][best_idx]\n\n# Usage\noptimal_threshold, best_f1 = find_optimal_threshold(y_true, y_scores)\nprint(f\"Seuil optimal: {optimal_threshold:.3f}, F1: {best_f1:.3f}\")\n```\n\n**Calibration des Probabilit√©s :**\n```python\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.isotonic import IsotonicRegression\n\n# Calibration isotonique\ncalibrated_clf = CalibratedClassifierCV(base_estimator, method='isotonic', cv=3)\ncalibrated_clf.fit(X_train, y_train)\n\n# Comparaison avant/apr√®s calibration\ny_scores_raw = base_estimator.predict_proba(X_test)[:, 1]\ny_scores_cal = calibrated_clf.predict_proba(X_test)[:, 1]\n\n# Courbes PR comparatives\nfig, ax = plt.subplots(figsize=(10, 6))\n\nPrecisionRecallDisplay.from_predictions(\n    y_test, y_scores_raw, ax=ax, name='Raw Scores'\n)\nPrecisionRecallDisplay.from_predictions(\n    y_test, y_scores_cal, ax=ax, name='Calibrated Scores'\n)\n\nax.set_title('Impact of Probability Calibration')\nax.legend()\n```\n\n**üìä Visualisations Avanc√©es :**\n\n**Courbe PR Interactive :**\n```python\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Donn√©es pour la courbe\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\nf1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n\n# Subplot avec courbe PR et F1\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=('Precision-Recall Curve', 'F1-Score vs Threshold'),\n    specs=[[{'secondary_y': False}, {'secondary_y': False}]]\n)\n\n# Courbe PR\nfig.add_trace(\n    go.Scatter(\n        x=recall, y=precision,\n        mode='lines+markers',\n        name=f'PR Curve (AP={ap_score:.3f})',\n        hovertemplate='Recall: %{x:.3f}<br>Precision: %{y:.3f}<extra></extra>'\n    ),\n    row=1, col=1\n)\n\n# Baseline\nfig.add_hline(\n    y=y_true.mean(), line_dash=\"dash\", line_color=\"red\",\n    annotation_text=f\"Baseline ({y_true.mean():.3f})\",\n    row=1, col=1\n)\n\n# F1-Score\nfig.add_trace(\n    go.Scatter(\n        x=thresholds, y=f1_scores,\n        mode='lines',\n        name='F1-Score',\n        hovertemplate='Threshold: %{x:.3f}<br>F1: %{y:.3f}<extra></extra>'\n    ),\n    row=1, col=2\n)\n\n# Seuil optimal\noptimal_idx = np.argmax(f1_scores)\nfig.add_vline(\n    x=thresholds[optimal_idx], line_dash=\"dash\", line_color=\"green\",\n    annotation_text=f\"Optimal ({thresholds[optimal_idx]:.3f})\",\n    row=1, col=2\n)\n\nfig.update_layout(\n    title='Interactive Precision-Recall Analysis',\n    showlegend=True,\n    height=500\n)\n\nfig.show()\n```\n\n**Heatmap de Performance :**\n```python\n# Grille de seuils pour analyse\nthreshold_grid = np.linspace(0.1, 0.9, 20)\nrecall_grid = np.linspace(0.1, 1.0, 20)\n\n# Matrice de F1-scores\nf1_matrix = np.zeros((len(threshold_grid), len(recall_grid)))\n\nfor i, thresh in enumerate(threshold_grid):\n    y_pred = (y_scores >= thresh).astype(int)\n    for j, target_recall in enumerate(recall_grid):\n        # Calculer F1 si rappel >= target\n        current_recall = recall_score(y_true, y_pred)\n        if current_recall >= target_recall:\n            f1_matrix[i, j] = f1_score(y_true, y_pred)\n        else:\n            f1_matrix[i, j] = np.nan\n\n# Visualisation\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(f1_matrix, cmap='viridis', aspect='auto', origin='lower')\n\n# Contours\ncontours = ax.contour(f1_matrix, levels=10, colors='white', alpha=0.6)\nax.clabel(contours, inline=True, fontsize=8)\n\n# Labels\nax.set_xticks(range(0, len(recall_grid), 4))\nax.set_xticklabels([f'{r:.1f}' for r in recall_grid[::4]])\nax.set_yticks(range(0, len(threshold_grid), 4))\nax.set_yticklabels([f'{t:.1f}' for t in threshold_grid[::4]])\n\nax.set_xlabel('Target Recall')\nax.set_ylabel('Threshold')\nax.set_title('F1-Score Heatmap: Threshold vs Target Recall')\n\nplt.colorbar(im, label='F1-Score')\nplt.tight_layout()\n```\n\n**üéØ Applications Avanc√©es :**\n\n**D√©tection d'Anomalies Multi-Seuils :**\n```python\nclass AdaptiveThresholdDetector:\n    def __init__(self, base_model, precision_target=0.8):\n        self.base_model = base_model\n        self.precision_target = precision_target\n        self.optimal_threshold = None\n        \n    def fit(self, X, y):\n        self.base_model.fit(X, y)\n        y_scores = self.base_model.predict_proba(X)[:, 1]\n        \n        # Trouver seuil pour pr√©cision cible\n        precision, recall, thresholds = precision_recall_curve(y, y_scores)\n        valid_idx = precision >= self.precision_target\n        \n        if np.any(valid_idx):\n            # Maximiser rappel sous contrainte pr√©cision\n            best_recall_idx = np.argmax(recall[valid_idx])\n            self.optimal_threshold = thresholds[valid_idx][best_recall_idx]\n        else:\n            # Fallback: maximiser F1\n            f1_scores = 2 * (precision[:-1] * recall[:-1]) / (precision[:-1] + recall[:-1])\n            self.optimal_threshold = thresholds[np.argmax(f1_scores)]\n            \n        return self\n    \n    def predict(self, X):\n        y_scores = self.base_model.predict_proba(X)[:, 1]\n        return (y_scores >= self.optimal_threshold).astype(int)\n    \n    def predict_proba(self, X):\n        return self.base_model.predict_proba(X)\n\n# Usage\ndetector = AdaptiveThresholdDetector(RandomForestClassifier(), precision_target=0.85)\ndetector.fit(X_train, y_train)\ny_pred = detector.predict(X_test)\n```\n\n**Optimisation Multi-Objectifs :**\n```python\nfrom scipy.optimize import minimize\n\ndef multi_objective_loss(threshold, y_true, y_scores, alpha=0.5):\n    \"\"\"\n    Fonction de co√ªt combinant pr√©cision et rappel\n    alpha: poids relatif (0=rappel seul, 1=pr√©cision seule)\n    \"\"\"\n    y_pred = (y_scores >= threshold).astype(int)\n    \n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    \n    # Maximiser moyenne pond√©r√©e (minimiser son oppos√©)\n    objective = -(alpha * precision + (1 - alpha) * recall)\n    \n    return objective\n\n# Optimisation\nresult = minimize(\n    multi_objective_loss,\n    x0=0.5,  # Seuil initial\n    args=(y_true, y_scores, 0.7),  # alpha=0.7 favorise pr√©cision\n    bounds=[(0.01, 0.99)],\n    method='L-BFGS-B'\n)\n\noptimal_threshold = result.x[0]\nprint(f\"Seuil optimal multi-objectifs: {optimal_threshold:.3f}\")\n```\n\n**üåü Impact et Applications Modernes :**\nLa courbe Pr√©cision-Rappel est devenue l'√©talon-or pour √©valuer les mod√®les sur donn√©es d√©s√©quilibr√©es. Elle guide les syst√®mes de recommandation (Netflix, Amazon), la d√©tection de fraude (banques), le diagnostic m√©dical (radiologie IA), et la mod√©ration de contenu (r√©seaux sociaux). Son √©volution vers des m√©triques adaptatives et multi-objectifs refl√®te la complexit√© croissante des applications IA modernes.",
    category: "evaluation",
    icon: "LineChart"
  },
  {
    term: "Erreur quadratique moyenne (MSE)",
    description: "M√©trique de r√©gression calculant la moyenne des carr√©s des erreurs entre pr√©dictions et valeurs r√©elles. P√©nalise fortement les grandes erreurs.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "Erreur absolue moyenne (MAE)",
    description: "M√©trique de r√©gression calculant la moyenne des valeurs absolues des erreurs. Moins sensible aux outliers que MSE.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "R¬≤ (Coefficient de d√©termination)",
    description: "Mesure la proportion de variance dans la variable d√©pendante expliqu√©e par les variables ind√©pendantes. Varie de 0 √† 1, 1 indiquant un ajustement parfait.",
    category: "evaluation",
    icon: "TrendingUp"
  },
  {
    term: "RMSE (Root Mean Square Error)",
    description: "Racine carr√©e de MSE, exprim√©e dans les m√™mes unit√©s que la variable cible. Facilite l'interpr√©tation de l'erreur moyenne.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "Validation crois√©e (Cross-Validation)",
    description: "La validation crois√©e est comme faire passer plusieurs examens diff√©rents √† un √©tudiant pour avoir une note vraiment repr√©sentative ! **Principe d'or** : ne jamais faire confiance √† une seule √©valuation - multiplier les tests pour une estimation robuste des performances. **K-Fold classique** : diviser les donn√©es en k 'plis' √©gaux, entra√Æner sur k-1 plis, tester sur le pli restant, r√©p√©ter k fois, moyenner les r√©sultats. **Analogie p√©dagogique** : comme √©valuer un √©tudiant avec 5 examens diff√©rents plut√¥t qu'un seul - plus fiable et moins d√©pendant du hasard ! **Variantes populaires** : **Stratified K-Fold** (pr√©serve les proportions de classes), **Leave-One-Out** (k = n, tr√®s co√ªteux), **Time Series Split** (respecte l'ordre temporel). **Avantages magiques** : utilise **toutes** les donn√©es pour entra√Ænement ET validation, r√©duit la variance de l'estimation, d√©tecte l'instabilit√© du mod√®le. **Co√ªt computationnel** : k fois plus cher qu'une validation simple, mais investissement rentable ! **R√®gle empirique** : k=5 ou k=10 sont des choix populaires (compromis biais-variance). **Pi√®ge √† √©viter** : data leakage entre plis (preprocessing sur tout le dataset). **Interpr√©tation** : moyenne ¬± √©cart-type des k scores r√©v√®le performance ET stabilit√©. **Applications critiques** : s√©lection de mod√®les, tuning d'hyperparam√®tres, estimation finale de performance. La validation crois√©e transforme une √©valuation fragile en diagnostic robuste !",
    category: "evaluation",
    icon: "RefreshCw"
  },
  {
    term: "Validation holdout",
    description: "Division simple des donn√©es en ensembles d'entra√Ænement et de validation. Rapide mais peut √™tre moins fiable que la validation crois√©e.",
    category: "evaluation",
    icon: "Divide"
  },
  {
    term: "Bootstrap",
    description: "**üéØ La Magie du R√©√©chantillonnage !**\n\nComme un magicien qui tire plusieurs lapins du m√™me chapeau, le Bootstrap r√©volutionne l'estimation statistique en cr√©ant de multiples √©chantillons √† partir d'un seul dataset original, permettant d'√©valuer la variabilit√© et la fiabilit√© de nos mod√®les.\n\n**üé© Analogie du Magicien :**\nImaginez un magicien avec un chapeau contenant 1000 boules num√©rot√©es. Au lieu de regarder une seule fois, il tire 1000 boules avec remise, note le r√©sultat, remet tout, et r√©p√®te l'op√©ration 1000 fois. Chaque tirage donne une vision l√©g√®rement diff√©rente du contenu !\n\n**‚öôÔ∏è M√©canisme Fondamental :**\n\n**Principe de Base :**\n```\nDataset Original (n √©chantillons)\n        ‚Üì\nR√©√©chantillonnage avec remise\n        ‚Üì\nB √©chantillons Bootstrap (m√™me taille n)\n        ‚Üì\nCalcul de la statistique sur chaque √©chantillon\n        ‚Üì\nDistribution empirique de la statistique\n```\n\n**Processus D√©taill√© :**\n1. **√âchantillon Original** : Dataset de taille n\n2. **G√©n√©ration Bootstrap** : Tirer n observations avec remise\n3. **R√©p√©tition** : Cr√©er B √©chantillons (typiquement B = 1000-10000)\n4. **Calcul** : Statistique d'int√©r√™t sur chaque √©chantillon\n5. **Agr√©gation** : Distribution empirique des r√©sultats\n\n**üî¢ Math√©matiques du Bootstrap :**\n\n**Probabilit√© de S√©lection :**\n- Probabilit√© qu'un √©l√©ment soit s√©lectionn√© : 1 - (1-1/n)^n ‚âà 0.632\n- Environ 63.2% des donn√©es originales dans chaque √©chantillon\n- Certaines observations r√©p√©t√©es, d'autres absentes\n\n**Estimateur Bootstrap :**\n```\nŒ∏ÃÇ* = (1/B) Œ£ Œ∏ÃÇ*b\nSE(Œ∏ÃÇ) = ‚àö[(1/(B-1)) Œ£ (Œ∏ÃÇ*b - Œ∏ÃÇ*)¬≤]\n```\n\n**üéØ Applications en Machine Learning :**\n\n**√âvaluation de Mod√®les :**\n- **Performance Metrics** : Distribution de l'accuracy, F1-score\n- **Intervalles de Confiance** : Plages de performance attendues\n- **Comparaison de Mod√®les** : Tests statistiques robustes\n- **Stabilit√©** : Variance des pr√©dictions\n\n**Feature Importance :**\n- **Permutation Importance** : Stabilit√© des importances\n- **SHAP Values** : Distribution des contributions\n- **Coefficient Stability** : Robustesse des param√®tres\n\n**Hyperparameter Tuning :**\n- **Cross-Validation** : Estimation robuste des performances\n- **Bayesian Optimization** : Incertitude sur les hyperparam√®tres\n- **Early Stopping** : Crit√®res de convergence\n\n**üõ†Ô∏è Variantes Sp√©cialis√©es :**\n\n**Bootstrap Param√©trique :**\n- **Assumption** : Distribution connue des donn√©es\n- **G√©n√©ration** : √âchantillonnage depuis distribution estim√©e\n- **Avantage** : Plus efficace si assumptions correctes\n- **Usage** : Donn√©es suivant lois connues\n\n**Bootstrap Non-Param√©trique :**\n- **Assumption** : Aucune sur la distribution\n- **G√©n√©ration** : R√©√©chantillonnage direct des donn√©es\n- **Robustesse** : Fonctionne sans assumptions\n- **Usage** : Cas g√©n√©ral, donn√©es complexes\n\n**Block Bootstrap :**\n- **Donn√©es Temporelles** : Pr√©servation de la structure temporelle\n- **Blocs** : √âchantillonnage de s√©quences cons√©cutives\n- **Taille de Bloc** : Param√®tre critique √† optimiser\n- **Applications** : S√©ries temporelles, donn√©es spatiales\n\n**Wild Bootstrap :**\n- **H√©t√©rosc√©dasticit√©** : Variance non-constante\n- **R√©sidus** : Multiplication par variables al√©atoires\n- **Robustesse** : Contre violations d'homosc√©dasticit√©\n\n**üìä Intervalles de Confiance :**\n\n**Percentile Method :**\n- **Simple** : Quantiles 2.5% et 97.5% des r√©sultats Bootstrap\n- **IC 95%** : [Œ∏ÃÇ*‚ÇÄ.‚ÇÄ‚ÇÇ‚ÇÖ, Œ∏ÃÇ*‚ÇÄ.‚Çâ‚Çá‚ÇÖ]\n- **Avantage** : Facile √† calculer et interpr√©ter\n- **Limitation** : Peut √™tre biais√©\n\n**Bias-Corrected (BC) :**\n- **Correction** : Ajustement pour le biais de l'estimateur\n- **Formule** : Utilise la proportion de Œ∏ÃÇ*b < Œ∏ÃÇ\n- **Am√©lioration** : Meilleure couverture que percentile\n\n**Bias-Corrected and Accelerated (BCa) :**\n- **Gold Standard** : Correction biais + ajustement asym√©trie\n- **Acceleration** : Correction pour la non-lin√©arit√©\n- **Performance** : Meilleure couverture, surtout petits √©chantillons\n\n**üöÄ Applications Sectorielles :**\n\n**Finance :**\n- **Risk Metrics** : VaR, Expected Shortfall avec IC\n- **Portfolio Optimization** : Incertitude sur les rendements\n- **Backtesting** : Robustesse des strat√©gies\n- **Stress Testing** : Sc√©narios de crise\n\n**M√©decine :**\n- **Clinical Trials** : Efficacit√© des traitements\n- **Biomarkers** : Validation de marqueurs\n- **Diagnostic Tests** : Performance des tests\n- **Meta-Analysis** : Synth√®se d'√©tudes\n\n**Marketing :**\n- **A/B Testing** : Significativit√© des diff√©rences\n- **Customer Lifetime Value** : Incertitude sur les pr√©dictions\n- **Churn Prediction** : Stabilit√© des mod√®les\n- **Price Elasticity** : Robustesse des estimations\n\n**‚ö° Avantages Distinctifs :**\n\n**Non-Param√©trique :**\n- **Aucune Assumption** : Pas d'hypoth√®se sur la distribution\n- **Flexibilit√©** : Applicable √† toute statistique\n- **Robustesse** : R√©sistant aux outliers\n\n**Simplicit√© Conceptuelle :**\n- **Intuition** : Facile √† comprendre et expliquer\n- **Impl√©mentation** : Simple √† programmer\n- **Interpr√©tation** : R√©sultats directement utilisables\n\n**Polyvalence :**\n- **Toute Statistique** : Moyenne, m√©diane, corr√©lation, etc.\n- **Mod√®les Complexes** : R√©seaux de neurones, ensembles\n- **M√©triques Custom** : Statistiques m√©tier sp√©cifiques\n\n**üö® Limitations et Pr√©cautions :**\n\n**Assumptions Critiques :**\n- **Repr√©sentativit√©** : √âchantillon original doit √™tre repr√©sentatif\n- **Ind√©pendance** : Observations ind√©pendantes (sauf variantes sp√©cialis√©es)\n- **Taille** : √âchantillon suffisamment grand (n > 30 recommand√©)\n\n**Co√ªt Computationnel :**\n- **Temps** : B fois plus long que calcul simple\n- **M√©moire** : Stockage de B r√©sultats\n- **Parall√©lisation** : Facilement parall√©lisable\n\n**Biais Potentiels :**\n- **Small Sample** : Biais dans petits √©chantillons\n- **Extreme Values** : Sensibilit√© aux valeurs extr√™mes\n- **Model Assumptions** : Violations non d√©tect√©es\n\n**üîß Impl√©mentation Pratique :**\n\n**Python (scikit-learn) :**\n```python\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score\n\n# Bootstrap sampling\nbootstrap_scores = []\nfor i in range(1000):\n    X_boot, y_boot = resample(X, y)\n    model.fit(X_boot, y_boot)\n    score = accuracy_score(y_test, model.predict(X_test))\n    bootstrap_scores.append(score)\n\n# Confidence interval\nci_lower = np.percentile(bootstrap_scores, 2.5)\nci_upper = np.percentile(bootstrap_scores, 97.5)\n```\n\n**R (boot package) :**\n```r\nlibrary(boot)\n\n# Bootstrap function\nboot_stat <- function(data, indices) {\n  return(mean(data[indices]))\n}\n\n# Bootstrap sampling\nboot_results <- boot(data, boot_stat, R=1000)\nboot.ci(boot_results, type=\"bca\")\n```\n\n**üìà Bonnes Pratiques :**\n\n**Nombre d'√âchantillons :**\n- **B = 1000** : Minimum pour intervalles de confiance\n- **B = 10000** : Recommand√© pour analyses critiques\n- **Trade-off** : Pr√©cision vs temps de calcul\n\n**Validation :**\n- **Convergence** : V√©rifier stabilit√© avec B croissant\n- **Diagnostic Plots** : Histogrammes des r√©sultats Bootstrap\n- **Comparison** : Avec m√©thodes analytiques quand disponibles\n\n**Stratification :**\n- **Classes D√©s√©quilibr√©es** : Bootstrap stratifi√©\n- **Groupes** : Pr√©servation des proportions\n- **Time Series** : Block bootstrap appropri√©\n\n**üåü Impact et R√©volution :**\nLe Bootstrap, introduit par Bradley Efron en 1979, a r√©volutionn√© la statistique moderne en rendant l'inf√©rence statistique accessible sans assumptions distributionnelles. Avec l'av√®nement du machine learning, il devient l'outil de r√©f√©rence pour quantifier l'incertitude des mod√®les complexes, permettant une IA plus fiable et transparente.",
    category: "evaluation",
    icon: "Shuffle"
  },
  {
    term: "Biais-Variance Tradeoff",
    description: "**Le dilemme fondamental du machine learning !** Comme un archer qui doit choisir entre viser toujours au m√™me endroit (biais) ou avoir une vis√©e variable mais centr√©e (variance), tout mod√®le ML navigue entre ces deux sources d'erreur antagonistes.\n\n**üéØ Analogie de l'Archer :**\n\n**Biais √âlev√©, Variance Faible :**\n- Fl√®ches group√©es mais loin du centre\n- Mod√®le simple, pr√©dictions coh√©rentes mais fausses\n- Sous-apprentissage (underfitting)\n\n**Biais Faible, Variance √âlev√©e :**\n- Fl√®ches dispers√©es autour du centre\n- Mod√®le complexe, pr√©dictions variables\n- Sur-apprentissage (overfitting)\n\n**√âquilibre Optimal :**\n- Fl√®ches group√©es pr√®s du centre\n- Compromis entre simplicit√© et pr√©cision\n\n**üìä D√©composition Math√©matique :**\n\n**Erreur Totale :**\n```\nE[Erreur] = Biais¬≤ + Variance + Bruit\n```\n\n**Biais :**\n```\nBiais = E[fÃÇ(x)] - f(x)\n```\n- Diff√©rence entre pr√©diction moyenne et vraie valeur\n- Erreur syst√©matique du mod√®le\n- Ind√©pendant des donn√©es d'entra√Ænement\n\n**Variance :**\n```\nVariance = E[(fÃÇ(x) - E[fÃÇ(x)])¬≤]\n```\n- Variabilit√© des pr√©dictions entre datasets\n- Sensibilit√© aux donn√©es d'entra√Ænement\n- Instabilit√© du mod√®le\n\n**üîç Sources et Manifestations :**\n\n**Biais √âlev√© (Underfitting) :**\n- **Mod√®les trop simples** : R√©gression lin√©aire sur donn√©es non-lin√©aires\n- **Features insuffisantes** : Variables explicatives manquantes\n- **Hypoth√®ses fortes** : Assumptions incorrectes sur les donn√©es\n- **R√©gularisation excessive** : P√©nalit√©s trop importantes\n\n**Variance √âlev√©e (Overfitting) :**\n- **Mod√®les trop complexes** : R√©seaux profonds sur petits datasets\n- **Trop de param√®tres** : Plus de param√®tres que d'exemples\n- **Pas de r√©gularisation** : Libert√© totale d'apprentissage\n- **Donn√©es bruit√©es** : Apprentissage du bruit\n\n**‚öñÔ∏è Strat√©gies d'√âquilibrage :**\n\n**R√©duction du Biais :**\n- **Complexit√© accrue** : Plus de couches, polyn√¥mes d'ordre sup√©rieur\n- **Feature engineering** : Variables d√©riv√©es, interactions\n- **Ensembles** : Combinaison de mod√®les faibles\n- **Moins de r√©gularisation** : R√©duction des p√©nalit√©s\n\n**R√©duction de la Variance :**\n- **R√©gularisation** : L1, L2, Dropout, Early stopping\n- **Plus de donn√©es** : Datasets plus larges\n- **Validation crois√©e** : √âvaluation robuste\n- **Bagging** : Moyennage de mod√®les\n\n**üõ†Ô∏è Techniques Pratiques :**\n\n**Courbes d'Apprentissage :**\n- **Gap train/validation** : Indicateur de variance\n- **Plateau pr√©coce** : Signe de biais √©lev√©\n- **Convergence lente** : Besoin de plus de donn√©es\n\n**Validation Crois√©e :**\n- **Score moyen** : Estimation du biais\n- **√âcart-type** : Mesure de la variance\n- **Stabilit√©** : Robustesse du mod√®le\n\n**üìà Mod√®les et Tradeoff :**\n\n**Biais √âlev√©, Variance Faible :**\n- **R√©gression lin√©aire** : Assumptions fortes\n- **Naive Bayes** : Ind√©pendance des features\n- **k-NN avec k √©lev√©** : Moyennage local important\n\n**Biais Faible, Variance √âlev√©e :**\n- **Arbres de d√©cision profonds** : M√©morisation possible\n- **k-NN avec k=1** : Sensible au bruit\n- **R√©seaux de neurones** : Grande capacit√©\n\n**√âquilibre Naturel :**\n- **Random Forest** : Bagging d'arbres\n- **SVM avec RBF** : R√©gularisation int√©gr√©e\n- **Gradient Boosting** : Correction it√©rative\n\n**üéØ Applications Sectorielles :**\n\n**Finance :**\n- **Trading** : Variance √©lev√©e = strat√©gies instables\n- **Cr√©dit** : Biais √©lev√© = discrimination syst√©mique\n- **Risque** : √âquilibre pour robustesse\n\n**M√©decine :**\n- **Diagnostic** : Biais = erreurs syst√©matiques dangereuses\n- **Pronostic** : Variance = pr√©dictions incoh√©rentes\n- **Essais cliniques** : Validation rigoureuse n√©cessaire\n\n**üî¨ M√©thodes d'Analyse :**\n\n**Bootstrap :**\n- Estimation empirique biais/variance\n- R√©√©chantillonnage avec remise\n- Intervalles de confiance\n\n**Simulation Monte Carlo :**\n- G√©n√©ration de datasets multiples\n- Calcul exact des composantes\n- Validation th√©orique\n\n**üí° Insights Strat√©giques :**\n\n**R√®gles Empiriques :**\n- **Petits datasets** : Privil√©gier mod√®les simples (biais acceptable)\n- **Gros datasets** : Mod√®les complexes viables (variance contr√¥l√©e)\n- **Donn√©es bruit√©es** : R√©gularisation forte n√©cessaire\n\n**Optimisation Pratique :**\n- **Commencer simple** : Baseline avec biais √©lev√©\n- **Complexifier graduellement** : Monitoring de la variance\n- **Validation rigoureuse** : √âviter l'overfitting\n- **Ensembles** : Meilleur des deux mondes\n\n**üìä Impact Mesurable :**\nNetflix r√©duit l'erreur de recommandation de 15% en optimisant le tradeoff biais-variance via des ensembles de 100+ mod√®les. Google am√©liore la pr√©cision de recherche de 8% en √©quilibrant complexit√© et g√©n√©ralisation.",
    category: "evaluation",
    icon: "Scale"
  },
  {
    term: "Courbe d'apprentissage (Learning Curve)",
    description: "**üìà Le Diagnostic de l'Apprentissage !**\n\nComme un m√©decin qui suit l'√©volution d'un patient gr√¢ce √† des examens r√©guliers, la courbe d'apprentissage r√©v√®le la sant√© de votre mod√®le en tra√ßant ses performances selon la quantit√© de donn√©es d'entra√Ænement, permettant de diagnostiquer le sous-apprentissage, le sur-apprentissage, et d'estimer les b√©n√©fices d'obtenir plus de donn√©es.\n\n**üè• Analogie M√©dicale :**\nImaginez un √©tudiant en m√©decine qui passe des examens avec de plus en plus de mat√©riel d'√©tude. Au d√©but avec peu de livres, ses notes sont faibles (sous-apprentissage). Avec plus de ressources, ses performances s'am√©liorent. Mais √† un moment, ajouter plus de livres n'am√©liore plus ses notes - il a atteint son potentiel d'apprentissage !\n\n**üìä Anatomie d'une Courbe d'Apprentissage :**\n\n**Axes Fondamentaux :**\n- **Axe X** : Taille de l'ensemble d'entra√Ænement (nombre d'√©chantillons)\n- **Axe Y** : Performance du mod√®le (accuracy, F1-score, RMSE, etc.)\n- **Courbes** : Score d'entra√Ænement vs Score de validation\n\n**Construction M√©thodique :**\n```\nPour chaque taille d'entra√Ænement t ‚àà [t_min, t_max]:\n  1. S√©lectionner t √©chantillons d'entra√Ænement\n  2. Entra√Æner le mod√®le sur ces t √©chantillons\n  3. √âvaluer sur l'ensemble d'entra√Ænement ‚Üí Score_train(t)\n  4. √âvaluer sur l'ensemble de validation ‚Üí Score_val(t)\n  5. R√©p√©ter k fois (cross-validation)\n  6. Moyenner les r√©sultats\n```\n\n**üé≠ Les Quatre Visages de l'Apprentissage :**\n\n**1. Sous-Apprentissage (Underfitting) :**\n```\nCaract√©ristiques :\n- Score_train faible et stable\n- Score_val faible et stable\n- Gap minimal entre train et val\n- Plateau pr√©coce\n```\n\n**Diagnostic :**\n- **Mod√®le trop simple** pour capturer les patterns\n- **Features insuffisantes** ou mal choisies\n- **Hyperparam√®tres** trop restrictifs\n\n**Solutions :**\n- Augmenter la complexit√© du mod√®le\n- Ajouter des features ou interactions\n- R√©duire la r√©gularisation\n- Optimiser les hyperparam√®tres\n\n**2. Sur-Apprentissage (Overfitting) :**\n```\nCaract√©ristiques :\n- Score_train tr√®s √©lev√©\n- Score_val plafonn√© ou d√©croissant\n- Gap important et croissant\n- Divergence des courbes\n```\n\n**Diagnostic :**\n- **Mod√®le trop complexe** pour les donn√©es disponibles\n- **Donn√©es insuffisantes** pour la complexit√©\n- **Bruit** dans les donn√©es d'entra√Ænement\n\n**Solutions :**\n- Collecter plus de donn√©es\n- R√©duire la complexit√© du mod√®le\n- Augmenter la r√©gularisation\n- Early stopping, dropout\n\n**3. Apprentissage Optimal :**\n```\nCaract√©ristiques :\n- Score_train et Score_val convergent\n- Gap stable et minimal\n- Am√©lioration continue avec plus de donn√©es\n- Plateau √† haute performance\n```\n\n**Diagnostic :**\n- **√âquilibre parfait** complexit√©/donn√©es\n- **G√©n√©ralisation** excellente\n- **Robustesse** du mod√®le\n\n**4. Donn√©es Insuffisantes :**\n```\nCaract√©ristiques :\n- Courbes encore croissantes\n- Pas de plateau atteint\n- Gap d√©croissant\n- Potentiel d'am√©lioration visible\n```\n\n**üîç Analyse Avanc√©e des Patterns :**\n\n**Convergence Analysis :**\n```python\n# D√©tection de convergence\ndef is_converged(scores, window=5, threshold=0.01):\n    if len(scores) < window:\n        return False\n    recent_scores = scores[-window:]\n    return np.std(recent_scores) < threshold\n\n# Estimation du plateau\ndef estimate_plateau(train_sizes, scores):\n    # Fit polynomial et d√©riv√©e\n    coeffs = np.polyfit(train_sizes, scores, 3)\n    derivative = np.polyder(coeffs)\n    # Plateau quand d√©riv√©e ‚Üí 0\n    return np.roots(derivative)\n```\n\n**Gap Analysis :**\n```python\n# Analyse du gap train-validation\ndef analyze_gap(train_scores, val_scores):\n    gap = train_scores - val_scores\n    gap_trend = np.polyfit(range(len(gap)), gap, 1)[0]\n    \n    if gap_trend > 0.01:\n        return \"Overfitting croissant\"\n    elif gap_trend < -0.01:\n        return \"Am√©lioration de la g√©n√©ralisation\"\n    else:\n        return \"Gap stable\"\n```\n\n**üìê M√©triques et Indicateurs :**\n\n**Learning Efficiency :**\n```\nEfficiency = (Score_final - Score_initial) / log(N_samples)\n```\n*Mesure la rapidit√© d'apprentissage*\n\n**Data Efficiency :**\n```\nData_Efficiency = Score_target / N_samples_needed\n```\n*Quantit√© de donn√©es n√©cessaire pour atteindre un objectif*\n\n**Generalization Gap :**\n```\nGap(t) = Score_train(t) - Score_val(t)\nStable_Gap = lim_{t‚Üí‚àû} Gap(t)\n```\n*Mesure de la capacit√© de g√©n√©ralisation*\n\n**üéØ Applications Strat√©giques :**\n\n**Planification de Collecte de Donn√©es :**\n```python\n# Estimation ROI de nouvelles donn√©es\ndef estimate_data_roi(current_size, current_score, target_score):\n    # Fit learning curve\n    def learning_function(n, a, b, c):\n        return a - b * np.exp(-c * n)\n    \n    # Extrapolation\n    popt, _ = curve_fit(learning_function, sizes, scores)\n    needed_size = -np.log((popt[0] - target_score) / popt[1]) / popt[2]\n    \n    return max(0, needed_size - current_size)\n```\n\n**Optimisation des Ressources :**\n- **Budget Limit√©** : Trouver le sweet spot donn√©es/performance\n- **Temps Contraint** : Identifier le minimum viable\n- **Co√ªt/B√©n√©fice** : Quantifier l'impact de donn√©es suppl√©mentaires\n\n**üöÄ Applications Sectorielles :**\n\n**Vision par Ordinateur :**\n- **ImageNet** : Millions d'images n√©cessaires\n- **Transfer Learning** : R√©duction drastique des besoins\n- **Data Augmentation** : Augmentation artificielle\n- **Synthetic Data** : G√©n√©ration de donn√©es\n\n**NLP (Natural Language Processing) :**\n- **BERT/GPT** : Scaling laws observ√©s\n- **Few-Shot Learning** : Apprentissage avec peu d'exemples\n- **Domain Adaptation** : Transfert entre domaines\n- **Active Learning** : S√©lection intelligente des donn√©es\n\n**Recommandation Systems :**\n- **Cold Start** : Nouveaux utilisateurs/items\n- **Sparsity** : Donn√©es √©parses\n- **Temporal Dynamics** : √âvolution des pr√©f√©rences\n- **Implicit Feedback** : Signaux indirects\n\n**üîß Impl√©mentation Pratique :**\n\n**Scikit-learn :**\n```python\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import RandomForestClassifier\n\n# G√©n√©ration de la courbe\ntrain_sizes, train_scores, val_scores = learning_curve(\n    RandomForestClassifier(),\n    X, y,\n    train_sizes=np.linspace(0.1, 1.0, 10),\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\n# Visualisation\nplt.figure(figsize=(10, 6))\nplt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training')\nplt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation')\nplt.fill_between(train_sizes, \n                 np.mean(train_scores, axis=1) - np.std(train_scores, axis=1),\n                 np.mean(train_scores, axis=1) + np.std(train_scores, axis=1),\n                 alpha=0.1)\nplt.xlabel('Training Set Size')\nplt.ylabel('Accuracy Score')\nplt.legend()\nplt.title('Learning Curve')\n```\n\n**TensorFlow/Keras :**\n```python\nclass LearningCurveCallback(tf.keras.callbacks.Callback):\n    def __init__(self, X_val, y_val):\n        self.X_val = X_val\n        self.y_val = y_val\n        self.train_sizes = []\n        self.train_scores = []\n        self.val_scores = []\n    \n    def on_epoch_end(self, epoch, logs=None):\n        # √âvaluation sur diff√©rentes tailles\n        for size in [0.2, 0.4, 0.6, 0.8, 1.0]:\n            subset_size = int(size * len(self.model.x))\n            # Entra√Ænement sur subset\n            # √âvaluation et stockage\n```\n\n**üìä Visualisations Avanc√©es :**\n\n**Heatmap de Performance :**\n```python\n# Performance vs taille vs hyperparam√®tre\nfig, ax = plt.subplots(figsize=(12, 8))\nperformance_matrix = np.array([[score(size, param) \n                               for param in param_range] \n                               for size in size_range])\nsns.heatmap(performance_matrix, \n            xticklabels=param_range,\n            yticklabels=size_range,\n            annot=True, fmt='.3f')\n```\n\n**3D Learning Surface :**\n```python\n# Surface 3D : taille √ó complexit√© √ó performance\nfig = plt.figure(figsize=(12, 9))\nax = fig.add_subplot(111, projection='3d')\nX, Y = np.meshgrid(train_sizes, complexity_range)\nZ = performance_surface(X, Y)\nax.plot_surface(X, Y, Z, cmap='viridis')\n```\n\n**‚ö° Optimisations et Bonnes Pratiques :**\n\n**Stratified Sampling :**\n- **Classes √âquilibr√©es** : Pr√©servation des proportions\n- **Temporal Splits** : Respect de l'ordre temporel\n- **Geographical Splits** : √âviter le data leakage spatial\n\n**Cross-Validation Robuste :**\n- **K-Fold Stratified** : R√©duction de la variance\n- **Time Series CV** : Validation temporelle\n- **Group K-Fold** : √âviter le leakage par groupes\n\n**Computational Efficiency :**\n- **Incremental Learning** : R√©utilisation des mod√®les\n- **Parallel Processing** : Entra√Ænements simultan√©s\n- **Early Stopping** : Arr√™t intelligent\n\n**üåü Impact et Applications Modernes :**\nLes courbes d'apprentissage sont devenues essentielles dans l'√®re du big data et du deep learning pour optimiser les investissements en donn√©es. Elles guident les d√©cisions strat√©giques sur la collecte de donn√©es, l'architecture des mod√®les, et la planification des ressources, permettant un d√©veloppement d'IA plus efficace et √©conomique.",
    category: "evaluation",
    icon: "TrendingUp"
  },
  {
    term: "Courbe de validation (Validation Curve)",
    description: "**‚öôÔ∏è L'Art du R√©glage Optimal !**\n\nComme un ing√©nieur qui ajuste finement les param√®tres d'une machine pour obtenir les meilleures performances, la courbe de validation r√©v√®le l'impact de chaque hyperparam√®tre sur votre mod√®le, permettant de trouver le sweet spot entre sous-apprentissage et sur-apprentissage pour maximiser la g√©n√©ralisation.\n\n**üéõÔ∏è Analogie de l'Ing√©nieur :**\nImaginez r√©gler une radio pour capter une station. Trop √† gauche (sous-apprentissage) : signal faible et brouill√©. Trop √† droite (sur-apprentissage) : signal fort mais parasites. Il existe un point optimal o√π le signal est clair et fort - c'est exactement ce que trouve la courbe de validation !\n\n**üìä Anatomie d'une Courbe de Validation :**\n\n**Axes Fondamentaux :**\n- **Axe X** : Valeurs de l'hyperparam√®tre (complexit√©, r√©gularisation, etc.)\n- **Axe Y** : Performance du mod√®le (accuracy, F1-score, RMSE, etc.)\n- **Courbes** : Score d'entra√Ænement vs Score de validation\n\n**Construction M√©thodique :**\n```\nPour chaque valeur d'hyperparam√®tre h ‚àà [h_min, h_max]:\n  1. Configurer le mod√®le avec h\n  2. Entra√Æner sur l'ensemble d'entra√Ænement\n  3. √âvaluer sur l'ensemble d'entra√Ænement ‚Üí Score_train(h)\n  4. √âvaluer sur l'ensemble de validation ‚Üí Score_val(h)\n  5. R√©p√©ter k fois (cross-validation)\n  6. Moyenner les r√©sultats\n```\n\n**üéØ Hyperparam√®tres Critiques :**\n\n**Param√®tres de R√©gularisation :**\n\n**Ridge/Lasso (Œ±) :**\n```\nŒ± ‚Üí 0 : Pas de r√©gularisation (risque overfitting)\nŒ± ‚Üí ‚àû : R√©gularisation maximale (risque underfitting)\nOptimal : Minimum de la courbe de validation\n```\n\n**Pattern Typique :**\n- **Score_train** : D√©cro√Æt avec Œ± croissant\n- **Score_val** : Forme en U invers√©\n- **Optimum** : Minimum de l'erreur de validation\n\n**Random Forest (n_estimators) :**\n```\nn_estimators faible : Sous-apprentissage\nn_estimators √©lev√© : Am√©lioration puis plateau\nOptimal : D√©but du plateau (efficacit√© computationnelle)\n```\n\n**SVM (C et Œ≥) :**\n```\nC faible : Fronti√®re simple (underfitting)\nC √©lev√© : Fronti√®re complexe (overfitting)\nŒ≥ faible : Influence globale\nŒ≥ √©lev√© : Influence locale (overfitting)\n```\n\n**Neural Networks (learning_rate) :**\n```\nLR trop faible : Convergence lente\nLR trop √©lev√© : Instabilit√©, divergence\nOptimal : Convergence rapide et stable\n```\n\n**üîç Patterns d'Interpr√©tation :**\n\n**1. Courbe en U (R√©gularisation) :**\n```\nCaract√©ristiques :\n- Score_val d√©cro√Æt puis cro√Æt\n- Minimum clair\n- Score_train d√©cro√Æt monotoniquement\n\nInterpr√©tation :\n- Gauche : Underfitting (r√©gularisation excessive)\n- Droite : Overfitting (r√©gularisation insuffisante)\n- Minimum : √âquilibre optimal\n```\n\n**2. Plateau Croissant (Capacit√©) :**\n```\nCaract√©ristiques :\n- Score_val cro√Æt puis plateau\n- Score_train cro√Æt continuellement\n- Gap stable apr√®s plateau\n\nInterpr√©tation :\n- D√©but : Capacit√© insuffisante\n- Plateau : Capacit√© optimale atteinte\n- Apr√®s : Rendements d√©croissants\n```\n\n**3. Divergence Critique (Instabilit√©) :**\n```\nCaract√©ristiques :\n- Score_val chute brutalement\n- Score_train peut rester √©lev√©\n- Variance √©lev√©e\n\nInterpr√©tation :\n- Instabilit√© num√©rique\n- Hyperparam√®tre critique d√©pass√©\n- N√©cessit√© de contraintes\n```\n\n**üìê M√©triques d'Analyse :**\n\n**Optimal Point Detection :**\n```python\ndef find_optimal_param(param_values, val_scores, strategy='min'):\n    if strategy == 'min':\n        optimal_idx = np.argmin(val_scores)\n    elif strategy == 'max':\n        optimal_idx = np.argmax(val_scores)\n    elif strategy == '1se':\n        # One Standard Error Rule\n        best_score = np.min(val_scores)\n        se = np.std(val_scores) / np.sqrt(len(val_scores))\n        threshold = best_score + se\n        optimal_idx = np.where(val_scores <= threshold)[0][0]\n    \n    return param_values[optimal_idx]\n```\n\n**Stability Analysis :**\n```python\ndef analyze_stability(param_values, val_scores, val_stds):\n    # Coefficient de variation\n    cv = val_stds / np.abs(val_scores)\n    \n    # Zone stable (CV < 0.1)\n    stable_zone = param_values[cv < 0.1]\n    \n    # Recommandation conservative\n    if len(stable_zone) > 0:\n        return stable_zone[np.argmax(val_scores[cv < 0.1])]\n    else:\n        return param_values[np.argmin(cv)]\n```\n\n**üé® Visualisations Avanc√©es :**\n\n**Multi-Parameter Heatmap :**\n```python\n# Validation curve 2D\ndef plot_2d_validation_curve(param1_range, param2_range, scores):\n    fig, ax = plt.subplots(figsize=(10, 8))\n    \n    # Heatmap des scores\n    im = ax.imshow(scores, cmap='viridis', aspect='auto')\n    \n    # Contours pour les iso-performances\n    contours = ax.contour(scores, levels=10, colors='white', alpha=0.6)\n    ax.clabel(contours, inline=True, fontsize=8)\n    \n    # Optimum\n    max_idx = np.unravel_index(np.argmax(scores), scores.shape)\n    ax.plot(max_idx[1], max_idx[0], 'r*', markersize=15)\n    \n    plt.colorbar(im)\n    plt.title('2D Validation Surface')\n```\n\n**Interactive Exploration :**\n```python\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Courbe interactive\nfig = go.Figure()\n\n# Courbe de validation\nfig.add_trace(go.Scatter(\n    x=param_values,\n    y=val_scores_mean,\n    error_y=dict(type='data', array=val_scores_std),\n    mode='lines+markers',\n    name='Validation',\n    line=dict(color='blue')\n))\n\n# Courbe d'entra√Ænement\nfig.add_trace(go.Scatter(\n    x=param_values,\n    y=train_scores_mean,\n    error_y=dict(type='data', array=train_scores_std),\n    mode='lines+markers',\n    name='Training',\n    line=dict(color='red')\n))\n\n# Point optimal\noptimal_idx = np.argmax(val_scores_mean)\nfig.add_trace(go.Scatter(\n    x=[param_values[optimal_idx]],\n    y=[val_scores_mean[optimal_idx]],\n    mode='markers',\n    marker=dict(size=15, color='gold', symbol='star'),\n    name='Optimal'\n))\n```\n\n**üîß Impl√©mentation Pratique :**\n\n**Scikit-learn :**\n```python\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# Random Forest - n_estimators\ntrain_scores, val_scores = validation_curve(\n    RandomForestClassifier(random_state=42),\n    X, y,\n    param_name='n_estimators',\n    param_range=np.logspace(1, 3, 10).astype(int),\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1\n)\n\n# SVM - Param√®tre C\ntrain_scores_c, val_scores_c = validation_curve(\n    SVC(kernel='rbf'),\n    X, y,\n    param_name='C',\n    param_range=np.logspace(-3, 2, 10),\n    cv=5,\n    scoring='accuracy'\n)\n\n# Visualisation\nplt.figure(figsize=(12, 5))\n\n# Subplot 1: n_estimators\nplt.subplot(1, 2, 1)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\nval_mean = np.mean(val_scores, axis=1)\nval_std = np.std(val_scores, axis=1)\n\nparam_range_rf = np.logspace(1, 3, 10).astype(int)\nplt.semilogx(param_range_rf, train_mean, 'o-', color='red', label='Training')\nplt.fill_between(param_range_rf, train_mean - train_std, train_mean + train_std, alpha=0.1, color='red')\nplt.semilogx(param_range_rf, val_mean, 'o-', color='blue', label='Validation')\nplt.fill_between(param_range_rf, val_mean - val_std, val_mean + val_std, alpha=0.1, color='blue')\nplt.xlabel('n_estimators')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Random Forest Validation Curve')\n```\n\n**Grid Search Integration :**\n```python\nfrom sklearn.model_selection import GridSearchCV\n\n# Recherche exhaustive\nparam_grid = {\n    'C': np.logspace(-3, 2, 20),\n    'gamma': np.logspace(-3, 1, 20)\n}\n\ngrid_search = GridSearchCV(\n    SVC(kernel='rbf'),\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    return_train_score=True\n)\n\ngrid_search.fit(X, y)\n\n# Extraction des r√©sultats\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n# Validation curve pour chaque param√®tre\nfor param in ['C', 'gamma']:\n    # Grouper par param√®tre\n    grouped = results_df.groupby(f'param_{param}').agg({\n        'mean_train_score': 'mean',\n        'std_train_score': 'mean',\n        'mean_test_score': 'mean',\n        'std_test_score': 'mean'\n    })\n    \n    # Plot\n    plt.figure(figsize=(10, 6))\n    plt.errorbar(grouped.index, grouped['mean_train_score'], \n                 yerr=grouped['std_train_score'], label='Training')\n    plt.errorbar(grouped.index, grouped['mean_test_score'], \n                 yerr=grouped['std_test_score'], label='Validation')\n    plt.xscale('log')\n    plt.xlabel(param)\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.title(f'Validation Curve - {param}')\n```\n\n**üöÄ Applications Avanc√©es :**\n\n**Bayesian Optimization :**\n```python\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\n\n# Espace de recherche\ndimensions = [\n    Real(low=1e-6, high=1e1, prior='log-uniform', name='C'),\n    Real(low=1e-6, high=1e1, prior='log-uniform', name='gamma')\n]\n\n# Fonction objectif\n@use_named_args(dimensions)\ndef objective(**params):\n    model = SVC(**params)\n    scores = cross_val_score(model, X, y, cv=5)\n    return -np.mean(scores)  # Minimisation\n\n# Optimisation\nresult = gp_minimize(objective, dimensions, n_calls=50, random_state=42)\n\n# Validation curve bas√©e sur l'exploration\nC_values = [x[0] for x in result.x_iters]\ngamma_values = [x[1] for x in result.x_iters]\nscores = [-y for y in result.func_vals]\n```\n\n**Multi-Objective Optimization :**\n```python\n# Optimisation Pareto (performance vs complexit√©)\ndef multi_objective_validation(param_range, model_class, X, y):\n    results = []\n    \n    for param in param_range:\n        model = model_class(**{param_name: param})\n        \n        # Performance\n        scores = cross_val_score(model, X, y, cv=5)\n        performance = np.mean(scores)\n        \n        # Complexit√© (temps d'entra√Ænement)\n        start_time = time.time()\n        model.fit(X, y)\n        complexity = time.time() - start_time\n        \n        results.append({\n            'param': param,\n            'performance': performance,\n            'complexity': complexity\n        })\n    \n    return pd.DataFrame(results)\n\n# Front de Pareto\ndef pareto_front(df):\n    # Maximiser performance, minimiser complexit√©\n    pareto_points = []\n    for i, row in df.iterrows():\n        dominated = False\n        for j, other in df.iterrows():\n            if (other['performance'] >= row['performance'] and \n                other['complexity'] <= row['complexity'] and\n                (other['performance'] > row['performance'] or \n                 other['complexity'] < row['complexity'])):\n                dominated = True\n                break\n        if not dominated:\n            pareto_points.append(i)\n    \n    return df.iloc[pareto_points]\n```\n\n**üéØ Strat√©gies d'Optimisation :**\n\n**One Standard Error Rule :**\n```python\ndef one_se_rule(param_values, val_scores, val_stds):\n    \"\"\"\n    S√©lectionne le mod√®le le plus simple dans la zone\n    d'une erreur standard du meilleur mod√®le\n    \"\"\"\n    best_score = np.max(val_scores)\n    best_idx = np.argmax(val_scores)\n    se_threshold = best_score - val_stds[best_idx]\n    \n    # Mod√®les dans la zone acceptable\n    acceptable = val_scores >= se_threshold\n    \n    # Le plus simple (param√®tre le plus petit)\n    if np.any(acceptable):\n        acceptable_params = param_values[acceptable]\n        return np.min(acceptable_params)\n    else:\n        return param_values[best_idx]\n```\n\n**Early Stopping Integration :**\n```python\nclass ValidationCurveEarlyStopping:\n    def __init__(self, patience=5, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_score = -np.inf\n        self.wait = 0\n        self.best_params = None\n    \n    def should_stop(self, current_score, current_params):\n        if current_score > self.best_score + self.min_delta:\n            self.best_score = current_score\n            self.best_params = current_params\n            self.wait = 0\n        else:\n            self.wait += 1\n        \n        return self.wait >= self.patience\n```\n\n**üåü Impact et Applications Modernes :**\nLes courbes de validation sont devenues essentielles dans l'optimisation automatique des hyperparam√®tres (AutoML). Elles guident les algorithmes d'optimisation bay√©sienne, permettent l'early stopping intelligent, et r√©v√®lent les trade-offs performance/complexit√© cruciaux pour le d√©ploiement en production. Dans l'√®re du deep learning, elles restent l'outil de r√©f√©rence pour comprendre et optimiser le comportement des mod√®les.",
    category: "evaluation",
    icon: "Settings"
  },
  {
    term: "Test statistique",
    description: "M√©thodes pour d√©terminer si les diff√©rences de performance entre mod√®les sont statistiquement significatives (t-test, test de Wilcoxon, etc.).",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Intervalles de confiance",
    description: "Plages de valeurs qui contiennent probablement la vraie valeur d'une m√©trique de performance avec un niveau de confiance donn√©.",
    category: "evaluation",
    icon: "Target"
  },
  {
    term: "M√©triques m√©tier (Business Metrics)",
    description: "Mesures align√©es sur les objectifs commerciaux plut√¥t que purement techniques, comme le ROI, satisfaction client, ou r√©duction des co√ªts.",
    category: "evaluation",
    icon: "DollarSign"
  },
  {
    term: "A/B Testing",
    description: "M√©thode d'exp√©rimentation comparant deux versions (A et B) pour d√©terminer laquelle performe mieux selon une m√©trique d√©finie.",
    category: "evaluation",
    icon: "GitCompare"
  },
  {
    term: "Significance Testing",
    description: "Tests statistiques pour d√©terminer si les r√©sultats observ√©s sont dus au hasard ou repr√©sentent une diff√©rence r√©elle entre les conditions.",
    category: "evaluation",
    icon: "CheckCircle"
  },
  {
    term: "Power Analysis",
    description: "Calcul de la taille d'√©chantillon n√©cessaire pour d√©tecter un effet de taille donn√©e avec une probabilit√© sp√©cifi√©e.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "M√©triques de ranking",
    description: "Mesures pour √©valuer la qualit√© des syst√®mes de classement : NDCG, MAP, MRR. Importantes pour les moteurs de recherche et recommandations.",
    category: "evaluation",
    icon: "List"
  },
  {
    term: "M√©triques de clustering",
    description: "Mesures pour √©valuer la qualit√© des clusters : silhouette score, inertie, indice de Davies-Bouldin. Aident √† choisir le nombre optimal de clusters.",
    category: "evaluation",
    icon: "Layers"
  },
  {
    term: "M√©triques de g√©n√©ration de texte",
    description: "Mesures sp√©cialis√©es pour √©valuer la qualit√© du texte g√©n√©r√© : BLEU, ROUGE, perplexit√©, coh√©rence s√©mantique.",
    category: "evaluation",
    icon: "MessageSquare"
  },
  {
    term: "Fairness Metrics",
    description: "Mesures pour √©valuer l'√©quit√© des mod√®les ML : parit√© d√©mographique, √©galit√© des chances, calibration √©quitable.",
    category: "evaluation",
    icon: "Scale"
  },
  {
    term: "Robustness Testing",
    description: "√âvaluation de la stabilit√© du mod√®le face aux perturbations des donn√©es, changements de distribution, ou attaques adverses.",
    category: "evaluation",
    icon: "Shield"
  },
  {
    term: "Ablation Study",
    description: "Analyse syst√©matique de l'impact de chaque composant du mod√®le en les retirant un par un pour comprendre leur contribution.",
    category: "evaluation",
    icon: "Minus"
  },
  {
    term: "Baseline Models",
    description: "Mod√®les simples utilis√©s comme r√©f√©rence pour √©valuer si des approches plus complexes apportent une am√©lioration significative.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Human Evaluation",
    description: "√âvaluation par des experts humains, particuli√®rement importante pour les t√¢ches subjectives comme la g√©n√©ration de texte cr√©atif.",
    category: "evaluation",
    icon: "Users"
  }
];