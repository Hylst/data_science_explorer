/**
 * Model Evaluation and Metrics
 * Performance metrics, validation techniques, and model assessment methods
 */

import { GlossaryEntry } from './types';

export const evaluationTerms: GlossaryEntry[] = [
  {
    term: "√âvaluation de mod√®les (Model Evaluation)",
    description: "Processus d'assessment des performances d'un mod√®le ML en utilisant diverses m√©triques et techniques de validation pour d√©terminer sa qualit√© et capacit√© de g√©n√©ralisation.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Matrice de confusion (Confusion Matrix)",
    description: "La matrice de confusion est comme le bulletin scolaire d√©taill√© d'un mod√®le de classification - elle r√©v√®le exactement o√π il excelle et o√π il √©choue ! **Anatomie visuelle** : tableau 2x2 (binaire) ou n√ón (multiclasse) croisant pr√©dictions vs r√©alit√©. **Les 4 cases magiques** (binaire) : **TP** (Vrais Positifs) = 'Bravo, bien vu !', **TN** (Vrais N√©gatifs) = 'Correct, rien √† signaler', **FP** (Faux Positifs) = 'Fausse alerte !', **FN** (Faux N√©gatifs) = 'Rat√©, c'√©tait important !'. **Analogie m√©dicale** : diagnostic de maladie - FP = patient sain diagnostiqu√© malade (stress inutile), FN = patient malade non d√©tect√© (danger !). **Lecture intuitive** : diagonale = succ√®s, hors-diagonale = erreurs. Plus la diagonale est 'chaude' et les c√¥t√©s 'froids', mieux c'est ! **Insights pr√©cieux** : r√©v√®le les **confusions sp√©cifiques** (classe A confondue avec B), **d√©s√©quilibres** de performance, **patterns d'erreurs**. **Calculs d√©riv√©s** : toutes les m√©triques importantes (pr√©cision, rappel, F1, accuracy) se calculent √† partir d'elle. **Visualisation** : heatmap color√©e pour identifier rapidement les probl√®mes. **Cas multiclasse** : matrice n√ón r√©v√©lant les confusions entre toutes les paires de classes. La matrice de confusion transforme des chiffres abstraits en diagnostic visuel actionnable !",
    category: "evaluation",
    icon: "Grid3x3"
  },
  {
    term: "Pr√©cision (Precision)",
    description: "La pr√©cision r√©pond √† la question cruciale : 'Quand mon mod√®le dit OUI, √† quelle fr√©quence a-t-il raison ?' - c'est la mesure de la fiabilit√© de ses pr√©dictions positives ! **Formule simple** : Pr√©cision = TP/(TP+FP) = Vrais Positifs / Tous les Positifs pr√©dits. **Analogie m√©dicale** : sur 100 patients diagnostiqu√©s 'malades', combien le sont vraiment ? Une pr√©cision de 90% = 90 vrais malades, 10 fausses alertes. **Analogie spam** : sur 100 emails class√©s 'spam', combien sont vraiment du spam ? **Quand privil√©gier la pr√©cision** : co√ªt √©lev√© des **faux positifs** - diagnostic m√©dical grave, recommandations produits, d√©tection de fraude (√©viter d'emb√™ter les clients honn√™tes). **Trade-off fondamental** : augmenter la pr√©cision (√™tre plus s√©lectif) peut r√©duire le rappel (rater des vrais cas). **Exemple concret** : d√©tecteur de tumeurs avec pr√©cision 95% = sur 100 'tumeurs d√©tect√©es', 95 sont r√©elles, 5 sont des fausses alertes (stress inutile). **Interpr√©tation business** : pr√©cision √©lev√©e = confiance dans les alertes, moins de 'bruit', mais risque de rater des cas. **Pi√®ge classique** : pr√©cision parfaite (100%) facile √† obtenir en √©tant ultra-conservateur, mais au d√©triment du rappel. **Contexte d√©cisionnel** : pr√©f√©rer pr√©cision quand l'action suite √† une pr√©diction positive est co√ªteuse ou irr√©versible.",
    category: "evaluation",
    icon: "Target"
  },
  {
    term: "Rappel (Recall/Sensitivity)",
    description: "Le rappel r√©pond √† la question vitale : 'De tous les vrais cas positifs, combien mon mod√®le en a-t-il d√©tect√©s ?' - c'est la mesure de sa capacit√© √† ne rien laisser passer ! **Formule essentielle** : Rappel = TP/(TP+FN) = Vrais Positifs / Tous les Positifs r√©els. **Analogie s√©curitaire** : sur 100 vrais criminels, combien le syst√®me de surveillance en a-t-il rep√©r√©s ? Rappel 80% = 80 d√©tect√©s, 20 √©chappent ! **Analogie m√©dicale** : sur 100 patients r√©ellement malades, combien le test en d√©tecte-t-il ? **Quand privil√©gier le rappel** : co√ªt catastrophique des **faux n√©gatifs** - d√©tection de cancer, syst√®mes de s√©curit√©, alertes d'urgence (mieux vaut trop d'alertes que rater un danger). **Trade-off in√©vitable** : augmenter le rappel (√™tre moins s√©lectif) g√©n√®re souvent plus de faux positifs, r√©duisant la pr√©cision. **Exemple critique** : d√©tecteur d'incendie avec rappel 95% = d√©tecte 95% des vrais incendies, mais rate 5% (potentiellement catastrophique). **Synonymes** : Sensibilit√©, Taux de Vrais Positifs (TPR). **Interpr√©tation business** : rappel √©lev√© = s√©curit√© maximale, aucun cas important rat√©, mais plus de 'bruit'. **Contexte d√©cisionnel** : privil√©gier rappel quand rater un cas positif a des cons√©quences graves ou irr√©versibles. **Analogie filet** : rappel = taille des mailles du filet - plus fines (rappel √©lev√©) attrapent plus de poissons mais aussi plus de d√©chets.",
    category: "evaluation",
    icon: "Search"
  },
  {
    term: "F1-Score",
    description: "Le F1-Score est comme un diplomate qui n√©gocie la paix entre Pr√©cision et Rappel : il trouve le compromis parfait quand ces deux m√©triques rivales se disputent ! **Formule magique** : F1 = 2 √ó (Pr√©cision √ó Rappel) / (Pr√©cision + Rappel) = moyenne harmonique (plus stricte que moyenne arithm√©tique). **Pourquoi harmonique ?** : punit s√©v√®rement les d√©s√©quilibres - si Pr√©cision=90% et Rappel=10%, F1=18% (pas 50% !). Force l'√©quilibre ! **Analogie sportive** : comme noter un athl√®te sur sprint ET endurance - exceller dans un seul domaine ne suffit pas, il faut √™tre bon partout. **Cas d'usage parfait** : datasets d√©s√©quilibr√©s o√π l'accuracy est trompeuse (99% de classe majoritaire). **Exemple concret** : d√©tection de fraude - F1 √©lev√© = bon √©quilibre entre 'attraper les fraudeurs' (rappel) et '√©viter les fausses accusations' (pr√©cision). **Interpr√©tation** : F1=1.0 (parfait), F1=0.0 (catastrophique), F1>0.8 (g√©n√©ralement bon). **Avantage cl√©** : m√©trique unique qui r√©sume la performance globale sur la classe positive. **Limitation** : ignore les vrais n√©gatifs (pas toujours probl√©matique). **Variantes** : F2-Score (favorise rappel), F0.5-Score (favorise pr√©cision). **Analogie culinaire** : comme √©quilibrer sucr√©-sal√© - trop de l'un g√¢che le plat, l'harmonie fait la perfection. **Usage pratique** : m√©trique de r√©f√©rence pour comparer des mod√®les sur t√¢ches de classification binaire d√©s√©quilibr√©es.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Exactitude (Accuracy)",
    description: "**La m√©trique la plus intuitive mais la plus tra√Ætre !** L'exactitude est comme un thermom√®tre qui mesure la 'temp√©rature g√©n√©rale' de votre mod√®le - simple √† comprendre, mais qui peut masquer des probl√®mes graves.\n\n**üéØ Formule Ultra-Simple :**\nAccuracy = (TP + TN) / (TP + TN + FP + FN) = Pr√©dictions Correctes / Total des Pr√©dictions\n\n**üè´ Analogie Scolaire :**\nComme un pourcentage de bonnes r√©ponses √† un QCM : 85/100 questions correctes = 85% d'exactitude. Facile √† comprendre, rassurant... mais attention aux pi√®ges !\n\n**‚ö†Ô∏è Le Pi√®ge Mortel des Classes D√©s√©quilibr√©es :**\nImaginez d√©tecter une maladie rare (1% de la population) : un mod√®le 'idiot' qui dit toujours 'pas malade' aura 99% d'exactitude ! Impressionnant sur le papier, catastrophique en r√©alit√©.\n\n**üö® Exemple Concret du Pi√®ge :**\n- Dataset : 1000 emails (950 normaux, 50 spams)\n- Mod√®le paresseux : 'tout est normal' ‚Üí 95% d'exactitude\n- Probl√®me : 0% des spams d√©tect√©s !\n\n**‚úÖ Quand Utiliser l'Accuracy :**\n- Classes **√©quilibr√©es** (50/50 ou proche)\n- Co√ªt √©gal des erreurs (FP = FN)\n- Vue d'ensemble rapide des performances\n- Communication avec non-experts\n\n**‚ùå Quand l'√âviter :**\n- Classes tr√®s d√©s√©quilibr√©es\n- Co√ªt diff√©rent des types d'erreurs\n- D√©tection d'√©v√©nements rares\n- Applications critiques (m√©dical, s√©curit√©)\n\n**üîç Alternatives Plus Robustes :**\n- **F1-Score** : √©quilibre pr√©cision/rappel\n- **Balanced Accuracy** : moyenne des sensibilit√©s par classe\n- **Cohen's Kappa** : accord corrig√© du hasard\n- **AUC-ROC** : performance ind√©pendante du seuil\n\n**üí° R√®gle d'Or :**\nL'accuracy seule ne suffit JAMAIS - toujours l'accompagner d'autres m√©triques pour un diagnostic complet. C'est la m√©trique 'grand public' qui cache souvent la complexit√© r√©elle !",
    category: "evaluation",
    icon: "CheckCircle"
  },
  {
    term: "Sp√©cificit√© (Specificity)",
    description: "**Le gardien vigilant contre les fausses alertes !** La sp√©cificit√© mesure √† quel point votre mod√®le est dou√© pour dire 'NON' quand c'est vraiment NON - c'est l'art d'√©viter les faux positifs.\n\n**üéØ Formule Essentielle :**\nSp√©cificit√© = TN / (TN + FP) = Vrais N√©gatifs / Tous les N√©gatifs R√©els\n\n**üö® Question Cl√© :**\n'De tous les cas qui sont vraiment n√©gatifs, combien mon mod√®le les identifie-t-il correctement comme n√©gatifs ?'\n\n**üè• Analogie M√©dicale Parfaite :**\nTest de grossesse : sur 100 femmes NON enceintes, combien le test indique-t-il correctement 'n√©gatif' ? Sp√©cificit√© 95% = 95 r√©sultats corrects, 5 faux positifs (stress inutile !).\n\n**üîç Synonymes Importants :**\n- **Taux de Vrais N√©gatifs (TNR)**\n- **S√©lectivit√©**\n- **1 - Taux de Faux Positifs**\n\n**‚öñÔ∏è Dualit√© avec la Sensibilit√© :**\n- **Sensibilit√© (Rappel)** : 'Ne rien rater d'important'\n- **Sp√©cificit√©** : 'Ne pas crier au loup'\n- Trade-off in√©vitable : am√©liorer l'un d√©grade souvent l'autre\n\n**üéØ Cas d'Usage Critiques :**\n- **Screening m√©dical** : √©viter les fausses alertes co√ªteuses\n- **D√©tection de spam** : ne pas bloquer d'emails importants\n- **Syst√®mes de s√©curit√©** : r√©duire les fausses alarmes\n- **Contr√¥le qualit√©** : ne pas rejeter de bons produits\n\n**üìä Interpr√©tation Pratique :**\n- **Sp√©cificit√© > 95%** : Excellent, tr√®s peu de fausses alertes\n- **Sp√©cificit√© 80-95%** : Bon, acceptable pour la plupart des cas\n- **Sp√©cificit√© < 80%** : Probl√©matique, trop de faux positifs\n\n**‚ö†Ô∏è Pi√®ge Classique :**\nSp√©cificit√© parfaite (100%) facile √† obtenir en √©tant ultra-conservateur (tout classer n√©gatif), mais au d√©triment de la sensibilit√© !\n\n**üîÑ Relation avec ROC :**\nAxe X de la courbe ROC = 1 - Sp√©cificit√© = Taux de Faux Positifs. Plus la sp√©cificit√© est √©lev√©e, plus on est √† gauche sur la courbe.\n\n**üí° R√®gle Pratique :**\nPrivil√©gier la sp√©cificit√© quand le co√ªt d'une fausse alerte est √©lev√© (temps, argent, stress, ressources). C'est la m√©trique de la prudence et de la pr√©cision !",
    category: "evaluation",
    icon: "Shield"
  },
  {
    term: "Courbe ROC (ROC Curve)",
    description: "Graphique montrant la performance d'un mod√®le de classification binaire en tra√ßant le taux de vrais positifs contre le taux de faux positifs √† diff√©rents seuils.",
    category: "evaluation",
    icon: "TrendingUp"
  },
  {
    term: "AUC (Area Under Curve)",
    description: "**La mesure ultime de discrimination !** Comme un test m√©dical qui doit parfaitement s√©parer les malades des bien-portants, l'AUC quantifie la capacit√© d'un mod√®le √† distinguer entre les classes positives et n√©gatives sur l'ensemble du spectre de seuils possibles.\n\n**üìä Analogie G√©om√©trique :**\nImaginez la courbe ROC comme le profil d'une montagne : plus l'aire sous cette courbe est grande (proche de 1.0), plus le mod√®le est performant. Une AUC de 0.5 ressemble √† une ligne droite (performance al√©atoire), tandis qu'une AUC de 1.0 forme un carr√© parfait.\n\n**üéØ Interpr√©tation Intuitive :**\n\n**Signification Probabiliste :**\nL'AUC repr√©sente la probabilit√© qu'un mod√®le classe correctement un exemple positif choisi al√©atoirement plus haut qu'un exemple n√©gatif choisi al√©atoirement.\n\n**√âchelle de Performance :**\n‚Ä¢ **0.9 - 1.0** : Excellence (diagnostic m√©dical)\n‚Ä¢ **0.8 - 0.9** : Tr√®s bon (d√©tection fraude)\n‚Ä¢ **0.7 - 0.8** : Bon (marketing pr√©dictif)\n‚Ä¢ **0.6 - 0.7** : Moyen (am√©lioration n√©cessaire)\n‚Ä¢ **0.5 - 0.6** : Faible (√† peine mieux que le hasard)\n‚Ä¢ **< 0.5** : Pire que le hasard (inverser les pr√©dictions !)\n\n**üîç Construction Math√©matique :**\n\n**Courbe ROC :**\n- **Axe X** : Taux de Faux Positifs (1 - Sp√©cificit√©)\n- **Axe Y** : Taux de Vrais Positifs (Sensibilit√©)\n- **Points** : Performance √† diff√©rents seuils\n\n**Calcul de l'AUC :**\n```\nAUC = ‚à´‚ÇÄ¬π TPR(FPR) d(FPR)\n```\n\n**M√©thode Trap√©zo√Ødale :**\n- Approximation num√©rique par trap√®zes\n- Pr√©cision d√©pendante du nombre de seuils\n- Impl√©mentation standard dans sklearn\n\n**‚ö° Avantages Distinctifs :**\n\n**Invariance au Seuil :**\n- √âvalue toutes les performances possibles\n- Pas besoin de choisir un seuil optimal\n- Vision globale du mod√®le\n\n**Invariance √† l'√âchelle :**\n- Mesure qualit√© du ranking, pas valeurs absolues\n- Robuste aux transformations monotones\n- Comparable entre mod√®les diff√©rents\n\n**üö® Limitations Critiques :**\n\n**Classes D√©s√©quilibr√©es :**\n- AUC peut √™tre optimiste\n- Privil√©gie la classe majoritaire\n- Pr√©f√©rer AUC-PR (Precision-Recall)\n\n**Interpr√©tation M√©tier :**\n- Pas directement li√©e aux co√ªts business\n- Ne refl√®te pas l'impact des erreurs\n- Compl√©ment n√©cessaire avec m√©triques m√©tier\n\n**üéØ Applications Sectorielles :**\n\n**M√©decine :**\n- **Diagnostic** : AUC > 0.95 pour tests critiques\n- **Screening** : Balance sensibilit√©/sp√©cificit√©\n- **Biomarqueurs** : Validation de nouveaux tests\n\n**Finance :**\n- **Cr√©dit** : Scoring de risque de d√©faut\n- **Fraude** : D√©tection transactions suspectes\n- **Trading** : Signaux d'achat/vente\n\n**Marketing :**\n- **Churn** : Pr√©diction d√©sabonnement\n- **Conversion** : Probabilit√© d'achat\n- **Segmentation** : Classification clients\n\n**üõ†Ô∏è Variantes Sp√©cialis√©es :**\n\n**AUC-PR (Precision-Recall) :**\n- Meilleure pour classes d√©s√©quilibr√©es\n- Focus sur la classe positive\n- Moins sensible aux vrais n√©gatifs\n\n**Partial AUC :**\n- AUC dans une r√©gion sp√©cifique\n- Utile pour contraintes m√©tier\n- Ex: FPR < 0.1 pour applications critiques\n\n**Multi-class AUC :**\n- **One-vs-Rest** : AUC moyenne par classe\n- **One-vs-One** : AUC pour chaque paire\n- **Macro/Micro averaging** : Strat√©gies d'agr√©gation\n\n**üìà Optimisation Pratique :**\n\n**Feature Engineering :**\n- S√©lection bas√©e sur AUC individuelle\n- Interactions augmentant la s√©parabilit√©\n- Transformations non-lin√©aires\n\n**Hyperparameter Tuning :**\n- Validation crois√©e avec AUC\n- Optimisation bay√©sienne\n- Early stopping bas√© sur AUC validation\n\n**üî¨ Tests Statistiques :**\n\n**Comparaison de Mod√®les :**\n- Test de DeLong pour AUC\n- Bootstrap pour intervalles de confiance\n- Correction de Bonferroni pour tests multiples\n\n**Significativit√© :**\n- p-value < 0.05 pour diff√©rence significative\n- Taille d'effet (diff√©rence d'AUC)\n- Puissance statistique du test\n\n**üí° Bonnes Pratiques :**\n- **Validation crois√©e** stratifi√©e\n- **Intervalles de confiance** syst√©matiques\n- **Comparaison** avec baseline simple\n- **Analyse** des courbes ROC compl√®tes\n- **Contexte m√©tier** toujours consid√©r√©\n\n**üìä Impact Mesurable :**\nGoogle am√©liore ses mod√®les publicitaires de 0.001 AUC par trimestre, g√©n√©rant des millions de revenus suppl√©mentaires. En m√©decine, une am√©lioration d'AUC de 0.05 peut sauver des milliers de vies.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Courbe Pr√©cision-Rappel",
    description: "Graphique montrant le compromis entre pr√©cision et rappel √† diff√©rents seuils. Particuli√®rement utile pour les datasets d√©s√©quilibr√©s.",
    category: "evaluation",
    icon: "LineChart"
  },
  {
    term: "Erreur quadratique moyenne (MSE)",
    description: "M√©trique de r√©gression calculant la moyenne des carr√©s des erreurs entre pr√©dictions et valeurs r√©elles. P√©nalise fortement les grandes erreurs.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "Erreur absolue moyenne (MAE)",
    description: "M√©trique de r√©gression calculant la moyenne des valeurs absolues des erreurs. Moins sensible aux outliers que MSE.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "R¬≤ (Coefficient de d√©termination)",
    description: "Mesure la proportion de variance dans la variable d√©pendante expliqu√©e par les variables ind√©pendantes. Varie de 0 √† 1, 1 indiquant un ajustement parfait.",
    category: "evaluation",
    icon: "TrendingUp"
  },
  {
    term: "RMSE (Root Mean Square Error)",
    description: "Racine carr√©e de MSE, exprim√©e dans les m√™mes unit√©s que la variable cible. Facilite l'interpr√©tation de l'erreur moyenne.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "Validation crois√©e (Cross-Validation)",
    description: "La validation crois√©e est comme faire passer plusieurs examens diff√©rents √† un √©tudiant pour avoir une note vraiment repr√©sentative ! **Principe d'or** : ne jamais faire confiance √† une seule √©valuation - multiplier les tests pour une estimation robuste des performances. **K-Fold classique** : diviser les donn√©es en k 'plis' √©gaux, entra√Æner sur k-1 plis, tester sur le pli restant, r√©p√©ter k fois, moyenner les r√©sultats. **Analogie p√©dagogique** : comme √©valuer un √©tudiant avec 5 examens diff√©rents plut√¥t qu'un seul - plus fiable et moins d√©pendant du hasard ! **Variantes populaires** : **Stratified K-Fold** (pr√©serve les proportions de classes), **Leave-One-Out** (k = n, tr√®s co√ªteux), **Time Series Split** (respecte l'ordre temporel). **Avantages magiques** : utilise **toutes** les donn√©es pour entra√Ænement ET validation, r√©duit la variance de l'estimation, d√©tecte l'instabilit√© du mod√®le. **Co√ªt computationnel** : k fois plus cher qu'une validation simple, mais investissement rentable ! **R√®gle empirique** : k=5 ou k=10 sont des choix populaires (compromis biais-variance). **Pi√®ge √† √©viter** : data leakage entre plis (preprocessing sur tout le dataset). **Interpr√©tation** : moyenne ¬± √©cart-type des k scores r√©v√®le performance ET stabilit√©. **Applications critiques** : s√©lection de mod√®les, tuning d'hyperparam√®tres, estimation finale de performance. La validation crois√©e transforme une √©valuation fragile en diagnostic robuste !",
    category: "evaluation",
    icon: "RefreshCw"
  },
  {
    term: "Validation holdout",
    description: "Division simple des donn√©es en ensembles d'entra√Ænement et de validation. Rapide mais peut √™tre moins fiable que la validation crois√©e.",
    category: "evaluation",
    icon: "Divide"
  },
  {
    term: "Bootstrap",
    description: "Technique de r√©√©chantillonnage avec remise pour estimer la distribution des statistiques et √©valuer la variabilit√© des performances du mod√®le.",
    category: "evaluation",
    icon: "Shuffle"
  },
  {
    term: "Biais-Variance Tradeoff",
    description: "**Le dilemme fondamental du machine learning !** Comme un archer qui doit choisir entre viser toujours au m√™me endroit (biais) ou avoir une vis√©e variable mais centr√©e (variance), tout mod√®le ML navigue entre ces deux sources d'erreur antagonistes.\n\n**üéØ Analogie de l'Archer :**\n\n**Biais √âlev√©, Variance Faible :**\n- Fl√®ches group√©es mais loin du centre\n- Mod√®le simple, pr√©dictions coh√©rentes mais fausses\n- Sous-apprentissage (underfitting)\n\n**Biais Faible, Variance √âlev√©e :**\n- Fl√®ches dispers√©es autour du centre\n- Mod√®le complexe, pr√©dictions variables\n- Sur-apprentissage (overfitting)\n\n**√âquilibre Optimal :**\n- Fl√®ches group√©es pr√®s du centre\n- Compromis entre simplicit√© et pr√©cision\n\n**üìä D√©composition Math√©matique :**\n\n**Erreur Totale :**\n```\nE[Erreur] = Biais¬≤ + Variance + Bruit\n```\n\n**Biais :**\n```\nBiais = E[fÃÇ(x)] - f(x)\n```\n- Diff√©rence entre pr√©diction moyenne et vraie valeur\n- Erreur syst√©matique du mod√®le\n- Ind√©pendant des donn√©es d'entra√Ænement\n\n**Variance :**\n```\nVariance = E[(fÃÇ(x) - E[fÃÇ(x)])¬≤]\n```\n- Variabilit√© des pr√©dictions entre datasets\n- Sensibilit√© aux donn√©es d'entra√Ænement\n- Instabilit√© du mod√®le\n\n**üîç Sources et Manifestations :**\n\n**Biais √âlev√© (Underfitting) :**\n- **Mod√®les trop simples** : R√©gression lin√©aire sur donn√©es non-lin√©aires\n- **Features insuffisantes** : Variables explicatives manquantes\n- **Hypoth√®ses fortes** : Assumptions incorrectes sur les donn√©es\n- **R√©gularisation excessive** : P√©nalit√©s trop importantes\n\n**Variance √âlev√©e (Overfitting) :**\n- **Mod√®les trop complexes** : R√©seaux profonds sur petits datasets\n- **Trop de param√®tres** : Plus de param√®tres que d'exemples\n- **Pas de r√©gularisation** : Libert√© totale d'apprentissage\n- **Donn√©es bruit√©es** : Apprentissage du bruit\n\n**‚öñÔ∏è Strat√©gies d'√âquilibrage :**\n\n**R√©duction du Biais :**\n- **Complexit√© accrue** : Plus de couches, polyn√¥mes d'ordre sup√©rieur\n- **Feature engineering** : Variables d√©riv√©es, interactions\n- **Ensembles** : Combinaison de mod√®les faibles\n- **Moins de r√©gularisation** : R√©duction des p√©nalit√©s\n\n**R√©duction de la Variance :**\n- **R√©gularisation** : L1, L2, Dropout, Early stopping\n- **Plus de donn√©es** : Datasets plus larges\n- **Validation crois√©e** : √âvaluation robuste\n- **Bagging** : Moyennage de mod√®les\n\n**üõ†Ô∏è Techniques Pratiques :**\n\n**Courbes d'Apprentissage :**\n- **Gap train/validation** : Indicateur de variance\n- **Plateau pr√©coce** : Signe de biais √©lev√©\n- **Convergence lente** : Besoin de plus de donn√©es\n\n**Validation Crois√©e :**\n- **Score moyen** : Estimation du biais\n- **√âcart-type** : Mesure de la variance\n- **Stabilit√©** : Robustesse du mod√®le\n\n**üìà Mod√®les et Tradeoff :**\n\n**Biais √âlev√©, Variance Faible :**\n- **R√©gression lin√©aire** : Assumptions fortes\n- **Naive Bayes** : Ind√©pendance des features\n- **k-NN avec k √©lev√©** : Moyennage local important\n\n**Biais Faible, Variance √âlev√©e :**\n- **Arbres de d√©cision profonds** : M√©morisation possible\n- **k-NN avec k=1** : Sensible au bruit\n- **R√©seaux de neurones** : Grande capacit√©\n\n**√âquilibre Naturel :**\n- **Random Forest** : Bagging d'arbres\n- **SVM avec RBF** : R√©gularisation int√©gr√©e\n- **Gradient Boosting** : Correction it√©rative\n\n**üéØ Applications Sectorielles :**\n\n**Finance :**\n- **Trading** : Variance √©lev√©e = strat√©gies instables\n- **Cr√©dit** : Biais √©lev√© = discrimination syst√©mique\n- **Risque** : √âquilibre pour robustesse\n\n**M√©decine :**\n- **Diagnostic** : Biais = erreurs syst√©matiques dangereuses\n- **Pronostic** : Variance = pr√©dictions incoh√©rentes\n- **Essais cliniques** : Validation rigoureuse n√©cessaire\n\n**üî¨ M√©thodes d'Analyse :**\n\n**Bootstrap :**\n- Estimation empirique biais/variance\n- R√©√©chantillonnage avec remise\n- Intervalles de confiance\n\n**Simulation Monte Carlo :**\n- G√©n√©ration de datasets multiples\n- Calcul exact des composantes\n- Validation th√©orique\n\n**üí° Insights Strat√©giques :**\n\n**R√®gles Empiriques :**\n- **Petits datasets** : Privil√©gier mod√®les simples (biais acceptable)\n- **Gros datasets** : Mod√®les complexes viables (variance contr√¥l√©e)\n- **Donn√©es bruit√©es** : R√©gularisation forte n√©cessaire\n\n**Optimisation Pratique :**\n- **Commencer simple** : Baseline avec biais √©lev√©\n- **Complexifier graduellement** : Monitoring de la variance\n- **Validation rigoureuse** : √âviter l'overfitting\n- **Ensembles** : Meilleur des deux mondes\n\n**üìä Impact Mesurable :**\nNetflix r√©duit l'erreur de recommandation de 15% en optimisant le tradeoff biais-variance via des ensembles de 100+ mod√®les. Google am√©liore la pr√©cision de recherche de 8% en √©quilibrant complexit√© et g√©n√©ralisation.",
    category: "evaluation",
    icon: "Scale"
  },
  {
    term: "Courbe d'apprentissage (Learning Curve)",
    description: "Graphique montrant l'√©volution des performances du mod√®le en fonction de la taille de l'ensemble d'entra√Ænement. Aide √† diagnostiquer le sur/sous-apprentissage.",
    category: "evaluation",
    icon: "TrendingUp"
  },
  {
    term: "Courbe de validation (Validation Curve)",
    description: "Graphique montrant les performances d'entra√Ænement et validation en fonction d'un hyperparam√®tre. Aide √† identifier la valeur optimale.",
    category: "evaluation",
    icon: "LineChart"
  },
  {
    term: "Test statistique",
    description: "M√©thodes pour d√©terminer si les diff√©rences de performance entre mod√®les sont statistiquement significatives (t-test, test de Wilcoxon, etc.).",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Intervalles de confiance",
    description: "Plages de valeurs qui contiennent probablement la vraie valeur d'une m√©trique de performance avec un niveau de confiance donn√©.",
    category: "evaluation",
    icon: "Target"
  },
  {
    term: "M√©triques m√©tier (Business Metrics)",
    description: "Mesures align√©es sur les objectifs commerciaux plut√¥t que purement techniques, comme le ROI, satisfaction client, ou r√©duction des co√ªts.",
    category: "evaluation",
    icon: "DollarSign"
  },
  {
    term: "A/B Testing",
    description: "M√©thode d'exp√©rimentation comparant deux versions (A et B) pour d√©terminer laquelle performe mieux selon une m√©trique d√©finie.",
    category: "evaluation",
    icon: "GitCompare"
  },
  {
    term: "Significance Testing",
    description: "Tests statistiques pour d√©terminer si les r√©sultats observ√©s sont dus au hasard ou repr√©sentent une diff√©rence r√©elle entre les conditions.",
    category: "evaluation",
    icon: "CheckCircle"
  },
  {
    term: "Power Analysis",
    description: "Calcul de la taille d'√©chantillon n√©cessaire pour d√©tecter un effet de taille donn√©e avec une probabilit√© sp√©cifi√©e.",
    category: "evaluation",
    icon: "Calculator"
  },
  {
    term: "M√©triques de ranking",
    description: "Mesures pour √©valuer la qualit√© des syst√®mes de classement : NDCG, MAP, MRR. Importantes pour les moteurs de recherche et recommandations.",
    category: "evaluation",
    icon: "List"
  },
  {
    term: "M√©triques de clustering",
    description: "Mesures pour √©valuer la qualit√© des clusters : silhouette score, inertie, indice de Davies-Bouldin. Aident √† choisir le nombre optimal de clusters.",
    category: "evaluation",
    icon: "Layers"
  },
  {
    term: "M√©triques de g√©n√©ration de texte",
    description: "Mesures sp√©cialis√©es pour √©valuer la qualit√© du texte g√©n√©r√© : BLEU, ROUGE, perplexit√©, coh√©rence s√©mantique.",
    category: "evaluation",
    icon: "MessageSquare"
  },
  {
    term: "Fairness Metrics",
    description: "Mesures pour √©valuer l'√©quit√© des mod√®les ML : parit√© d√©mographique, √©galit√© des chances, calibration √©quitable.",
    category: "evaluation",
    icon: "Scale"
  },
  {
    term: "Robustness Testing",
    description: "√âvaluation de la stabilit√© du mod√®le face aux perturbations des donn√©es, changements de distribution, ou attaques adverses.",
    category: "evaluation",
    icon: "Shield"
  },
  {
    term: "Ablation Study",
    description: "Analyse syst√©matique de l'impact de chaque composant du mod√®le en les retirant un par un pour comprendre leur contribution.",
    category: "evaluation",
    icon: "Minus"
  },
  {
    term: "Baseline Models",
    description: "Mod√®les simples utilis√©s comme r√©f√©rence pour √©valuer si des approches plus complexes apportent une am√©lioration significative.",
    category: "evaluation",
    icon: "BarChart3"
  },
  {
    term: "Human Evaluation",
    description: "√âvaluation par des experts humains, particuli√®rement importante pour les t√¢ches subjectives comme la g√©n√©ration de texte cr√©atif.",
    category: "evaluation",
    icon: "Users"
  }
];