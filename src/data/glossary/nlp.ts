export const nlpTerms = [
  {
    term: "Analyse de sentiment (Sentiment Analysis)",
    description: `**L'art de d√©coder les √©motions dans le texte !** L'analyse de sentiment est une technique de NLP qui d√©termine automatiquement l'attitude √©motionnelle (positive, n√©gative, neutre) exprim√©e dans un texte.

**üéØ Applications Concr√®tes :**
‚Ä¢ **R√©seaux sociaux** : Analyser l'opinion publique sur une marque
‚Ä¢ **E-commerce** : Classifier les avis clients automatiquement
‚Ä¢ **Finance** : Pr√©dire les mouvements de march√© via les news
‚Ä¢ **Support client** : Prioriser les tickets selon l'urgence √©motionnelle

**üîß Approches Techniques :**
‚Ä¢ **Lexicale** : Dictionnaires de mots positifs/n√©gatifs
‚Ä¢ **Machine Learning** : Classification supervis√©e (SVM, Naive Bayes)
‚Ä¢ **Deep Learning** : RNN, LSTM, Transformers (BERT)
‚Ä¢ **Hybride** : Combinaison de plusieurs m√©thodes

**üí° D√©fis Sp√©cifiques :**
‚Ä¢ Sarcasme et ironie
‚Ä¢ N√©gations ("pas mal" vs "mal")
‚Ä¢ Contexte culturel et linguistique
‚Ä¢ Sentiments mixtes dans un m√™me texte

L'analyse de sentiment transforme l'opinion subjective en donn√©es objectives exploitables !`,
    category: "nlp",
    icon: "Heart"
  },
  {
    term: "TF-IDF (Term Frequency-Inverse Document Frequency)",
    description: `**L'art de distinguer l'important du banal !** TF-IDF est une technique fondamentale qui identifie les termes vraiment significatifs en √©quilibrant leur fr√©quence locale avec leur raret√© globale.

**üéØ Principe Fondamental :**
TF-IDF r√©sout le paradoxe de la pertinence textuelle : comment identifier les mots qui caract√©risent vraiment un document sans √™tre noy√©s par les mots ultra-fr√©quents ("le", "de", "et") ou les mots ultra-rares ?

**üßÆ Formule :**
**TF-IDF(t,d,D) = TF(t,d) √ó IDF(t,D)**

O√π :
‚Ä¢ **TF** = Fr√©quence du terme dans le document
‚Ä¢ **IDF** = Inverse de la fr√©quence documentaire
‚Ä¢ **t** = terme, **d** = document, **D** = corpus

**üí° Applications :**
‚Ä¢ Recherche d'information et moteurs de recherche
‚Ä¢ Classification de texte automatique
‚Ä¢ D√©tection de plagiat et similarit√© documentaire
‚Ä¢ Extraction de mots-cl√©s pertinents
‚Ä¢ Analyse de sentiment et opinion mining

**‚ö° Avantages :**
‚Ä¢ Simple √† comprendre et impl√©menter
‚Ä¢ Efficace computationnellement
‚Ä¢ R√©sultats interpr√©tables
‚Ä¢ Baseline solide pour de nombreuses t√¢ches

**‚ö†Ô∏è Limitations :**
‚Ä¢ Ignore l'ordre des mots et le contexte s√©mantique
‚Ä¢ Sensible √† la taille du corpus
‚Ä¢ Assume l'ind√©pendance des termes
‚Ä¢ Peut √™tre domin√© par des termes tr√®s sp√©cifiques

**üîß Bonnes Pratiques :**
‚Ä¢ Pr√©processing adapt√© (normalisation, suppression mots vides)
‚Ä¢ Utilisation de n-grammes pour capturer le contexte
‚Ä¢ Combinaison avec des embeddings pour la s√©mantique
‚Ä¢ Filtrage par fr√©quence (min_df, max_df)

TF-IDF reste un **fondement essentiel** du NLP, combinant simplicit√©, efficacit√© et interpr√©tabilit√© !`,
    category: "nlp",
    icon: "BarChart3"
  },
  {
    term: "N-grammes (N-grams)",
    description: `**Les s√©quences qui donnent du sens !** Les N-grammes sont des s√©quences contigu√´s de N √©l√©ments (mots, caract√®res) extraites d'un texte, capturant le contexte local et les patterns linguistiques.

**üî¢ Types Principaux :**
‚Ä¢ **Unigrammes (1-gram)** : Mots individuels ["chat", "mange"]
‚Ä¢ **Bigrammes (2-gram)** : Paires de mots ["chat mange", "mange souris"]
‚Ä¢ **Trigrammes (3-gram)** : Triplets ["le chat mange"]
‚Ä¢ **N-grammes de caract√®res** : S√©quences de caract√®res

**üíª Exemple Pratique :**
\`\`\`python
from sklearn.feature_extraction.text import CountVectorizer

# Bigrammes de mots
vectorizer = CountVectorizer(ngram_range=(2, 2))
texts = ["le chat mange la souris"]
bigrammes = vectorizer.fit_transform(texts)
print(vectorizer.get_feature_names_out())
# ['chat mange', 'la souris', 'le chat', 'mange la']
\`\`\`

**üéØ Applications :**
‚Ä¢ **Mod√®les de langue** : Pr√©diction du mot suivant
‚Ä¢ **D√©tection de langue** : Patterns caract√©ristiques
‚Ä¢ **Correction orthographique** : S√©quences probables
‚Ä¢ **Classification de texte** : Features contextuelles
‚Ä¢ **G√©n√©ration de texte** : Cha√Ænes de Markov

**‚öñÔ∏è Trade-offs :**
‚Ä¢ **N faible** : Perte de contexte, mais g√©n√©ralisation
‚Ä¢ **N √©lev√©** : Contexte riche, mais sparsit√© et overfitting
‚Ä¢ **Compromis optimal** : G√©n√©ralement N=2 ou N=3

Les N-grammes transforment le texte brut en features structur√©es exploitables !`,
    category: "nlp",
    icon: "Link"
  }
];